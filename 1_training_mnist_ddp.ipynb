{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1_training_mnist_ddp(Distributed Data Parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train and test an MNIST model on SageMaker using PyTorch.\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = True\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add data-root and default-shm-size=10G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Redirecting to /bin/systemctl restart docker.service\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker Restart\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "DAEMON_PATH=\"/etc/docker\"\n",
    "MEMORY_SIZE=10G\n",
    "\n",
    "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# echo $FLAG\n",
    "\n",
    "if [ \"$FLAG\" == true ]; then\n",
    "    echo \"Already revised\"\n",
    "else\n",
    "    echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "    sudo service docker restart\n",
    "    echo \"Docker Restart\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing deps and restarting kernel\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.144 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "jupyterlab-server 2.22.1 requires jsonschema>=4.17.3, but you have jsonschema 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install --upgrade pip --quiet\n",
    "    !{sys.executable} -m pip install -U sagemaker --quiet\n",
    "    !{sys.executable} -m pip install -U torchvision --quiet\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-mnist'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.169.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: dataset/mnist\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "datasets.MNIST('dataset/mnist', download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-322537213286/sagemaker/DEMO-pytorch-mnist\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='dataset/mnist', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "The script provides all the code we need for training and hosting a SageMaker model (`model_fn` function to load a model).\n",
    "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method, the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "Because the SageMaker imports the training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model as we do in this example, so that SageMaker does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For example, the script run by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pygmentize ./training_code/mnist/pytorch_mnist_ptddp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "     {'Name': 'train:Loss', 'Regex': 'Train Loss: (.*?),'},\n",
    "     {'Name': 'test:Loss', 'Regex': 'Average loss: (.*?),'},\n",
    "     {'Name': 'test:Accuracy', 'Regex': 'Accuracy: (.*?)%'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training with Distributed Data Parallel\n",
    "\n",
    "\n",
    "The training script provides the code you need for distributed data parallel (DDP) training. The training script is very similar to a PyTorch training script you might run outside of SageMaker.\n",
    "\n",
    "In the following code block, you can update the estimator function to use a different instance type, instance count, and distrubtion strategy. You're also passing in the training script you reviewed in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'epochs': 5,\n",
    "    'backend': 'nccl',  \n",
    "    # 'backend': 'smddp', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distribution = {}\n",
    "\n",
    "if hyperparameters['backend'] == 'nccl':\n",
    "    # ### MPIRUN 수행\n",
    "    distribution[\"mpi\"]={\"enabled\": True}\n",
    "elif hyperparameters['backend'] == 'smddp':\n",
    "    ### SageMaker DDP\n",
    "    distribution[\"smdistributed\"] = {\"dataparallel\": {\"enabled\": True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = 'ml.p3.16xlarge'  # 'ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'local_gpu'\n",
    "# instance_type = 'local_gpu'\n",
    "instance_count = 2\n",
    "max_run = 1*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-322537213286/sagemaker/DEMO-pytorch-mnist'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if instance_type =='local_gpu':\n",
    "    from sagemaker.local import LocalSession\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    s3_data_path = f'file://{Path.cwd()}/dataset/mnist'\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    s3_data_path = inputs\n",
    "s3_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Pytorch` estimator를 이용한 training job 생성하기\n",
    "\n",
    "\n",
    "<p><strong><code>sagemaker.pytorch.PyTorch</code></strong> estimator는 처음 실행하는 스크립트 위치와 다양한 연계 코드들이 위치한 디렉토리 정보를 찾아서 스크립트를 S3에 upload하고 SageMaker의 training job을 수행하게 됩니다. training job은 학습을 수행한 단위입니다. 학습을 1번 돌리면 training job이 1개 생성됩니다. 몇 가지 중요 파라미터를 아래와 같이 설명드립니다. </p>\n",
    "\n",
    "- **entry_point** : 학습을 처음 실행하는 Python 소스 파일의 절대 또는 상대 경로이며, source_dir이 지정된 경우 entry_point는 source_dir 내 파일이 됩니다.\n",
    "- **source_dir** : 학습에 연계되는 다양한 소스코드 파일이 들어 있는 디렉토리 위치이며, 절대, 상대 경로 또는 S3 URI가 모두 가능하며,source_dir이 S3 URI 인 경우 tar.gz 파일이 됩니다.\n",
    "- **role** : Amazon SageMaker가 사용자를 대신해 작업(예: S3 버킷에서 모델 결과물이라고 하는 훈련 결과 읽기 및 Amazon S3에 훈련 결과 쓰기)을 수행하는 AWS Identity and Access Management(IAM) 역할입니다.\n",
    "- **train_instance_count** : 학습을 수행하는 instance 개수를 정의할 수 있습니다.\n",
    "- **train_instance_type** : 학습을 수행하는 instance 타입을 정의할 수 있습니다.\n",
    "- **train_volume_size** : 학습 인스턴스에 연결할 Amazon Elastic Block Store(Amazon EBS) 스토리지 볼륨의 크기(GB)입니다. File 모드를 사용할 경우 이 값이 훈련 데이터를 충분히 저장할 수 있는 크기여야 합니다(File 모드가 기본값)\n",
    "- **train_max_run** : 최대 학습 시간을 설정할 수 있으며, 이 시간이 지나면 Amazon SageMaker는 현재 상태에 관계없이 작업을 종료합니다. (기본값 : 24 * 60 * 60)\n",
    "- **framework_version** : 학습에 사용될 특정 Pytorch 버전을 정의할 수 있습니다.\n",
    "- **py_version** : 컨테이너 환경이 python3일 경우 py3, python2일 경우 py2로 설정하면 됩니다. python2는 지원이 중단되었지만 기존 python2로 구성된 파일들을 지원하기 위해 현재 계속 사용할 수 있습니다. 없을 경우에는 기본적으로 py3 입니다.\n",
    "- **hyperparameters** : 학습에 사용할 하이퍼 파라미터를 정의할 수 있으며, 정의된 하이퍼 파라미터 값들은 모두 학습 컨테이너로 전송이 됩니다.\n",
    "- **distribution** : 분산과 관련된 값들을 학습 컨테이너로 전송합니다.\n",
    "\n",
    "<p> 추가적으로 분산/ 멀티 GPU 학습도 가능합니다. SageMaker는 <strong><a href=\"https://github.com/horovod/horovod\" target=\"_blank\" class ='btn-default'>Horovod</a></strong>에 최적화된 환경을 제공하고 있으며, Pytorch의 경우 1.5.0부터 기본 docker에서 apex를 지원합니다.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='pytorch_mnist_ddp.py',\n",
    "                    source_dir=f'{Path.cwd()}/training_code/mnist',\n",
    "                    role=role,\n",
    "                    framework_version='1.13.1',\n",
    "                    py_version='py39',\n",
    "                    instance_count=instance_count,\n",
    "                    instance_type=instance_type,\n",
    "                    distribution=distribution,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    disable_profiler=True,\n",
    "                    debugger_hook_config=False,\n",
    "                    max_run=max_run,\n",
    "                    hyperparameters=hyperparameters,\n",
    "                    sagemaker_session=sagemaker_session\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: mnist-ml-p3-16xlarge-0702-07591688284762\n"
     ]
    }
   ],
   "source": [
    "current_time = strftime(\"%m%d-%H%M%s\")\n",
    "i_type = instance_type.replace('.','-')\n",
    "job_name = f'mnist-{i_type}-{current_time}'\n",
    "\n",
    "estimator.fit(\n",
    "    inputs={'training': s3_data_path}, \n",
    "    job_name=job_name,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-02 07:59:23 Starting - Starting the training job......\n",
      "2023-07-02 08:00:20 Starting - Preparing the instances for training.........\n",
      "2023-07-02 08:01:32 Downloading - Downloading input data......\n",
      "2023-07-02 08:02:27 Training - Downloading the training image..................\n",
      "2023-07-02 08:05:33 Training - Training image download completed. Training in progress...\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:01,662 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:01,731 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:01,743 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:01,746 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:02,215 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:02,295 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:02,308 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:02,308 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:02,310 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:02,310 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.155.171.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:03,323 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:03,496 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:03,496 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:03,496 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:03,496 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:03,501 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:02,637 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:02,705 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:02,718 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:02,721 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:03,121 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:03,202 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:03,215 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:03,215 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:03,217 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:03,218 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:03,218 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.185.187.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:04,231 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:04,402 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:04,402 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:04,402 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:04,402 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:8', 'algo-2:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:04,403 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:04,475 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:04,555 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:04,569 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"nccl\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mnist-ml-p3-16xlarge-0702-07591688284762\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-322537213286/mnist-ml-p3-16xlarge-0702-07591688284762/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"pytorch_mnist_ddp\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"pytorch_mnist_ddp.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"nccl\",\"epochs\":5}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=pytorch_mnist_ddp.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=pytorch_mnist_ddp\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-322537213286/mnist-ml-p3-16xlarge-0702-07591688284762/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"nccl\",\"epochs\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mnist-ml-p3-16xlarge-0702-07591688284762\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/mnist-ml-p3-16xlarge-0702-07591688284762/source/sourcedir.tar.gz\",\"module_name\":\"pytorch_mnist_ddp\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"pytorch_mnist_ddp.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"nccl\",\"--epochs\",\"5\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=nccl\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_NUM_NEURONS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAINING -x SM_HP_BACKEND -x SM_HP_EPOCHS -x PYTHONPATH /opt/conda/bin/python3.9 -m mpi4py pytorch_mnist_ddp.py --backend nccl --epochs 5\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:04,635 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:05,936 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.185.187' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:06,508 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=58, name='orted', status='sleeping', started='08:06:05')]\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:06,508 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=58, name='orted', status='sleeping', started='08:06:05')]\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:06,509 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=58, name='orted', status='sleeping', started='08:06:05')]\u001b[0m\n",
      "\u001b[34mData for JOB [41117,1] offset 0 Total slots allocated 16\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [41117,1] App: 0 Process rank: 15 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:130 [0] NCCL INFO Bootstrap : Using eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:130 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:130 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:130 [0] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:134 [4] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:135 [5] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:137 [7] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:136 [6] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:131 [1] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:133 [3] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:61 [0] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:62 [1] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:63 [2] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:64 [3] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:65 [4] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:66 [5] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:67 [6] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:68 [7] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:136 [6] NCCL INFO Bootstrap : Using eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:131 [1] NCCL INFO Bootstrap : Using eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:134 [4] NCCL INFO Bootstrap : Using eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:137 [7] NCCL INFO Bootstrap : Using eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:135 [5] NCCL INFO Bootstrap : Using eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:133 [3] NCCL INFO Bootstrap : Using eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:68 [7] NCCL INFO Bootstrap : Using eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:135 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:135 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:136 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:136 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:134 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:134 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:63 [2] NCCL INFO Bootstrap : Using eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:133 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:133 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:131 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:131 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:137 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:137 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:64 [3] NCCL INFO Bootstrap : Using eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:65 [4] NCCL INFO Bootstrap : Using eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:66 [5] NCCL INFO Bootstrap : Using eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:67 [6] NCCL INFO Bootstrap : Using eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:61 [0] NCCL INFO Bootstrap : Using eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:62 [1] NCCL INFO Bootstrap : Using eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:67 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:67 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:61 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:61 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:68 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:68 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:63 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:63 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:64 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:64 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:62 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:62 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:65 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:65 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:66 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:66 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:132 [2] NCCL INFO cudaDriverVersion 12000\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:132 [2] NCCL INFO Bootstrap : Using eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:132 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:132 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO NET/Socket : Using [0]eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO NET/Socket : Using [0]eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO NET/Socket : Using [0]eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO NET/Socket : Using [0]eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO NET/Socket : Using [0]eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO NET/Socket : Using [0]eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO NET/Socket : Using [0]eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.155.171<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO NET/Socket : Using [0]eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.185.187<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Trees [0] 4/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 4/-1/-1->7->6 [3] 4/-1/-1->7->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Trees [0] 6/-1/-1->5->1 [1] 6/-1/-1->5->1 [2] 6/-1/-1->5->1 [3] 6/-1/-1->5->1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Trees [0] -1/-1/-1->4->7 [1] -1/-1/-1->4->7 [2] -1/-1/-1->4->7 [3] -1/-1/-1->4->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Trees [0] 5/-1/-1->1->2 [1] 5/-1/-1->1->2 [2] 5/-1/-1->1->2 [3] 5/-1/-1->1->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Trees [0] 2/-1/-1->3->0 [1] 2/-1/-1->3->0 [2] 2/-1/-1->3->0 [3] 2/-1/-1->3->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 00/04 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 01/04 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 02/04 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 03/04 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Trees [0] 3/8/-1->0->-1 [1] 3/-1/-1->0->8 [2] 3/8/-1->0->-1 [3] 3/-1/-1->0->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Trees [0] 1/-1/-1->2->3 [1] 1/-1/-1->2->3 [2] 1/-1/-1->2->3 [3] 1/-1/-1->2->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Trees [0] 12/-1/-1->15->14 [1] 12/-1/-1->15->14 [2] 12/-1/-1->15->14 [3] 12/-1/-1->15->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Trees [0] 14/-1/-1->13->9 [1] 14/-1/-1->13->9 [2] 14/-1/-1->13->9 [3] 14/-1/-1->13->9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Trees [0] -1/-1/-1->12->15 [1] -1/-1/-1->12->15 [2] -1/-1/-1->12->15 [3] -1/-1/-1->12->15\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Trees [0] 10/-1/-1->11->8 [1] 10/-1/-1->11->8 [2] 10/-1/-1->11->8 [3] 10/-1/-1->11->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Trees [0] 13/-1/-1->9->10 [1] 13/-1/-1->9->10 [2] 13/-1/-1->9->10 [3] 13/-1/-1->9->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Trees [0] 11/-1/-1->8->0 [1] 11/0/-1->8->-1 [2] 11/-1/-1->8->0 [3] 11/0/-1->8->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Trees [0] 9/-1/-1->10->11 [1] 9/-1/-1->10->11 [2] 9/-1/-1->10->11 [3] 9/-1/-1->10->11\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 00/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 00/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 00/0 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 00/0 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 00/0 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 01/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 01/0 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 01/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 01/0 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 01/0 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 02/0 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 02/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 02/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 02/0 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 02/0 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 00/0 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 00/0 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 03/0 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 03/0 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 03/0 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 01/0 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 03/0 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 03/0 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 00/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 00/0 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 02/0 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 01/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 02/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 01/0 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 03/0 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 03/0 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 02/0 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 03/0 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 00/0 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 01/0 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 00/0 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 02/0 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 01/0 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 03/0 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 02/0 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 03/0 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 00/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 00/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 01/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 01/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 02/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 02/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 03/0 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 03/0 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 00/0 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 00/0 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 01/0 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 01/0 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 00/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 01/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 02/0 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 02/0 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 02/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 03/0 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 03/0 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 03/0 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 00/0 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 01/0 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 02/0 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 03/0 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 00/0 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 01/0 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 02/0 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 03/0 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 00/0 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 01/0 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 00/1 : 10[190] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 02/0 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 03/0 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 00/1 : 9[180] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 01/1 : 10[190] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 01/1 : 9[180] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 01/0 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:659 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 00/0 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 00/0 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 00/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 01/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 02/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 03/0 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 00/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 01/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 02/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 00/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 03/0 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 00/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 01/1 : 2[190] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 01/1 : 1[180] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 02/0 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:659 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 01/0 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 03/0 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:659 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 02/0 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 02/0 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:659 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 03/0 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 03/0 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 00/0 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 01/0 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 02/0 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 03/0 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 00/0 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 01/0 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 02/0 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 03/0 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 00/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 01/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 02/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 03/0 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 00/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 01/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 02/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 03/0 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 00/0 : 8[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:659 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 00/0 : 0[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 01/0 : 8[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:659 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 01/0 : 0[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 02/0 : 8[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:659 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 02/0 : 0[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 03/0 : 8[170] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:659 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 03/0 : 0[170] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 00/0 : 8[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 00/0 : 0[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 01/0 : 8[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 01/0 : 0[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 02/0 : 8[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 02/0 : 0[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 03/0 : 8[170] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 03/0 : 0[170] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 00/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 00/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 01/1 : 3[1a0] -> 4[1b0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 01/1 : 0[170] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 00/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 01/1 : 3[1a0] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 00/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 00/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 00/1 : 8[170] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 00/1 : 11[1a0] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO Channel 01/1 : 3[1a0] -> 6[1d0] via P2P/indirect/7[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 01/1 : 2[190] -> 5[1c0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 01/1 : 11[1a0] -> 12[1b0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 01/1 : 8[170] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 00/1 : 11[1a0] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 00/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 00/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 00/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 01/1 : 4[1b0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 01/1 : 11[1a0] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO Channel 01/1 : 2[190] -> 7[1e0] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 01/1 : 1[180] -> 6[1d0] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 00/1 : 10[190] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 00/1 : 11[1a0] -> 14[1d0] via P2P/indirect/15[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 00/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 00/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 00/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 01/1 : 10[190] -> 13[1c0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO Channel 01/1 : 11[1a0] -> 14[1d0] via P2P/indirect/15[1e0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 01/1 : 5[1c0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO Channel 01/1 : 1[180] -> 7[1e0] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 00/1 : 12[1b0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 00/1 : 9[180] -> 14[1d0] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 00/1 : 10[190] -> 15[1e0] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 01/1 : 0[170] -> 6[1d0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 01/1 : 12[1b0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 00/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 00/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 01/1 : 9[180] -> 14[1d0] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO Channel 01/1 : 10[190] -> 15[1e0] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 01/1 : 6[1d0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO Channel 01/1 : 0[170] -> 7[1e0] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 00/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 00/1 : 8[170] -> 14[1d0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 00/1 : 13[1c0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 01/1 : 7[1e0] -> 0[170] via P2P/indirect/4[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 01/1 : 13[1c0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 00/1 : 9[180] -> 15[1e0] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 00/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 01/1 : 7[1e0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 01/1 : 8[170] -> 14[1d0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO Channel 01/1 : 9[180] -> 15[1e0] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 00/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 00/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 01/1 : 6[1d0] -> 1[180] via P2P/indirect/5[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO Channel 01/1 : 7[1e0] -> 2[190] via P2P/indirect/3[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 00/1 : 14[1d0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 00/1 : 8[170] -> 15[1e0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 01/1 : 14[1d0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO Channel 01/1 : 8[170] -> 15[1e0] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 00/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 00/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 00/1 : 15[1e0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 01/1 : 5[1c0] -> 2[190] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO Channel 01/1 : 6[1d0] -> 3[1a0] via P2P/indirect/2[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 01/1 : 15[1e0] -> 8[170] via P2P/indirect/12[1b0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 00/1 : 15[1e0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 00/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 01/1 : 15[1e0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 00/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO Channel 01/1 : 5[1c0] -> 3[1a0] via P2P/indirect/1[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 00/1 : 15[1e0] -> 10[190] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 00/1 : 14[1d0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO Channel 01/1 : 15[1e0] -> 10[190] via P2P/indirect/11[1a0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 01/1 : 4[1b0] -> 2[190] via P2P/indirect/6[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 01/1 : 14[1d0] -> 9[180] via P2P/indirect/13[1c0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 00/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO Channel 01/1 : 4[1b0] -> 3[1a0] via P2P/indirect/0[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 00/1 : 14[1d0] -> 11[1a0] via P2P/indirect/10[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 00/1 : 13[1c0] -> 10[190] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 01/1 : 13[1c0] -> 10[190] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO Channel 01/1 : 14[1d0] -> 11[1a0] via P2P/indirect/10[190]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 00/1 : 13[1c0] -> 11[1a0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 00/1 : 12[1b0] -> 10[190] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO Channel 01/1 : 13[1c0] -> 11[1a0] via P2P/indirect/9[180]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 01/1 : 12[1b0] -> 10[190] via P2P/indirect/14[1d0]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 00/1 : 12[1b0] -> 11[1a0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO Channel 01/1 : 12[1b0] -> 11[1a0] via P2P/indirect/8[170]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:723 [2] NCCL INFO comm 0x55effcba4580 rank 2 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:722 [1] NCCL INFO comm 0x564dfc0cc6a0 rank 1 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:721 [3] NCCL INFO comm 0x55d43cd34900 rank 3 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:716 [0] NCCL INFO comm 0x558731238160 rank 0 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:718 [4] NCCL INFO comm 0x55d8e24abb40 rank 4 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:720 [7] NCCL INFO comm 0x55f964c8fca0 rank 7 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:719 [6] NCCL INFO comm 0x5652f20f1700 rank 6 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:717 [5] NCCL INFO comm 0x564130ab42c0 rank 5 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:651 [3] NCCL INFO comm 0x55a82fe43720 rank 11 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:652 [1] NCCL INFO comm 0x565127248780 rank 9 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:646 [4] NCCL INFO comm 0x55f06504d9a0 rank 12 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:650 [7] NCCL INFO comm 0x55f78313f880 rank 15 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:648 [0] NCCL INFO comm 0x5597108008c0 rank 8 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:649 [2] NCCL INFO comm 0x55a1d5eb1f40 rank 10 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:647 [5] NCCL INFO comm 0x558b399e6f40 rank 13 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:645 [6] NCCL INFO comm 0x55f028afe7e0 rank 14 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-07-02 08:06:14.918 algo-2:66 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-07-02 08:06:14.919 algo-2:65 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-07-02 08:06:14.919 algo-1:137 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-07-02 08:06:14.919 algo-1:136 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-07-02 08:06:14.920 algo-1:130 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-07-02 08:06:14.920 algo-1:135 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-07-02 08:06:14.920 algo-2:64 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-07-02 08:06:14.920 algo-2:68 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-07-02 08:06:14.921 algo-1:134 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-07-02 08:06:14.920 algo-2:61 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-07-02 08:06:14.921 algo-2:62 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-07-02 08:06:14.922 algo-1:131 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-07-02 08:06:14.923 algo-2:67 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-07-02 08:06:14.924 algo-1:132 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-07-02 08:06:14.924 algo-1:133 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-07-02 08:06:14.925 algo-2:63 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-07-02 08:06:15.544 algo-1:137 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-07-02 08:06:15.550 algo-2:68 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-07-02 08:06:15.550 algo-2:66 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-07-02 08:06:15.550 algo-2:61 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-07-02 08:06:15.550 algo-2:65 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-07-02 08:06:15.550 algo-2:64 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-07-02 08:06:15.551 algo-2:62 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-07-02 08:06:15.551 algo-2:67 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-07-02 08:06:15.552 algo-2:63 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-07-02 08:06:15.553 algo-1:130 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-07-02 08:06:15.564 algo-1:135 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-07-02 08:06:15.565 algo-1:136 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-07-02 08:06:15.565 algo-1:134 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-07-02 08:06:15.571 algo-1:131 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-07-02 08:06:15.574 algo-1:133 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-07-02 08:06:15.574 algo-1:132 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(warn_msg)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [0/60000 (0%)]#011Loss: 2.344856\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [5120/60000 (8%)]#011Loss: 1.276712\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [10240/60000 (17%)]#011Loss: 0.669897\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [15360/60000 (25%)]#011Loss: 0.417496\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [20480/60000 (34%)]#011Loss: 0.212154\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [25600/60000 (42%)]#011Loss: 0.281349\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [30720/60000 (51%)]#011Loss: 0.175186\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [35840/60000 (59%)]#011Loss: 0.075825\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [40960/60000 (68%)]#011Loss: 0.173057\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [46080/60000 (76%)]#011Loss: 0.053721\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [51200/60000 (85%)]#011Loss: 0.145543\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 1 [56320/60000 (93%)]#011Loss: 0.141225\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0987, Accuracy: 9680/10000 (97%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [0/60000 (0%)]#011Loss: 0.243038\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [5120/60000 (8%)]#011Loss: 0.297243\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [10240/60000 (17%)]#011Loss: 0.148834\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [15360/60000 (25%)]#011Loss: 0.108656\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [20480/60000 (34%)]#011Loss: 0.192916\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [25600/60000 (42%)]#011Loss: 0.211690\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [30720/60000 (51%)]#011Loss: 0.056550\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [35840/60000 (59%)]#011Loss: 0.024393\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [40960/60000 (68%)]#011Loss: 0.076480\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [46080/60000 (76%)]#011Loss: 0.051642\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [51200/60000 (85%)]#011Loss: 0.093499\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 2 [56320/60000 (93%)]#011Loss: 0.054820\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0583, Accuracy: 9811/10000 (98%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [0/60000 (0%)]#011Loss: 0.010093\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [5120/60000 (8%)]#011Loss: 0.183786\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [10240/60000 (17%)]#011Loss: 0.213576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [15360/60000 (25%)]#011Loss: 0.203487\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [20480/60000 (34%)]#011Loss: 0.103605\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [25600/60000 (42%)]#011Loss: 0.104350\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [30720/60000 (51%)]#011Loss: 0.059530\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [35840/60000 (59%)]#011Loss: 0.017711\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [40960/60000 (68%)]#011Loss: 0.146599\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [46080/60000 (76%)]#011Loss: 0.035518\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [51200/60000 (85%)]#011Loss: 0.108635\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 3 [56320/60000 (93%)]#011Loss: 0.053732\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0451, Accuracy: 9854/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [0/60000 (0%)]#011Loss: 0.094734\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [5120/60000 (8%)]#011Loss: 0.161684\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [10240/60000 (17%)]#011Loss: 0.148155\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [15360/60000 (25%)]#011Loss: 0.096701\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [20480/60000 (34%)]#011Loss: 0.299750\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [25600/60000 (42%)]#011Loss: 0.234459\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [30720/60000 (51%)]#011Loss: 0.010419\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [35840/60000 (59%)]#011Loss: 0.046116\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [40960/60000 (68%)]#011Loss: 0.085132\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [46080/60000 (76%)]#011Loss: 0.005638\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [51200/60000 (85%)]#011Loss: 0.016693\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 4 [56320/60000 (93%)]#011Loss: 0.056344\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0384, Accuracy: 9868/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [0/60000 (0%)]#011Loss: 0.063996\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [5120/60000 (8%)]#011Loss: 0.091391\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [10240/60000 (17%)]#011Loss: 0.104908\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [15360/60000 (25%)]#011Loss: 0.216393\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [20480/60000 (34%)]#011Loss: 0.078329\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [25600/60000 (42%)]#011Loss: 0.043617\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [30720/60000 (51%)]#011Loss: 0.013658\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [35840/60000 (59%)]#011Loss: 0.020128\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [40960/60000 (68%)]#011Loss: 0.060294\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [46080/60000 (76%)]#011Loss: 0.041362\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [51200/60000 (85%)]#011Loss: 0.020753\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Train Epoch: 5 [56320/60000 (93%)]#011Loss: 0.052103\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:654 [1] NCCL INFO [Service thread] Connection closed by localRank 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:657 [5] NCCL INFO [Service thread] Connection closed by localRank 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:658 [4] NCCL INFO [Service thread] Connection closed by localRank 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:66:66 [5] NCCL INFO comm 0x558b399e6f40 rank 13 nranks 16 cudaDev 5 busId 1c0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:729 [3] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:725 [1] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:730 [7] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:133:133 [3] NCCL INFO comm 0x55d43cd34900 rank 3 nranks 16 cudaDev 3 busId 1a0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:659 [0] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:658 [4] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:660 [6] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:656 [3] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:655 [7] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:660 [6] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:654 [1] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:653 [2] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:659 [0] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:654 [1] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:659 [0] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:656 [3] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:654 [1] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:660 [6] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:65:65 [4] NCCL INFO comm 0x55f06504d9a0 rank 12 nranks 16 cudaDev 4 busId 1b0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:68:68 [7] NCCL INFO comm 0x55f78313f880 rank 15 nranks 16 cudaDev 7 busId 1e0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:730 [7] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:731 [5] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:727 [4] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:654 [1] NCCL INFO [Service thread] Connection closed by localRank 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:63:63 [2] NCCL INFO comm 0x55a1d5eb1f40 rank 10 nranks 16 cudaDev 2 busId 190 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO [Service thread] Connection closed by localRank 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:731 [5] NCCL INFO [Service thread] Connection closed by localRank 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:725 [1] NCCL INFO [Service thread] Connection closed by localRank 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:727 [4] NCCL INFO [Service thread] Connection closed by localRank 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:731 [5] NCCL INFO [Service thread] Connection closed by localRank 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:137:137 [7] NCCL INFO comm 0x55f964c8fca0 rank 7 nranks 16 cudaDev 7 busId 1e0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:64:64 [3] NCCL INFO comm 0x55a82fe43720 rank 11 nranks 16 cudaDev 3 busId 1a0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:727 [4] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:724 [6] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:67:67 [6] NCCL INFO comm 0x55f028afe7e0 rank 14 nranks 16 cudaDev 6 busId 1d0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:728 [2] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:724 [6] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:728 [2] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:61:61 [0] NCCL INFO comm 0x5597108008c0 rank 8 nranks 16 cudaDev 0 busId 170 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:62:62 [1] NCCL INFO comm 0x565127248780 rank 9 nranks 16 cudaDev 1 busId 180 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:131:131 [1] NCCL INFO comm 0x564dfc0cc6a0 rank 1 nranks 16 cudaDev 1 busId 180 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:135:135 [5] NCCL INFO comm 0x564130ab42c0 rank 5 nranks 16 cudaDev 5 busId 1c0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:136:136 [6] NCCL INFO comm 0x5652f20f1700 rank 6 nranks 16 cudaDev 6 busId 1d0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:132:132 [2] NCCL INFO comm 0x55effcba4580 rank 2 nranks 16 cudaDev 2 busId 190 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:134:134 [4] NCCL INFO comm 0x55d8e24abb40 rank 4 nranks 16 cudaDev 4 busId 1b0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Test set: Average loss: 0.0340, Accuracy: 9883/10000 (99%)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:726 [0] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:130:130 [0] NCCL INFO comm 0x558731238160 rank 0 nranks 16 cudaDev 0 busId 170 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:37,957 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:37,957 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:37,957 sagemaker-training-toolkit INFO     Begin writing status file from leader node to worker nodes (if any)\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:37,958 sagemaker-training-toolkit INFO     Start writing mpirun finished status to algo-2\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:38,113 sagemaker-training-toolkit INFO     output from subprocess run CompletedProcess(args=['ssh', 'algo-2', 'touch', '/tmp/done.algo-1'], returncode=0, stdout='', stderr='')\u001b[0m\n",
      "\u001b[34m2023-07-02 08:06:38,113 sagemaker-training-toolkit INFO     Finished writing status file\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:37,984 sagemaker-training-toolkit INFO     Invoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:37,984 sagemaker-training-toolkit INFO     process psutil.Process(pid=58, name='orted', status='terminated', started='08:06:05') terminated with exit code None\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:37,985 sagemaker-training-toolkit INFO     Reporting status for ORTEd process. gone: [psutil.Process(pid=58, name='orted', status='terminated', started='08:06:05')] alive: []\u001b[0m\n",
      "\u001b[35m2023-07-02 08:06:37,985 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34m2023-07-02 08:07:08,143 sagemaker-training-toolkit INFO     Finished writing status file from leader node to worker nodes (if any)\u001b[0m\n",
      "\u001b[34m2023-07-02 08:07:08,144 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2023-07-02 08:07:08,015 sagemaker-training-toolkit INFO     Begin looking for status file on algo-2\u001b[0m\n",
      "\u001b[35m2023-07-02 08:07:08,015 sagemaker-training-toolkit INFO     MPI training job status file found. Exit gracefully\u001b[0m\n",
      "\u001b[35m2023-07-02 08:07:08,015 sagemaker-training-toolkit INFO     End looking for status file\u001b[0m\n",
      "\u001b[35m2023-07-02 08:07:08,015 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2023-07-02 08:07:08,015 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-07-02 08:07:20 Uploading - Uploading generated training model\n",
      "2023-07-02 08:07:20 Completed - Training job completed\n",
      "Training seconds: 696\n",
      "Billable seconds: 696\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
