{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4_training_alpaca_deepspeed\n",
    "\n",
    "- https://github.com/tatsu-lab/stanford_alpaca.git 에서 파일을 다운로드 받은 후 4_training_alpaca_deepspeed 폴더의 코드를 덮어쓰기하여 학습을 하시면 됩니다.\n",
    "- 시작 전 docker 폴더에서 Dockerfile을 이용하여 custom_docker를 만들어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = True\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already revised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 24.5M  100 24.5M    0     0  59.6M      0 --:--:-- --:--:-- --:--:-- 59.6M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "DAEMON_PATH=\"/etc/docker\"\n",
    "MEMORY_SIZE=10G\n",
    "\n",
    "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# echo $FLAG\n",
    "\n",
    "if [ \"$FLAG\" == true ]; then\n",
    "    echo \"Already revised\"\n",
    "else\n",
    "    echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "    sudo service docker restart\n",
    "    echo \"Docker Restart\"\n",
    "fi\n",
    "\n",
    "sudo curl -L \"https://github.com/docker/compose/releases/download/v2.7.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n",
    "sudo chmod +x /usr/local/bin/docker-compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing deps and restarting kernel\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install --upgrade pip --quiet\n",
    "    !{sys.executable} -m pip install -U sagemaker --quiet\n",
    "    !{sys.executable} -m pip install -U transformers accelerate datasets --quiet\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and registering the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  9.728kB\n",
      "Step 1/9 : FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.0.0-gpu-py310\n",
      " ---> 2cf7a91d14ca\n",
      "Step 2/9 : RUN pip install --no-cache-dir rouge_score     && pip install --no-cache-dir fire     && pip install --no-cache-dir sentencepiece     && pip install --no-cache-dir wandb     && pip install --no-cache-dir openai\n",
      " ---> Using cache\n",
      " ---> f3806019c8a1\n",
      "Step 3/9 : RUN pip install --no-cache-dir transformers\n",
      " ---> Using cache\n",
      " ---> b84f599c35b4\n",
      "Step 4/9 : RUN pip uninstall -y accelerate     && pip install --no-cache-dir accelerate>=0.20.3\n",
      " ---> Using cache\n",
      " ---> 668be6d309a0\n",
      "Step 5/9 : RUN pip uninstall -y deepspeed\n",
      " ---> Using cache\n",
      " ---> 8941ff9a3a3f\n",
      "Step 6/9 : ARG DS_BUILD_OPS=0\n",
      " ---> Using cache\n",
      " ---> 10a00e2b06f0\n",
      "Step 7/9 : ARG DS_BUILD_FUSED_ADAM=1\n",
      " ---> Using cache\n",
      " ---> 9841970bc8d6\n",
      "Step 8/9 : RUN mkdir -p /tmp &&     cd /tmp &&     git clone https://github.com/microsoft/DeepSpeed.git &&     cd DeepSpeed &&     pip install -r requirements/requirements-dev.txt &&     pip install -r requirements/requirements.txt &&     DS_BUILD_OPS=$DS_BUILD_OPS DS_BUILD_FUSED_ADAM=$DS_BUILD_FUSED_ADAM pip install -v . &&     cd .. &&     rm -rf DeepSpeed\n",
      " ---> Using cache\n",
      " ---> d4ac9ebebbfd\n",
      "Step 9/9 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 7ff1c3f790ef\n",
      "Successfully built 7ff1c3f790ef\n",
      "Successfully tagged 322537213286.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:alpaca\n",
      "The push refers to repository [322537213286.dkr.ecr.us-west-2.amazonaws.com/pytorch-training]\n",
      "f41dd87dcfa5: Preparing\n",
      "0281bc6e8a10: Preparing\n",
      "ce94c78f4508: Preparing\n",
      "651e9d95cbe3: Preparing\n",
      "7419363418a1: Preparing\n",
      "0a86d2f63da9: Preparing\n",
      "e9149126e47f: Preparing\n",
      "81bcaebf20b7: Preparing\n",
      "b89ba47ef264: Preparing\n",
      "5e365e6e2026: Preparing\n",
      "30f10d0e1e2a: Preparing\n",
      "de6ad3f5baf9: Preparing\n",
      "8995be0bc275: Preparing\n",
      "7649740a6938: Preparing\n",
      "138718a88769: Preparing\n",
      "4c8ddbfabe2c: Preparing\n",
      "e11d715889d8: Preparing\n",
      "d2516bd9d454: Preparing\n",
      "ab91cb17a698: Preparing\n",
      "375dafba5be7: Preparing\n",
      "eb2d5581a4b3: Preparing\n",
      "6e5ea4d3b078: Preparing\n",
      "a83e3f8647a8: Preparing\n",
      "629205717bfa: Preparing\n",
      "91962ccfdb56: Preparing\n",
      "e42093c82aca: Preparing\n",
      "88f627f04385: Preparing\n",
      "53d4ef0348b1: Preparing\n",
      "7dec9be1e6de: Preparing\n",
      "1c442bf32dda: Preparing\n",
      "7a4317d0452c: Preparing\n",
      "569b5fc6f9ba: Preparing\n",
      "16acfff66e41: Preparing\n",
      "c2440becfb6e: Preparing\n",
      "93dc2ad27ff8: Preparing\n",
      "3b6112f80af1: Preparing\n",
      "1be54c625d9b: Preparing\n",
      "5d4d8e450a3a: Preparing\n",
      "aed2d71a436d: Preparing\n",
      "0a86d2f63da9: Waiting\n",
      "e9149126e47f: Waiting\n",
      "7af37e3e56a9: Preparing\n",
      "e5167e76bf1b: Preparing\n",
      "a490a70ab1cd: Preparing\n",
      "b3c248c52364: Preparing\n",
      "b89ba47ef264: Waiting\n",
      "81bcaebf20b7: Waiting\n",
      "d543b8cad89e: Preparing\n",
      "629205717bfa: Waiting\n",
      "93dc2ad27ff8: Waiting\n",
      "53d4ef0348b1: Waiting\n",
      "16acfff66e41: Waiting\n",
      "3b6112f80af1: Waiting\n",
      "1c442bf32dda: Waiting\n",
      "7dec9be1e6de: Waiting\n",
      "5d4d8e450a3a: Waiting\n",
      "5e365e6e2026: Waiting\n",
      "aed2d71a436d: Waiting\n",
      "de6ad3f5baf9: Waiting\n",
      "eb2d5581a4b3: Waiting\n",
      "8995be0bc275: Waiting\n",
      "a83e3f8647a8: Waiting\n",
      "7649740a6938: Waiting\n",
      "30f10d0e1e2a: Waiting\n",
      "138718a88769: Waiting\n",
      "4c8ddbfabe2c: Waiting\n",
      "e5167e76bf1b: Waiting\n",
      "a490a70ab1cd: Waiting\n",
      "d543b8cad89e: Waiting\n",
      "c2440becfb6e: Waiting\n",
      "ab91cb17a698: Waiting\n",
      "88f627f04385: Waiting\n",
      "91962ccfdb56: Waiting\n",
      "7af37e3e56a9: Waiting\n",
      "6e5ea4d3b078: Waiting\n",
      "e11d715889d8: Waiting\n",
      "b3c248c52364: Waiting\n",
      "0281bc6e8a10: Layer already exists\n",
      "f41dd87dcfa5: Layer already exists\n",
      "651e9d95cbe3: Layer already exists\n",
      "7419363418a1: Layer already exists\n",
      "ce94c78f4508: Layer already exists\n",
      "0a86d2f63da9: Layer already exists\n",
      "e9149126e47f: Layer already exists\n",
      "81bcaebf20b7: Layer already exists\n",
      "b89ba47ef264: Layer already exists\n",
      "5e365e6e2026: Layer already exists\n",
      "30f10d0e1e2a: Layer already exists\n",
      "de6ad3f5baf9: Layer already exists\n",
      "8995be0bc275: Layer already exists\n",
      "7649740a6938: Layer already exists\n",
      "138718a88769: Layer already exists\n",
      "4c8ddbfabe2c: Layer already exists\n",
      "e11d715889d8: Layer already exists\n",
      "375dafba5be7: Layer already exists\n",
      "ab91cb17a698: Layer already exists\n",
      "d2516bd9d454: Layer already exists\n",
      "eb2d5581a4b3: Layer already exists\n",
      "6e5ea4d3b078: Layer already exists\n",
      "a83e3f8647a8: Layer already exists\n",
      "629205717bfa: Layer already exists\n",
      "91962ccfdb56: Layer already exists\n",
      "e42093c82aca: Layer already exists\n",
      "88f627f04385: Layer already exists\n",
      "7dec9be1e6de: Layer already exists\n",
      "53d4ef0348b1: Layer already exists\n",
      "1c442bf32dda: Layer already exists\n",
      "7a4317d0452c: Layer already exists\n",
      "569b5fc6f9ba: Layer already exists\n",
      "c2440becfb6e: Layer already exists\n",
      "16acfff66e41: Layer already exists\n",
      "93dc2ad27ff8: Layer already exists\n",
      "3b6112f80af1: Layer already exists\n",
      "1be54c625d9b: Layer already exists\n",
      "5d4d8e450a3a: Layer already exists\n",
      "aed2d71a436d: Layer already exists\n",
      "e5167e76bf1b: Layer already exists\n",
      "7af37e3e56a9: Layer already exists\n",
      "a490a70ab1cd: Layer already exists\n",
      "b3c248c52364: Layer already exists\n",
      "d543b8cad89e: Layer already exists\n",
      "alpaca: digest: sha256:98ce876798040cd52d480312275bf7e2770fec834030220c72b4f1d8a58fa69c size: 9579\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=pytorch-training\n",
    "image_tag=alpaca\n",
    "\n",
    "cd '4_training_alpaca_deepspeed/docker'\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:${image_tag}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -f Dockerfile -t ${fullname} .\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/alpaca'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.178.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure FSx Input for your SageMaker Training job\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "file_system_directory_path= '/alpaca_cache' #\n",
    "file_system_id='fs-XXXXXXXXXXXXXXX'         # \n",
    "file_system_access_mode='ro'\n",
    "file_system_type='EFS'\n",
    "model_cache_fs=FileSystemInput(file_system_id=file_system_id,\n",
    "                         file_system_type=file_system_type,\n",
    "                         directory_path=file_system_directory_path,\n",
    "                         file_system_access_mode=file_system_access_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/home/ec2-user/SageMaker/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 'facebook/opt-125m'\n",
    "cache_dir = f'{Path.cwd()}/alpaca_cache'\n",
    "\n",
    "model_max_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    cache_dir=cache_dir,\n",
    "    model_max_length=model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training with Distributed Data Parallel\n",
    "\n",
    "\n",
    "The training script provides the code you need for distributed data parallel (DDP) training. The training script is very similar to a PyTorch training script you might run outside of SageMaker.\n",
    "\n",
    "In the following code block, you can update the estimator function to use a different instance type, instance count, and distrubtion strategy. You're also passing in the training script you reviewed in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'model_name_or_path' : 'facebook/opt-125m' ,\n",
    "    'data_path' : '/opt/ml/input/data/training/alpaca_data.json',\n",
    "    'bf16' : False,  ## p4d 이상 일때만 가능\n",
    "    'output_dir' : '/opt/ml/model',\n",
    "    'num_train_epochs' : 1,\n",
    "    'per_device_train_batch_size' : 4,\n",
    "    'per_device_eval_batch_size' : 4,\n",
    "    'gradient_accumulation_steps' : 8,\n",
    "    'evaluation_strategy' : 'no',\n",
    "    'save_strategy' : 'steps',\n",
    "    'save_steps' : 2000,\n",
    "    'save_total_limit' : 1,\n",
    "    'learning_rate' : 2e-5,\n",
    "    'weight_decay' : 0.,\n",
    "    'warmup_ratio' : 0.03,\n",
    "    'deepspeed' : \"/opt/ml/code/configs/default_offload_opt_param.json\",\n",
    "    'tf32' : False,  ## p4d 이상 일때만 가능\n",
    "    'cache_dir' : '/opt/ml/input/data/cache_dir',\n",
    "    'report_to' : 'none',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distribution = {}\n",
    "distribution[\"mpi\"]={\"enabled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instance_type = 'ml.p3.16xlarge'  # 'ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'local_gpu'\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "# instance_type = 'local_gpu'\n",
    "instance_count = 1\n",
    "max_run = 2*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://dataset-us-west-2-cyj/alpaca-dataset'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if instance_type =='local_gpu':\n",
    "    from sagemaker.local import LocalSession\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    s3_data_path = f'file://{Path.cwd()}/dataset'\n",
    "    model_cache = f'file://{Path.cwd()}/alpaca_cache'\n",
    "\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    s3_data_path = \"s3://dataset-us-west-2-cyj/alpaca-dataset\"\n",
    "    model_cache = model_cache_fs\n",
    "s3_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Pytorch` estimator를 이용한 training job 생성하기\n",
    "\n",
    "\n",
    "<p><strong><code>sagemaker.pytorch.PyTorch</code></strong> estimator는 처음 실행하는 스크립트 위치와 다양한 연계 코드들이 위치한 디렉토리 정보를 찾아서 스크립트를 S3에 upload하고 SageMaker의 training job을 수행하게 됩니다. training job은 학습을 수행한 단위입니다. 학습을 1번 돌리면 training job이 1개 생성됩니다. 몇 가지 중요 파라미터를 아래와 같이 설명드립니다. </p>\n",
    "\n",
    "- **entry_point** : 학습을 처음 실행하는 Python 소스 파일의 절대 또는 상대 경로이며, source_dir이 지정된 경우 entry_point는 source_dir 내 파일이 됩니다.\n",
    "- **source_dir** : 학습에 연계되는 다양한 소스코드 파일이 들어 있는 디렉토리 위치이며, 절대, 상대 경로 또는 S3 URI가 모두 가능하며,source_dir이 S3 URI 인 경우 tar.gz 파일이 됩니다.\n",
    "- **role** : Amazon SageMaker가 사용자를 대신해 작업(예: S3 버킷에서 모델 결과물이라고 하는 훈련 결과 읽기 및 Amazon S3에 훈련 결과 쓰기)을 수행하는 AWS Identity and Access Management(IAM) 역할입니다.\n",
    "- **train_instance_count** : 학습을 수행하는 instance 개수를 정의할 수 있습니다.\n",
    "- **train_instance_type** : 학습을 수행하는 instance 타입을 정의할 수 있습니다.\n",
    "- **train_volume_size** : 학습 인스턴스에 연결할 Amazon Elastic Block Store(Amazon EBS) 스토리지 볼륨의 크기(GB)입니다. File 모드를 사용할 경우 이 값이 훈련 데이터를 충분히 저장할 수 있는 크기여야 합니다(File 모드가 기본값)\n",
    "- **train_max_run** : 최대 학습 시간을 설정할 수 있으며, 이 시간이 지나면 Amazon SageMaker는 현재 상태에 관계없이 작업을 종료합니다. (기본값 : 24 * 60 * 60)\n",
    "- **framework_version** : 학습에 사용될 특정 Pytorch 버전을 정의할 수 있습니다.\n",
    "- **py_version** : 컨테이너 환경이 python3일 경우 py3, python2일 경우 py2로 설정하면 됩니다. python2는 지원이 중단되었지만 기존 python2로 구성된 파일들을 지원하기 위해 현재 계속 사용할 수 있습니다. 없을 경우에는 기본적으로 py3 입니다.\n",
    "- **hyperparameters** : 학습에 사용할 하이퍼 파라미터를 정의할 수 있으며, 정의된 하이퍼 파라미터 값들은 모두 학습 컨테이너로 전송이 됩니다.\n",
    "- **distribution** : 분산과 관련된 값들을 학습 컨테이너로 전송합니다.\n",
    "\n",
    "<p> 추가적으로 분산/ 멀티 GPU 학습도 가능합니다. SageMaker는 <strong><a href=\"https://github.com/horovod/horovod\" target=\"_blank\" class ='btn-default'>Horovod</a></strong>에 최적화된 환경을 제공하고 있으며, Pytorch의 경우 1.5.0부터 기본 docker에서 apex를 지원합니다.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = f'{accound_id}.dkr.ecr.{region_name}.amazonaws.com/pytorch-training:alpaca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "                    entry_point='train.py',\n",
    "                    source_dir=f'{Path.cwd()}/stanford_alpaca',\n",
    "                    role=role,\n",
    "                    image_uri=image_uri,\n",
    "                    instance_count=instance_count,\n",
    "                    instance_type=instance_type,\n",
    "                    distribution=distribution,\n",
    "                    disable_profiler=True,\n",
    "                    debugger_hook_config=False,\n",
    "                    max_run=max_run,\n",
    "                    hyperparameters=hyperparameters,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    subnets= ['subnet-XXXXXXXXXXXXXX'], # public subnet 가능\n",
    "                    security_group_ids=['sg-XXXXXXXXXXXXXX']\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo rm -rf code/core.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: megatron-lm-ml-p3-16xlarge-0819-09331692437593\n"
     ]
    }
   ],
   "source": [
    "current_time = strftime(\"%m%d-%H%M%s\")\n",
    "i_type = instance_type.replace('.','-')\n",
    "job_name = f'megatron-lm-{i_type}-{current_time}'\n",
    "\n",
    "estimator.fit(\n",
    "    inputs={'training': s3_data_path, 'cache_dir': model_cache}, \n",
    "    job_name=job_name,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-19 09:10:54 Starting - Starting the training job\n",
      "2023-08-19 09:10:54 Pending - Preparing the instances for training\n",
      "2023-08-19 09:10:54 Downloading - Downloading input data\n",
      "2023-08-19 09:10:54 Training - Training image download completed. Training in progress.\n",
      "2023-08-19 09:10:54 Uploading - Uploading generated training model\n",
      "2023-08-19 09:10:54 Completed - Training job completed\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:04,346 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:04,402 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:04,410 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:04,412 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:04,293 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:04,347 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:04,355 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:04,356 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:06,953 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:07,018 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:07,026 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:07,026 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:07,042 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:07,042 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.113.90.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:06,969 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:07,033 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:07,042 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:07,042 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:07,042 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:07,070 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:07,070 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.85.90.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:08,075 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:08,075 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.85.90.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:08,100 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:08,226 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:08,227 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:08,227 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:08,227 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:08,229 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:09,098 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:09,229 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:09,230 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:09,230 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:09,230 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:8', 'algo-2:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:09,230 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:09,297 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:09,380 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:09,388 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"cache_dir\": \"/opt/ml/input/data/cache_dir\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"cache_dir\": \"/opt/ml/input/data/cache_dir\",\n",
      "        \"data_path\": \"/opt/ml/input/data/training/alpaca_data.json\",\n",
      "        \"deepspeed\": \"/opt/ml/code/configs/default_offload_opt_param.json\",\n",
      "        \"evaluation_strategy\": \"no\",\n",
      "        \"gradient_accumulation_steps\": 8,\n",
      "        \"learning_rate\": 2e-05,\n",
      "        \"model_name_or_path\": \"facebook/opt-125m\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"report_to\": \"none\",\n",
      "        \"save_steps\": 2000,\n",
      "        \"save_strategy\": \"steps\",\n",
      "        \"save_total_limit\": 1,\n",
      "        \"tf32\": true,\n",
      "        \"warmup_ratio\": 0.03,\n",
      "        \"weight_decay\": 0.0\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"cache_dir\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"megatron-lm-ml-p4d-24xlarge-0819-08511692435093\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-322537213286/megatron-lm-ml-p4d-24xlarge-0819-08511692435093/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"cache_dir\":\"/opt/ml/input/data/cache_dir\",\"data_path\":\"/opt/ml/input/data/training/alpaca_data.json\",\"deepspeed\":\"/opt/ml/code/configs/default_offload_opt_param.json\",\"evaluation_strategy\":\"no\",\"gradient_accumulation_steps\":8,\"learning_rate\":2e-05,\"model_name_or_path\":\"facebook/opt-125m\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"report_to\":\"none\",\"save_steps\":2000,\"save_strategy\":\"steps\",\"save_total_limit\":1,\"tf32\":true,\"warmup_ratio\":0.03,\"weight_decay\":0.0}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"cache_dir\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"cache_dir\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-322537213286/megatron-lm-ml-p4d-24xlarge-0819-08511692435093/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true},\"channel_input_dirs\":{\"cache_dir\":\"/opt/ml/input/data/cache_dir\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"bf16\":true,\"cache_dir\":\"/opt/ml/input/data/cache_dir\",\"data_path\":\"/opt/ml/input/data/training/alpaca_data.json\",\"deepspeed\":\"/opt/ml/code/configs/default_offload_opt_param.json\",\"evaluation_strategy\":\"no\",\"gradient_accumulation_steps\":8,\"learning_rate\":2e-05,\"model_name_or_path\":\"facebook/opt-125m\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"report_to\":\"none\",\"save_steps\":2000,\"save_strategy\":\"steps\",\"save_total_limit\":1,\"tf32\":true,\"warmup_ratio\":0.03,\"weight_decay\":0.0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"cache_dir\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"megatron-lm-ml-p4d-24xlarge-0819-08511692435093\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/megatron-lm-ml-p4d-24xlarge-0819-08511692435093/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--cache_dir\",\"/opt/ml/input/data/cache_dir\",\"--data_path\",\"/opt/ml/input/data/training/alpaca_data.json\",\"--deepspeed\",\"/opt/ml/code/configs/default_offload_opt_param.json\",\"--evaluation_strategy\",\"no\",\"--gradient_accumulation_steps\",\"8\",\"--learning_rate\",\"2e-05\",\"--model_name_or_path\",\"facebook/opt-125m\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\",\"--report_to\",\"none\",\"--save_steps\",\"2000\",\"--save_strategy\",\"steps\",\"--save_total_limit\",\"1\",\"--tf32\",\"True\",\"--warmup_ratio\",\"0.03\",\"--weight_decay\",\"0.0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CACHE_DIR=/opt/ml/input/data/cache_dir\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_CACHE_DIR=/opt/ml/input/data/cache_dir\u001b[0m\n",
      "\u001b[34mSM_HP_DATA_PATH=/opt/ml/input/data/training/alpaca_data.json\u001b[0m\n",
      "\u001b[34mSM_HP_DEEPSPEED=/opt/ml/code/configs/default_offload_opt_param.json\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=no\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=facebook/opt-125m\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=none\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=2000\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=1\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.10/site-packages/gethostname.cpython-310-x86_64-linux-gnu.so -x FI_PROVIDER=efa -x NCCL_PROTO=simple -x FI_EFA_USE_DEVICE_RDMA=1 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_NUM_NEURONS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_CACHE_DIR -x SM_CHANNEL_TRAINING -x SM_HP_BF16 -x SM_HP_CACHE_DIR -x SM_HP_DATA_PATH -x SM_HP_DEEPSPEED -x SM_HP_EVALUATION_STRATEGY -x SM_HP_GRADIENT_ACCUMULATION_STEPS -x SM_HP_LEARNING_RATE -x SM_HP_MODEL_NAME_OR_PATH -x SM_HP_NUM_TRAIN_EPOCHS -x SM_HP_OUTPUT_DIR -x SM_HP_PER_DEVICE_EVAL_BATCH_SIZE -x SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE -x SM_HP_REPORT_TO -x SM_HP_SAVE_STEPS -x SM_HP_SAVE_STRATEGY -x SM_HP_SAVE_TOTAL_LIMIT -x SM_HP_TF32 -x SM_HP_WARMUP_RATIO -x SM_HP_WEIGHT_DECAY -x PYTHONPATH /opt/conda/bin/python3.10 -m mpi4py train.py --bf16 True --cache_dir /opt/ml/input/data/cache_dir --data_path /opt/ml/input/data/training/alpaca_data.json --deepspeed /opt/ml/code/configs/default_offload_opt_param.json --evaluation_strategy no --gradient_accumulation_steps 8 --learning_rate 2e-05 --model_name_or_path facebook/opt-125m --num_train_epochs 1 --output_dir /opt/ml/model --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --report_to none --save_steps 2000 --save_strategy steps --save_total_limit 1 --tf32 True --warmup_ratio 0.03 --weight_decay 0.0\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:09,469 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-19 09:04:09,504 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.85.90' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34mData for JOB [41126,1] offset 0 Total slots allocated 16\u001b[0m\n",
      "\u001b[34m========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [41126,1] App: 0 Process rank: 15 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:10,233 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=65, name='orted', status='sleeping', started='09:04:09')]\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:10,233 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=65, name='orted', status='sleeping', started='09:04:09')]\u001b[0m\n",
      "\u001b[35m2023-08-19 09:04:10,234 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=65, name='orted', status='sleeping', started='09:04:09')]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-08-19 09:04:17,955] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-08-19 09:04:17,988] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-08-19 09:04:18,004] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-08-19 09:04:18,004] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-08-19 09:04:18,004] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-08-19 09:04:18,017] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-08-19 09:04:18,017] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-08-19 09:04:18,034] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-08-19 09:04:20,114] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-08-19 09:04:20,154] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-08-19 09:04:20,170] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-08-19 09:04:20,170] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-08-19 09:04:20,170] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-08-19 09:04:20,170] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-08-19 09:04:20,186] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-08-19 09:04:20,186] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-08-19 09:04:21,185] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-08-19 09:04:21,241] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-08-19 09:04:21,251] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-08-19 09:04:21,295] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-08-19 09:04:21,340] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-08-19 09:04:21,345] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-08-19 09:04:21,369] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-08-19 09:04:21,417] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-08-19 09:04:23,400] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-08-19 09:04:23,451] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-08-19 09:04:23,452] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-08-19 09:04:23,453] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-08-19 09:04:23,471] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-08-19 09:04:23,471] [INFO] [comm.py:662:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-08-19 09:04:23,484] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-08-19 09:04:23,529] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-08-19 09:04:23,530] [INFO] [comm.py:631:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:69 [0] NCCL INFO Bootstrap : Using eth0:10.0.113.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:69 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:69 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:69 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:70 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:73 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:74 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:75 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:76 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:71 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:72 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:70 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:71 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:69 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:75 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:72 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:74 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:68 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:73 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:73 [4] NCCL INFO Bootstrap : Using eth0:10.0.113.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:71 [2] NCCL INFO Bootstrap : Using eth0:10.0.113.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:75 [7] NCCL INFO Bootstrap : Using eth0:10.0.85.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:73 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:73 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:70 [1] NCCL INFO Bootstrap : Using eth0:10.0.113.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:72 [3] NCCL INFO Bootstrap : Using eth0:10.0.113.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:71 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:71 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:70 [2] NCCL INFO Bootstrap : Using eth0:10.0.85.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:70 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:70 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:72 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:72 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:75 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:75 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:70 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:70 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:74 [5] NCCL INFO Bootstrap : Using eth0:10.0.113.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:75 [6] NCCL INFO Bootstrap : Using eth0:10.0.113.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:74 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:74 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:76 [7] NCCL INFO Bootstrap : Using eth0:10.0.113.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:75 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:75 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:76 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:76 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:69 [1] NCCL INFO Bootstrap : Using eth0:10.0.85.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:71 [3] NCCL INFO Bootstrap : Using eth0:10.0.85.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:69 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:69 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:71 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:71 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:74 [6] NCCL INFO Bootstrap : Using eth0:10.0.85.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:72 [4] NCCL INFO Bootstrap : Using eth0:10.0.85.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:74 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:74 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:72 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:72 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:68 [0] NCCL INFO Bootstrap : Using eth0:10.0.85.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:68 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:68 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:73 [5] NCCL INFO Bootstrap : Using eth0:10.0.85.90<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:73 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:73 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] 0/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/-1/-1->7->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/14/-1->6->-1 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] -1/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/12/-1->4->-1 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->12 [7] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/10/-1->2->-1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->10 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 00/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 01/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 02/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 03/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 04/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 05/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 06/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 07/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->7 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->8 [5] 1/-1/-1->0->7 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] -1/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->15 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/0/-1->8->-1 [5] 9/-1/-1->8->15 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->2 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/2/-1->10->-1 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] 14/-1/-1->13->12 [7] -1/-1/-1->13->12\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] -1/-1/-1->11->10 [3] 12/-1/-1->11->10 [4] 12/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] -1/-1/-1->11->10 [7] 12/-1/-1->11->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->6 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->13 [7] 15/6/-1->14->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] 8/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->4 [3] 13/-1/-1->12->11 [4] 13/-1/-1->12->11 [5] 13/-1/-1->12->11 [6] 13/4/-1->12->-1 [7] 13/-1/-1->12->11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 01/0 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 05/0 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 03/0 : 12[901c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 07/0 : 12[901c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 03/0 : 8[101c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 00/0 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 07/0 : 8[101c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 03/0 : 7[a01d0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 03/0 : 15[a01d0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 02/0 : 13[901d0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 04/0 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 01/0 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 03/0 : 15[a01d0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 02/0 : 13[901d0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 07/0 : 15[a01d0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 00/0 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 07/0 : 7[a01d0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 01/0 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 00/0 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 01/0 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 06/0 : 13[901d0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 02/0 : 5[901d0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 07/0 : 15[a01d0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 05/0 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 06/0 : 13[901d0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 04/0 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 05/0 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 05/0 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 04/0 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 02/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 06/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 06/0 : 5[901d0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 00/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 01/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 03/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 04/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 05/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 07/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 00/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 01/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 02/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 04/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 05/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 06/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 00/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 01/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 02/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 04/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 05/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 06/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 00/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 04/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 00/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 02/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 03/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 04/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 06/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 07/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 00/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 00/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 01/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 02/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 04/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 05/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 06/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 00/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 02/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 01/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 03/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 02/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 04/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 04/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 06/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 05/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 06/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 07/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 00/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 01/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 02/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 04/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 05/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 06/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 01/0 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 05/0 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 00/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 01/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 03/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 04/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 05/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 07/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 00/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 04/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 00/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 00/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 00/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 01/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 02/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 01/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 00/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 04/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 01/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 04/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 02/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 05/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 04/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 06/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 05/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 04/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 05/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 01/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 06/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 02/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 03/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 05/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 06/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 00/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 07/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 01/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 03/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 04/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 05/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 01/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 02/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 05/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 01/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 06/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 07/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 02/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 00/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 01/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 03/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 04/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 05/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 03/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 07/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 00/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 05/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 00/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 06/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 02/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 02/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 07/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 03/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 03/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 02/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 04/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 04/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Channel 06/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 06/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 07/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 06/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 07/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 01/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 03/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 05/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 07/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 00/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Channel 04/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 01/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 02/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 03/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 01/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 05/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Channel 05/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 06/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 07/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 01/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 03/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 05/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Channel 07/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 03/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Channel 07/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:203 [0] NCCL INFO comm 0x557b3261d230 rank 8 nranks 16 cudaDev 0 busId 101c0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:204 [5] NCCL INFO comm 0x55a029883d10 rank 13 nranks 16 cudaDev 5 busId 901d0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:202 [4] NCCL INFO comm 0x55ecd9552b10 rank 12 nranks 16 cudaDev 4 busId 901c0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:197 [7] NCCL INFO comm 0x56420a114390 rank 15 nranks 16 cudaDev 7 busId a01d0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:198 [2] NCCL INFO comm 0x5592ad2d8d70 rank 10 nranks 16 cudaDev 2 busId 201c0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:200 [3] NCCL INFO comm 0x5606d9359190 rank 11 nranks 16 cudaDev 3 busId 201d0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:199 [1] NCCL INFO comm 0x5559b85b9050 rank 9 nranks 16 cudaDev 1 busId 101d0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:201 [6] NCCL INFO comm 0x55c5b4f5fab0 rank 14 nranks 16 cudaDev 6 busId a01c0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:202 [2] NCCL INFO comm 0x563e8c71d470 rank 2 nranks 16 cudaDev 2 busId 201c0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:204 [3] NCCL INFO comm 0x5639e4861260 rank 3 nranks 16 cudaDev 3 busId 201d0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:207 [7] NCCL INFO comm 0x557422d3e370 rank 7 nranks 16 cudaDev 7 busId a01d0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:205 [5] NCCL INFO comm 0x5557b46b0890 rank 5 nranks 16 cudaDev 5 busId 901d0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:201 [4] NCCL INFO comm 0x55c97f67d920 rank 4 nranks 16 cudaDev 4 busId 901c0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:200 [0] NCCL INFO comm 0x55b10a5807d0 rank 0 nranks 16 cudaDev 0 busId 101c0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:203 [1] NCCL INFO comm 0x561be5ba7220 rank 1 nranks 16 cudaDev 1 busId 101d0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:206 [6] NCCL INFO comm 0x563bc71351e0 rank 6 nranks 16 cudaDev 6 busId a01c0 commId 0xe2cbd8f9761ef478 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-08-19 09:04:27,484] [INFO] [partition_parameters.py:332:__exit__] finished initializing model - num_params = 197, num_elems = 0.16B\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:WARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Parameter Offload: Total persistent parameters: 121344 in 122 params\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  0%|          | 0/101 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/101 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 00/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 01/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 02/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 03/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 04/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 05/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 06/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/10/-1->2->-1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->10 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 07/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->7 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->8 [5] 1/-1/-1->0->7 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] 8/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->6 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->13 [7] 15/6/-1->14->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/12/-1->4->-1 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->12 [7] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/14/-1->6->-1 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] -1/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] 0/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/-1/-1->7->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] 14/-1/-1->13->12 [7] -1/-1/-1->13->12\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] -1/-1/-1->11->10 [3] 12/-1/-1->11->10 [4] 12/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] -1/-1/-1->11->10 [7] 12/-1/-1->11->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->4 [3] 13/-1/-1->12->11 [4] 13/-1/-1->12->11 [5] 13/-1/-1->12->11 [6] 13/4/-1->12->-1 [7] 13/-1/-1->12->11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] -1/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->2 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/2/-1->10->-1 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->15 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/0/-1->8->-1 [5] 9/-1/-1->8->15 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 03/0 : 12[901c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 01/0 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 07/0 : 12[901c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 05/0 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 00/0 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 03/0 : 8[101c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 07/0 : 8[101c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 03/0 : 15[a01d0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 03/0 : 7[a01d0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 01/0 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 02/0 : 13[901d0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 04/0 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 03/0 : 15[a01d0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 07/0 : 15[a01d0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 02/0 : 13[901d0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 07/0 : 7[a01d0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 02/0 : 5[901d0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 00/0 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 01/0 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 00/0 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 01/0 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 06/0 : 13[901d0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 05/0 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 07/0 : 15[a01d0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 06/0 : 5[901d0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 05/0 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 04/0 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 02/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 06/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 06/0 : 13[901d0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 04/0 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 05/0 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 00/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 01/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 02/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 04/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 05/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 06/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 00/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 00/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 01/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 03/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 00/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 01/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 04/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 03/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 01/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 04/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 05/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 02/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 00/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 07/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 04/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 01/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 05/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 05/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 02/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 07/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 06/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 04/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 05/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 06/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 00/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 02/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 00/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 03/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 01/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 04/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 02/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 06/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 04/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 05/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 07/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 06/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 00/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 01/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 02/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 04/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 05/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 06/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 00/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 01/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 02/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 04/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 05/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 01/0 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 06/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 05/0 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 00/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 02/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 00/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 03/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 04/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 04/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 06/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 07/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 01/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 00/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 02/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 03/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 04/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 05/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 06/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 07/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 01/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 02/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 03/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 00/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 05/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 00/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 06/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 01/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 07/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 03/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 04/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 00/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 04/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 05/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 02/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 07/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 00/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 01/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 00/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 02/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 03/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 04/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 04/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 05/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 06/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 07/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 00/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 01/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 02/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 04/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 05/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 06/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 00/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 03/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 04/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 01/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 06/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 03/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 07/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 04/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 05/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 02/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 07/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Channel 06/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 01/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 03/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 05/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 07/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 01/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 01/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 02/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 02/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 00/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 03/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 05/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Channel 04/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 05/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 06/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 01/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 06/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Channel 05/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 07/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 01/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 03/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 05/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Channel 07/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 03/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Channel 07/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:299 [4] NCCL INFO comm 0x7f3f7808aad0 rank 4 nranks 16 cudaDev 4 busId 901c0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:300 [2] NCCL INFO comm 0x7fd4f408aa10 rank 2 nranks 16 cudaDev 2 busId 201c0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:302 [6] NCCL INFO comm 0x7f7264090a20 rank 6 nranks 16 cudaDev 6 busId a01c0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:76:303 [7] NCCL INFO comm 0x7f6bb008a550 rank 7 nranks 16 cudaDev 7 busId a01d0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:72:304 [3] NCCL INFO comm 0x7f521008a790 rank 3 nranks 16 cudaDev 3 busId 201d0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:74:301 [5] NCCL INFO comm 0x7f78d008bad0 rank 5 nranks 16 cudaDev 5 busId 901d0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:69:298 [0] NCCL INFO comm 0x7fa52008adf0 rank 0 nranks 16 cudaDev 0 busId 101c0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:70:305 [1] NCCL INFO comm 0x7f65b49f2a70 rank 1 nranks 16 cudaDev 1 busId 101d0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:75:295 [7] NCCL INFO comm 0x7f2cc008a4b0 rank 15 nranks 16 cudaDev 7 busId a01d0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:71:297 [3] NCCL INFO comm 0x7f0f8808ab50 rank 11 nranks 16 cudaDev 3 busId 201d0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:298 [6] NCCL INFO comm 0x7ff1449f2ac0 rank 14 nranks 16 cudaDev 6 busId a01c0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:301 [2] NCCL INFO comm 0x7fa0f808a660 rank 10 nranks 16 cudaDev 2 busId 201c0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:300 [0] NCCL INFO comm 0x7f0680089a70 rank 8 nranks 16 cudaDev 0 busId 101c0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:72:296 [4] NCCL INFO comm 0x7fa4ac089b10 rank 12 nranks 16 cudaDev 4 busId 901c0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:69:294 [1] NCCL INFO comm 0x7f120008b940 rank 9 nranks 16 cudaDev 1 busId 101d0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:73:299 [5] NCCL INFO comm 0x7f5dd89f2940 rank 13 nranks 16 cudaDev 5 busId 901d0 commId 0x11a8a97350aca3b8 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 1/101 [00:05<08:20,  5.01s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  1%|          | 1/101 [00:05<08:37,  5.18s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 2/101 [00:06<05:10,  3.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  2%|▏         | 2/101 [00:07<05:17,  3.21s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 3/101 [00:08<04:03,  2.48s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  3%|▎         | 3/101 [00:08<04:06,  2.52s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 4/101 [00:10<03:30,  2.17s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  4%|▍         | 4/101 [00:10<03:32,  2.19s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▍         | 5/101 [00:11<03:11,  1.99s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  5%|▍         | 5/101 [00:12<03:12,  2.01s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 6/101 [00:13<02:59,  1.89s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  6%|▌         | 6/101 [00:13<03:00,  1.90s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 7/101 [00:15<02:50,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  7%|▋         | 7/101 [00:15<02:51,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 8/101 [00:16<02:44,  1.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  8%|▊         | 8/101 [00:17<02:45,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 9/101 [00:18<02:40,  1.74s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  9%|▉         | 9/101 [00:18<02:40,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 10/101 [00:20<02:36,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 10%|▉         | 10/101 [00:20<02:37,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 11/101 [00:21<02:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 11%|█         | 11/101 [00:22<02:33,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 12/101 [00:23<02:31,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 12%|█▏        | 12/101 [00:23<02:31,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 13/101 [00:25<02:28,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 13%|█▎        | 13/101 [00:25<02:28,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 14/101 [00:27<02:27,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 14%|█▍        | 14/101 [00:27<02:27,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 15/101 [00:28<02:24,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 15%|█▍        | 15/101 [00:28<02:24,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 16/101 [00:30<02:22,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 16%|█▌        | 16/101 [00:30<02:22,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 17/101 [00:32<02:21,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 17%|█▋        | 17/101 [00:32<02:21,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 18/101 [00:33<02:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 18%|█▊        | 18/101 [00:33<02:19,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 19/101 [00:35<02:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 19%|█▉        | 19/101 [00:35<02:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 20/101 [00:37<02:16,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 20%|█▉        | 20/101 [00:37<02:16,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 21/101 [00:38<02:14,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 21%|██        | 21/101 [00:38<02:14,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 22/101 [00:40<02:13,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 22%|██▏       | 22/101 [00:40<02:13,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 23/101 [00:42<02:11,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 23%|██▎       | 23/101 [00:42<02:11,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 24/101 [00:43<02:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 24%|██▍       | 24/101 [00:44<02:09,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 25/101 [00:45<02:07,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 25%|██▍       | 25/101 [00:45<02:07,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 26/101 [00:47<02:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 26%|██▌       | 26/101 [00:47<02:06,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 27/101 [00:48<02:04,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 27%|██▋       | 27/101 [00:49<02:04,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 28/101 [00:50<02:02,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 28%|██▊       | 28/101 [00:50<02:02,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▊       | 29/101 [00:52<02:00,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 29%|██▊       | 29/101 [00:52<02:00,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 30/101 [00:53<01:59,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 30%|██▉       | 30/101 [00:54<01:59,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 31/101 [00:55<01:58,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 31%|███       | 31/101 [00:55<01:58,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 32/101 [00:57<01:56,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 32%|███▏      | 32/101 [00:57<01:56,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 33/101 [00:59<01:54,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 33%|███▎      | 33/101 [00:59<01:54,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▎      | 34/101 [01:00<01:52,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 34%|███▎      | 34/101 [01:00<01:52,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▍      | 35/101 [01:02<01:51,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 35%|███▍      | 35/101 [01:02<01:51,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 36/101 [01:04<01:49,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 36%|███▌      | 36/101 [01:04<01:49,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 37/101 [01:05<01:47,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 37%|███▋      | 37/101 [01:05<01:47,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 38/101 [01:07<01:45,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 38%|███▊      | 38/101 [01:07<01:45,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▊      | 39/101 [01:09<01:44,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 39%|███▊      | 39/101 [01:09<01:44,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 40/101 [01:10<01:42,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 40%|███▉      | 40/101 [01:10<01:42,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 41/101 [01:12<01:40,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 41%|████      | 41/101 [01:12<01:40,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 42/101 [01:14<01:39,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 42%|████▏     | 42/101 [01:14<01:39,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 43/101 [01:15<01:40,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 43%|████▎     | 43/101 [01:16<01:40,  1.73s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▎     | 44/101 [01:17<01:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 44%|████▎     | 44/101 [01:17<01:37,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▍     | 45/101 [01:19<01:35,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 45%|████▍     | 45/101 [01:19<01:35,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 46/101 [01:20<01:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 46%|████▌     | 46/101 [01:21<01:33,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 47/101 [01:22<01:31,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 47%|████▋     | 47/101 [01:22<01:31,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 48/101 [01:24<01:29,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 48%|████▊     | 48/101 [01:24<01:29,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▊     | 49/101 [01:26<01:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 49%|████▊     | 49/101 [01:26<01:27,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 50/101 [01:27<01:25,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|████▉     | 50/101 [01:27<01:25,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 51/101 [01:29<01:24,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|█████     | 51/101 [01:29<01:24,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████▏    | 52/101 [01:31<01:22,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 51%|█████▏    | 52/101 [01:31<01:22,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 53/101 [01:32<01:20,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 52%|█████▏    | 53/101 [01:32<01:20,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 54/101 [01:34<01:18,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 53%|█████▎    | 54/101 [01:34<01:18,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 55/101 [01:36<01:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 54%|█████▍    | 55/101 [01:36<01:17,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 56/101 [01:37<01:15,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 55%|█████▌    | 56/101 [01:37<01:15,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▋    | 57/101 [01:39<01:14,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 56%|█████▋    | 57/101 [01:39<01:14,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 58/101 [01:41<01:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 57%|█████▋    | 58/101 [01:41<01:13,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 59/101 [01:42<01:11,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 58%|█████▊    | 59/101 [01:43<01:11,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 60/101 [01:44<01:09,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 59%|█████▉    | 60/101 [01:44<01:09,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 61/101 [01:46<01:07,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 60%|██████    | 61/101 [01:46<01:07,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████▏   | 62/101 [01:47<01:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 61%|██████▏   | 62/101 [01:48<01:05,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 63/101 [01:49<01:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 62%|██████▏   | 63/101 [01:49<01:03,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 64/101 [01:51<01:02,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 63%|██████▎   | 64/101 [01:51<01:02,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 65/101 [01:52<01:00,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 64%|██████▍   | 65/101 [01:53<01:00,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 66/101 [01:54<00:58,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 65%|██████▌   | 66/101 [01:54<00:58,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▋   | 67/101 [01:56<00:57,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 66%|██████▋   | 67/101 [01:56<00:57,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 68/101 [01:58<00:55,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 67%|██████▋   | 68/101 [01:58<00:55,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 69/101 [01:59<00:53,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 68%|██████▊   | 69/101 [01:59<00:53,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 70/101 [02:01<00:51,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 69%|██████▉   | 70/101 [02:01<00:51,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 71/101 [02:03<00:50,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 70%|███████   | 71/101 [02:03<00:50,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████▏  | 72/101 [02:04<00:48,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 71%|███████▏  | 72/101 [02:04<00:48,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 73/101 [02:06<00:51,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 72%|███████▏  | 73/101 [02:07<00:51,  1.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 74/101 [02:08<00:47,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 73%|███████▎  | 74/101 [02:08<00:47,  1.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 75/101 [02:10<00:45,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|███████▍  | 75/101 [02:10<00:45,  1.75s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 76/101 [02:11<00:43,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 75%|███████▌  | 76/101 [02:12<00:43,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 77/101 [02:13<00:41,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 76%|███████▌  | 77/101 [02:13<00:41,  1.71s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 78/101 [02:15<00:39,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 77%|███████▋  | 78/101 [02:15<00:39,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 79/101 [02:16<00:37,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 78%|███████▊  | 79/101 [02:17<00:37,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 80/101 [02:18<00:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 79%|███████▉  | 80/101 [02:18<00:35,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 81/101 [02:20<00:33,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 80%|████████  | 81/101 [02:20<00:33,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 82/101 [02:21<00:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 81%|████████  | 82/101 [02:22<00:31,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 83/101 [02:23<00:30,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 82%|████████▏ | 83/101 [02:23<00:30,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 84/101 [02:25<00:28,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 83%|████████▎ | 84/101 [02:25<00:28,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 85/101 [02:26<00:26,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 84%|████████▍ | 85/101 [02:27<00:26,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 86/101 [02:28<00:25,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 85%|████████▌ | 86/101 [02:28<00:25,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 87/101 [02:30<00:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 86%|████████▌ | 87/101 [02:30<00:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 88/101 [02:32<00:21,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 87%|████████▋ | 88/101 [02:32<00:21,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 89/101 [02:33<00:20,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 88%|████████▊ | 89/101 [02:33<00:20,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 90/101 [02:35<00:18,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 89%|████████▉ | 90/101 [02:35<00:18,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 91/101 [02:37<00:16,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 90%|█████████ | 91/101 [02:37<00:16,  1.69s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 92/101 [02:38<00:15,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 91%|█████████ | 92/101 [02:38<00:15,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 93/101 [02:40<00:13,  1.68s/it][1,mpirank:8,algo-2]<stderr>:#015 92%|█████████▏| 93/101 [02:40<00:13,  1.68s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 94/101 [02:42<00:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 93%|█████████▎| 94/101 [02:42<00:11,  1.67s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 95/101 [02:43<00:09,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 94%|█████████▍| 95/101 [02:43<00:09,  1.66s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 96/101 [02:45<00:08,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 95%|█████████▌| 96/101 [02:45<00:08,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 97/101 [02:46<00:06,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 96%|█████████▌| 97/101 [02:47<00:06,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 98/101 [02:48<00:04,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 97%|█████████▋| 98/101 [02:48<00:04,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 99/101 [02:50<00:03,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 98%|█████████▊| 99/101 [02:50<00:03,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 100/101 [02:51<00:01,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 99%|█████████▉| 100/101 [02:52<00:01,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 101/101 [02:53<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|██████████| 101/101 [02:53<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'train_runtime': 173.5795, 'train_samples_per_second': 299.586, 'train_steps_per_second': 0.582, 'train_loss': 2.222540222772277, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 101/101 [02:53<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:{'train_runtime': 173.7494, 'train_samples_per_second': 299.293, 'train_steps_per_second': 0.581, 'train_loss': 2.210869817450495, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|██████████| 101/101 [02:53<00:00,  1.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 101/101 [02:53<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|██████████| 101/101 [02:53<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>: stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:312 [4] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:306 [0] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:310 [2] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:214 [0] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:213 [2] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:211 [4] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:312 [4] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:306 [0] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:310 [2] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:75:214 [0] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:211 [4] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:213 [2] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:306 [0] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:302 [6] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:308 [0] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:212 [6] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:70:206 [0] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:68:210 [0] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:312 [4] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:310 [2] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:73:211 [4] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:71:213 [2] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:302 [6] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:302 [6] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:212 [6] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:212 [6] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:302 [6] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:212 [6] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:302 [6] NCCL INFO [Service thread] Connection closed by localRank 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:74:212 [6] NCCL INFO [Service thread] Connection closed by localRank 1\u001b[0m\n",
      "\u001b[34m2023-08-19 09:08:23,124 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-19 09:08:23,124 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-19 09:08:23,124 sagemaker-training-toolkit INFO     Begin writing status file from leader node to worker nodes (if any)\u001b[0m\n",
      "\u001b[34m2023-08-19 09:08:23,124 sagemaker-training-toolkit INFO     Start writing mpirun finished status to algo-2\u001b[0m\n",
      "\u001b[34m2023-08-19 09:08:23,263 sagemaker-training-toolkit INFO     output from subprocess run CompletedProcess(args=['ssh', 'algo-2', 'touch', '/tmp/done.algo-1'], returncode=0, stdout='', stderr='')\u001b[0m\n",
      "\u001b[34m2023-08-19 09:08:23,263 sagemaker-training-toolkit INFO     Finished writing status file\u001b[0m\n",
      "\u001b[35m2023-08-19 09:08:23,158 sagemaker-training-toolkit INFO     Invoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[35m2023-08-19 09:08:23,159 sagemaker-training-toolkit INFO     process psutil.Process(pid=65, name='orted', status='terminated', started='09:04:09') terminated with exit code None\u001b[0m\n",
      "\u001b[35m2023-08-19 09:08:23,159 sagemaker-training-toolkit INFO     Reporting status for ORTEd process. gone: [psutil.Process(pid=65, name='orted', status='terminated', started='09:04:09')] alive: []\u001b[0m\n",
      "\u001b[35m2023-08-19 09:08:23,159 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34m2023-08-19 09:08:53,293 sagemaker-training-toolkit INFO     Finished writing status file from leader node to worker nodes (if any)\u001b[0m\n",
      "\u001b[34m2023-08-19 09:08:53,293 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2023-08-19 09:08:53,186 sagemaker-training-toolkit INFO     Begin looking for status file on algo-2\u001b[0m\n",
      "\u001b[35m2023-08-19 09:08:53,186 sagemaker-training-toolkit INFO     MPI training job status file found. Exit gracefully\u001b[0m\n",
      "\u001b[35m2023-08-19 09:08:53,186 sagemaker-training-toolkit INFO     End looking for status file\u001b[0m\n",
      "\u001b[35m2023-08-19 09:08:53,186 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2023-08-19 09:08:53,187 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 1722\n",
      "Billable seconds: 1722\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
