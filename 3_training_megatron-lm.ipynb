{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3_training_megatron-lm\n",
    "\n",
    "- https://github.com/NVIDIA/Megatron-LM 에서 파일을 다운로드 받은 후 3_training_megatron-lm 폴더의 코드를 덮어쓰기하여 학습을 하시면 됩니다.\n",
    "- 시작 전 docker 폴더에서 Dockerfile을 이용하여 custom_docker를 만들어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = True\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already revised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "Warning: Failed to open the file /usr/local/bin/docker-compose: Text file busy\n",
      "  0 24.5M    0  1362    0     0  10034      0  0:42:50 --:--:--  0:42:50 10034\n",
      "curl: (23) Failure writing output to destination\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "DAEMON_PATH=\"/etc/docker\"\n",
    "MEMORY_SIZE=10G\n",
    "\n",
    "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# echo $FLAG\n",
    "\n",
    "if [ \"$FLAG\" == true ]; then\n",
    "    echo \"Already revised\"\n",
    "else\n",
    "    echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "    sudo service docker restart\n",
    "    echo \"Docker Restart\"\n",
    "fi\n",
    "\n",
    "sudo curl -L \"https://github.com/docker/compose/releases/download/v2.7.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n",
    "sudo chmod +x /usr/local/bin/docker-compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing deps and restarting kernel\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install --upgrade pip --quiet\n",
    "    !{sys.executable} -m pip install -U sagemaker --quiet\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and registering the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon   7.68kB\n",
      "\u001b[91m[WARNING]: Empty continuation line found in:\n",
      "    RUN git clone https://github.com/NVIDIA/apex   && cd apex   && git checkout 27de66c   && pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./   && cd .. RUN pip install regex\n",
      "[WARNING]: Empty continuation lines will become errors in a future release.\n",
      "\u001b[0mStep 1/3 : FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.0.0-gpu-py310\n",
      " ---> 2cf7a91d14ca\n",
      "Step 2/3 : RUN git clone https://github.com/NVIDIA/apex   && cd apex   && git checkout 27de66c   && pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./   && cd .. RUN pip install regex\n",
      " ---> Using cache\n",
      " ---> 2c3787173d6a\n",
      "Step 3/3 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> ab63d7aec05a\n",
      "Successfully built ab63d7aec05a\n",
      "Successfully tagged 322537213286.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.0.0-gpu-py310-apex\n",
      "The push refers to repository [322537213286.dkr.ecr.us-west-2.amazonaws.com/pytorch-training]\n",
      "c950852b3594: Preparing\n",
      "0a86d2f63da9: Preparing\n",
      "e9149126e47f: Preparing\n",
      "81bcaebf20b7: Preparing\n",
      "b89ba47ef264: Preparing\n",
      "5e365e6e2026: Preparing\n",
      "30f10d0e1e2a: Preparing\n",
      "de6ad3f5baf9: Preparing\n",
      "8995be0bc275: Preparing\n",
      "7649740a6938: Preparing\n",
      "138718a88769: Preparing\n",
      "4c8ddbfabe2c: Preparing\n",
      "e11d715889d8: Preparing\n",
      "d2516bd9d454: Preparing\n",
      "ab91cb17a698: Preparing\n",
      "375dafba5be7: Preparing\n",
      "eb2d5581a4b3: Preparing\n",
      "6e5ea4d3b078: Preparing\n",
      "a83e3f8647a8: Preparing\n",
      "629205717bfa: Preparing\n",
      "91962ccfdb56: Preparing\n",
      "e42093c82aca: Preparing\n",
      "5e365e6e2026: Waiting\n",
      "88f627f04385: Preparing\n",
      "53d4ef0348b1: Preparing\n",
      "7dec9be1e6de: Preparing\n",
      "1c442bf32dda: Preparing\n",
      "30f10d0e1e2a: Waiting\n",
      "7a4317d0452c: Preparing\n",
      "375dafba5be7: Waiting\n",
      "569b5fc6f9ba: Preparing\n",
      "16acfff66e41: Preparing\n",
      "eb2d5581a4b3: Waiting\n",
      "c2440becfb6e: Preparing\n",
      "93dc2ad27ff8: Preparing\n",
      "6e5ea4d3b078: Waiting\n",
      "3b6112f80af1: Preparing\n",
      "1be54c625d9b: Preparing\n",
      "a83e3f8647a8: Waiting\n",
      "5d4d8e450a3a: Preparing\n",
      "91962ccfdb56: Waiting\n",
      "aed2d71a436d: Preparing\n",
      "629205717bfa: Waiting\n",
      "88f627f04385: Waiting\n",
      "7af37e3e56a9: Preparing\n",
      "e5167e76bf1b: Preparing\n",
      "53d4ef0348b1: Waiting\n",
      "a490a70ab1cd: Preparing\n",
      "b3c248c52364: Preparing\n",
      "569b5fc6f9ba: Waiting\n",
      "7dec9be1e6de: Waiting\n",
      "d543b8cad89e: Preparing\n",
      "c2440becfb6e: Waiting\n",
      "16acfff66e41: Waiting\n",
      "aed2d71a436d: Waiting\n",
      "1c442bf32dda: Waiting\n",
      "a490a70ab1cd: Waiting\n",
      "93dc2ad27ff8: Waiting\n",
      "b3c248c52364: Waiting\n",
      "7af37e3e56a9: Waiting\n",
      "3b6112f80af1: Waiting\n",
      "5d4d8e450a3a: Waiting\n",
      "7649740a6938: Waiting\n",
      "d543b8cad89e: Waiting\n",
      "1be54c625d9b: Waiting\n",
      "7a4317d0452c: Waiting\n",
      "ab91cb17a698: Waiting\n",
      "4c8ddbfabe2c: Waiting\n",
      "e5167e76bf1b: Waiting\n",
      "138718a88769: Waiting\n",
      "e11d715889d8: Waiting\n",
      "e9149126e47f: Layer already exists\n",
      "0a86d2f63da9: Layer already exists\n",
      "c950852b3594: Layer already exists\n",
      "81bcaebf20b7: Layer already exists\n",
      "b89ba47ef264: Layer already exists\n",
      "5e365e6e2026: Layer already exists\n",
      "30f10d0e1e2a: Layer already exists\n",
      "de6ad3f5baf9: Layer already exists\n",
      "8995be0bc275: Layer already exists\n",
      "7649740a6938: Layer already exists\n",
      "138718a88769: Layer already exists\n",
      "4c8ddbfabe2c: Layer already exists\n",
      "e11d715889d8: Layer already exists\n",
      "ab91cb17a698: Layer already exists\n",
      "d2516bd9d454: Layer already exists\n",
      "375dafba5be7: Layer already exists\n",
      "6e5ea4d3b078: Layer already exists\n",
      "eb2d5581a4b3: Layer already exists\n",
      "a83e3f8647a8: Layer already exists\n",
      "629205717bfa: Layer already exists\n",
      "88f627f04385: Layer already exists\n",
      "91962ccfdb56: Layer already exists\n",
      "53d4ef0348b1: Layer already exists\n",
      "e42093c82aca: Layer already exists\n",
      "7dec9be1e6de: Layer already exists\n",
      "1c442bf32dda: Layer already exists\n",
      "7a4317d0452c: Layer already exists\n",
      "569b5fc6f9ba: Layer already exists\n",
      "16acfff66e41: Layer already exists\n",
      "c2440becfb6e: Layer already exists\n",
      "93dc2ad27ff8: Layer already exists\n",
      "3b6112f80af1: Layer already exists\n",
      "5d4d8e450a3a: Layer already exists\n",
      "1be54c625d9b: Layer already exists\n",
      "aed2d71a436d: Layer already exists\n",
      "7af37e3e56a9: Layer already exists\n",
      "e5167e76bf1b: Layer already exists\n",
      "a490a70ab1cd: Layer already exists\n",
      "b3c248c52364: Layer already exists\n",
      "d543b8cad89e: Layer already exists\n",
      "2.0.0-gpu-py310-apex: digest: sha256:68cabb252c713408acb4aa438ea28522215770dca14b6350e4175c8995c88c87 size: 8737\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=pytorch-training\n",
    "image_tag=2.0.0-gpu-py310-apex\n",
    "\n",
    "cd '3_training_megatron-lm/docker'\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:${image_tag}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -f Dockerfile -t ${fullname} .\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/megatron-lm'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.174.0'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure FSx Input for your SageMaker Training job\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "file_system_directory_path= '/dataset' #\n",
    "file_system_id='fs-XXXXXXXXXXXXX'         # \n",
    "file_system_access_mode='ro'\n",
    "file_system_type='EFS'\n",
    "train_fs=FileSystemInput(file_system_id=file_system_id,\n",
    "                         file_system_type=file_system_type,\n",
    "                         directory_path=file_system_directory_path,\n",
    "                         file_system_access_mode=file_system_access_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training with Distributed Data Parallel\n",
    "\n",
    "\n",
    "The training script provides the code you need for distributed data parallel (DDP) training. The training script is very similar to a PyTorch training script you might run outside of SageMaker.\n",
    "\n",
    "In the following code block, you can update the estimator function to use a different instance type, instance count, and distrubtion strategy. You're also passing in the training script you reviewed in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'num-layers': 12,\n",
    "    'hidden-size': 768,\n",
    "    'num-attention-heads': 12,\n",
    "    'seq-length': 1024,\n",
    "    'max-position-embeddings': 1024,\n",
    "    'micro-batch-size': 12,\n",
    "    'global-batch-size': 192,\n",
    "    'lr': 0.0005,\n",
    "    # 'train-iters': 150000,\n",
    "    'train-iters': 4000,\n",
    "    'lr-decay-iters': 150000,\n",
    "    'lr-decay-style': 'cosine',\n",
    "    'lr-warmup-iters': 2000,\n",
    "    'weight-decay':  .1,\n",
    "    'adam-beta2':  .999,\n",
    "    'fp16' : 'true',\n",
    "    'log-interval': 10,\n",
    "    'save-interval': 2000,\n",
    "    'eval-interval': 200,\n",
    "    'eval-iters': 10,\n",
    "    'data-path':'/opt/ml/input/data/dataset/codeparrot_content_document',\n",
    "    'vocab-file':'/opt/ml/input/data/dataset/gpt2-vocab.json',\n",
    "    'merge-file':'/opt/ml/input/data/dataset/gpt2-merges.txt',\n",
    "    'save' : '/opt/ml/model/',\n",
    "    'tensor-model-parallel-size' : 4,\n",
    "    'pipeline-model-parallel-size' : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distribution = {}\n",
    "distribution[\"mpi\"]={\"enabled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instance_type = 'ml.p3.16xlarge'  # 'ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'local_gpu'\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "# instance_type = 'local_gpu'\n",
    "instance_count = 2\n",
    "max_run = 2*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.inputs.FileSystemInput at 0x7f4226cc59f0>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if instance_type =='local_gpu':\n",
    "    from sagemaker.local import LocalSession\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    s3_data_path = f'file://{Path.cwd()}/codeparrot'\n",
    "    model_weight = ''\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    s3_data_path = \"s3://bucket-name-XXXXXXXX/megatron-lm/codeparrot/\"\n",
    "    s3_data_path = train_fs\n",
    "s3_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Pytorch` estimator를 이용한 training job 생성하기\n",
    "\n",
    "\n",
    "<p><strong><code>sagemaker.pytorch.PyTorch</code></strong> estimator는 처음 실행하는 스크립트 위치와 다양한 연계 코드들이 위치한 디렉토리 정보를 찾아서 스크립트를 S3에 upload하고 SageMaker의 training job을 수행하게 됩니다. training job은 학습을 수행한 단위입니다. 학습을 1번 돌리면 training job이 1개 생성됩니다. 몇 가지 중요 파라미터를 아래와 같이 설명드립니다. </p>\n",
    "\n",
    "- **entry_point** : 학습을 처음 실행하는 Python 소스 파일의 절대 또는 상대 경로이며, source_dir이 지정된 경우 entry_point는 source_dir 내 파일이 됩니다.\n",
    "- **source_dir** : 학습에 연계되는 다양한 소스코드 파일이 들어 있는 디렉토리 위치이며, 절대, 상대 경로 또는 S3 URI가 모두 가능하며,source_dir이 S3 URI 인 경우 tar.gz 파일이 됩니다.\n",
    "- **role** : Amazon SageMaker가 사용자를 대신해 작업(예: S3 버킷에서 모델 결과물이라고 하는 훈련 결과 읽기 및 Amazon S3에 훈련 결과 쓰기)을 수행하는 AWS Identity and Access Management(IAM) 역할입니다.\n",
    "- **train_instance_count** : 학습을 수행하는 instance 개수를 정의할 수 있습니다.\n",
    "- **train_instance_type** : 학습을 수행하는 instance 타입을 정의할 수 있습니다.\n",
    "- **train_volume_size** : 학습 인스턴스에 연결할 Amazon Elastic Block Store(Amazon EBS) 스토리지 볼륨의 크기(GB)입니다. File 모드를 사용할 경우 이 값이 훈련 데이터를 충분히 저장할 수 있는 크기여야 합니다(File 모드가 기본값)\n",
    "- **train_max_run** : 최대 학습 시간을 설정할 수 있으며, 이 시간이 지나면 Amazon SageMaker는 현재 상태에 관계없이 작업을 종료합니다. (기본값 : 24 * 60 * 60)\n",
    "- **framework_version** : 학습에 사용될 특정 Pytorch 버전을 정의할 수 있습니다.\n",
    "- **py_version** : 컨테이너 환경이 python3일 경우 py3, python2일 경우 py2로 설정하면 됩니다. python2는 지원이 중단되었지만 기존 python2로 구성된 파일들을 지원하기 위해 현재 계속 사용할 수 있습니다. 없을 경우에는 기본적으로 py3 입니다.\n",
    "- **hyperparameters** : 학습에 사용할 하이퍼 파라미터를 정의할 수 있으며, 정의된 하이퍼 파라미터 값들은 모두 학습 컨테이너로 전송이 됩니다.\n",
    "- **distribution** : 분산과 관련된 값들을 학습 컨테이너로 전송합니다.\n",
    "\n",
    "<p> 추가적으로 분산/ 멀티 GPU 학습도 가능합니다. SageMaker는 <strong><a href=\"https://github.com/horovod/horovod\" target=\"_blank\" class ='btn-default'>Horovod</a></strong>에 최적화된 환경을 제공하고 있으며, Pytorch의 경우 1.5.0부터 기본 docker에서 apex를 지원합니다.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = f'{accound_id}.dkr.ecr.{region_name}.amazonaws.com/pytorch-training:2.0.0-gpu-py310-apex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "                    # entry_point='train_adv.py',\n",
    "                    entry_point='pretrain_gpt.py',\n",
    "                    source_dir=f'{Path.cwd()}/3_training_megatron-lm',\n",
    "                    role=role,\n",
    "                    image_uri=image_uri,\n",
    "                    framework_version='1.13.1',\n",
    "                    py_version='py39',\n",
    "                    instance_count=instance_count,\n",
    "                    instance_type=instance_type,\n",
    "                    distribution=distribution,\n",
    "                    # metric_definitions=metric_definitions,\n",
    "                    disable_profiler=True,\n",
    "                    debugger_hook_config=False,\n",
    "                    max_run=max_run,\n",
    "                    hyperparameters=hyperparameters,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo rm -rf code/core.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: megatron-lm-ml-p4d-24xlarge-0805-12361691239004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    }
   ],
   "source": [
    "current_time = strftime(\"%m%d-%H%M%s\")\n",
    "i_type = instance_type.replace('.','-')\n",
    "job_name = f'megatron-lm-{i_type}-{current_time}'\n",
    "\n",
    "estimator.fit(\n",
    "    inputs={'dataset': s3_data_path, 'model_weight': model_weight}, \n",
    "    job_name=job_name,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-05 12:36:46 Starting - Starting the training job......\n",
      "2023-08-05 12:37:31 Starting - Preparing the instances for training........................\n",
      "2023-08-05 12:41:35 Downloading - Downloading input data\n",
      "2023-08-05 12:41:35 Training - Downloading the training image..............................\n",
      "2023-08-05 12:46:21 Training - Training image download completed. Training in progress.......\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:35,821 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:35,877 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:35,884 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:35,886 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:37,361 sagemaker-training-toolkit INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:34,935 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:34,990 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:34,997 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:34,999 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:36,470 sagemaker-training-toolkit INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting regex (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 8.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from megatron-core==0.2.0) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->megatron-core==0.2.0) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->megatron-core==0.2.0) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->megatron-core==0.2.0) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->megatron-core==0.2.0) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->megatron-core==0.2.0) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->megatron-core==0.2.0) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->megatron-core==0.2.0) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: megatron-core\u001b[0m\n",
      "\u001b[34mBuilding wheel for megatron-core (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for megatron-core (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for megatron-core: filename=megatron_core-0.2.0-py3-none-any.whl size=48541 sha256=02fc09994f304554a27772a45bd64e7b7c8cb7e705fa3b96825f4037374d7e60\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-2xc4wo2u/wheels/ee/79/1e/3fb168dd34359b627e23b53045c3eb498188294150b39e2fb0\u001b[0m\n",
      "\u001b[34mSuccessfully built megatron-core\u001b[0m\n",
      "\u001b[35mCollecting regex (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 7.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from megatron-core==0.2.0) (2.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->megatron-core==0.2.0) (3.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->megatron-core==0.2.0) (4.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->megatron-core==0.2.0) (1.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->megatron-core==0.2.0) (3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->megatron-core==0.2.0) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->megatron-core==0.2.0) (2.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->megatron-core==0.2.0) (1.3.0)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: megatron-core\u001b[0m\n",
      "\u001b[35mBuilding wheel for megatron-core (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for megatron-core (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for megatron-core: filename=megatron_core-0.2.0-py3-none-any.whl size=48541 sha256=02fc09994f304554a27772a45bd64e7b7c8cb7e705fa3b96825f4037374d7e60\u001b[0m\n",
      "\u001b[35mStored in directory: /tmp/pip-ephem-wheel-cache-w72lifh3/wheels/ee/79/1e/3fb168dd34359b627e23b53045c3eb498188294150b39e2fb0\u001b[0m\n",
      "\u001b[35mSuccessfully built megatron-core\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, megatron-core\u001b[0m\n",
      "\u001b[34mSuccessfully installed megatron-core-0.2.0 regex-2023.6.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:41,840 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:41,841 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:41,919 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35mInstalling collected packages: regex, megatron-core\u001b[0m\n",
      "\u001b[35mSuccessfully installed megatron-core-0.2.0 regex-2023.6.3\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.1.2 -> 23.2.1\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:42,575 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:42,575 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:41,981 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:41,989 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:41,989 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:42,023 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:42,023 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.51.209.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:42,656 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:42,718 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:42,727 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:42,727 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:42,727 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:42,770 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:42,770 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.49.154.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:43,034 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:43,175 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:43,176 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:43,176 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:43,176 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:43,178 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:43,781 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:43,907 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:43,907 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:43,907 sagemaker-training-toolkit INFO     Worker algo-1 available for communication\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:43,907 sagemaker-training-toolkit INFO     Env Hosts: ['algo-2', 'algo-1'] Hosts: ['algo-2:8', 'algo-1:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:43,908 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:43,981 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:44,068 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:44,076 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"dataset\": \"/opt/ml/input/data/dataset\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam-beta2\": 0.999,\n",
      "        \"data-path\": \"/opt/ml/input/data/dataset/codeparrot_content_document\",\n",
      "        \"eval-interval\": 200,\n",
      "        \"eval-iters\": 10,\n",
      "        \"fp16\": \"true\",\n",
      "        \"global-batch-size\": 192,\n",
      "        \"hidden-size\": 768,\n",
      "        \"log-interval\": 10,\n",
      "        \"lr\": 0.0005,\n",
      "        \"lr-decay-iters\": 150000,\n",
      "        \"lr-decay-style\": \"cosine\",\n",
      "        \"lr-warmup-iters\": 2000,\n",
      "        \"max-position-embeddings\": 1024,\n",
      "        \"merge-file\": \"/opt/ml/input/data/dataset/gpt2-merges.txt\",\n",
      "        \"micro-batch-size\": 12,\n",
      "        \"num-attention-heads\": 12,\n",
      "        \"num-layers\": 12,\n",
      "        \"pipeline-model-parallel-size\": 1,\n",
      "        \"save\": \"/opt/ml/model/\",\n",
      "        \"save-interval\": 2000,\n",
      "        \"seq-length\": 1024,\n",
      "        \"tensor-model-parallel-size\": 4,\n",
      "        \"train-iters\": 4000,\n",
      "        \"vocab-file\": \"/opt/ml/input/data/dataset/gpt2-vocab.json\",\n",
      "        \"weight-decay\": 0.1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"dataset\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"megatron-lm-ml-p4d-24xlarge-0805-12361691239004\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-2\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-322537213286/megatron-lm-ml-p4d-24xlarge-0805-12361691239004/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"pretrain_gpt\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"pretrain_gpt.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"adam-beta2\":0.999,\"data-path\":\"/opt/ml/input/data/dataset/codeparrot_content_document\",\"eval-interval\":200,\"eval-iters\":10,\"fp16\":\"true\",\"global-batch-size\":192,\"hidden-size\":768,\"log-interval\":10,\"lr\":0.0005,\"lr-decay-iters\":150000,\"lr-decay-style\":\"cosine\",\"lr-warmup-iters\":2000,\"max-position-embeddings\":1024,\"merge-file\":\"/opt/ml/input/data/dataset/gpt2-merges.txt\",\"micro-batch-size\":12,\"num-attention-heads\":12,\"num-layers\":12,\"pipeline-model-parallel-size\":1,\"save\":\"/opt/ml/model/\",\"save-interval\":2000,\"seq-length\":1024,\"tensor-model-parallel-size\":4,\"train-iters\":4000,\"vocab-file\":\"/opt/ml/input/data/dataset/gpt2-vocab.json\",\"weight-decay\":0.1}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=pretrain_gpt.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"dataset\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=pretrain_gpt\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-west-2-322537213286/megatron-lm-ml-p4d-24xlarge-0805-12361691239004/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true},\"channel_input_dirs\":{\"dataset\":\"/opt/ml/input/data/dataset\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-2\",\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"adam-beta2\":0.999,\"data-path\":\"/opt/ml/input/data/dataset/codeparrot_content_document\",\"eval-interval\":200,\"eval-iters\":10,\"fp16\":\"true\",\"global-batch-size\":192,\"hidden-size\":768,\"log-interval\":10,\"lr\":0.0005,\"lr-decay-iters\":150000,\"lr-decay-style\":\"cosine\",\"lr-warmup-iters\":2000,\"max-position-embeddings\":1024,\"merge-file\":\"/opt/ml/input/data/dataset/gpt2-merges.txt\",\"micro-batch-size\":12,\"num-attention-heads\":12,\"num-layers\":12,\"pipeline-model-parallel-size\":1,\"save\":\"/opt/ml/model/\",\"save-interval\":2000,\"seq-length\":1024,\"tensor-model-parallel-size\":4,\"train-iters\":4000,\"vocab-file\":\"/opt/ml/input/data/dataset/gpt2-vocab.json\",\"weight-decay\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"megatron-lm-ml-p4d-24xlarge-0805-12361691239004\",\"log_level\":20,\"master_hostname\":\"algo-2\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/megatron-lm-ml-p4d-24xlarge-0805-12361691239004/source/sourcedir.tar.gz\",\"module_name\":\"pretrain_gpt\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"pretrain_gpt.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--adam-beta2\",\"0.999\",\"--data-path\",\"/opt/ml/input/data/dataset/codeparrot_content_document\",\"--eval-interval\",\"200\",\"--eval-iters\",\"10\",\"--fp16\",\"true\",\"--global-batch-size\",\"192\",\"--hidden-size\",\"768\",\"--log-interval\",\"10\",\"--lr\",\"0.0005\",\"--lr-decay-iters\",\"150000\",\"--lr-decay-style\",\"cosine\",\"--lr-warmup-iters\",\"2000\",\"--max-position-embeddings\",\"1024\",\"--merge-file\",\"/opt/ml/input/data/dataset/gpt2-merges.txt\",\"--micro-batch-size\",\"12\",\"--num-attention-heads\",\"12\",\"--num-layers\",\"12\",\"--pipeline-model-parallel-size\",\"1\",\"--save\",\"/opt/ml/model/\",\"--save-interval\",\"2000\",\"--seq-length\",\"1024\",\"--tensor-model-parallel-size\",\"4\",\"--train-iters\",\"4000\",\"--vocab-file\",\"/opt/ml/input/data/dataset/gpt2-vocab.json\",\"--weight-decay\",\"0.1\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_DATASET=/opt/ml/input/data/dataset\u001b[0m\n",
      "\u001b[35mSM_HP_ADAM-BETA2=0.999\u001b[0m\n",
      "\u001b[35mSM_HP_DATA-PATH=/opt/ml/input/data/dataset/codeparrot_content_document\u001b[0m\n",
      "\u001b[35mSM_HP_EVAL-INTERVAL=200\u001b[0m\n",
      "\u001b[35mSM_HP_EVAL-ITERS=10\u001b[0m\n",
      "\u001b[35mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[35mSM_HP_GLOBAL-BATCH-SIZE=192\u001b[0m\n",
      "\u001b[35mSM_HP_HIDDEN-SIZE=768\u001b[0m\n",
      "\u001b[35mSM_HP_LOG-INTERVAL=10\u001b[0m\n",
      "\u001b[35mSM_HP_LR=0.0005\u001b[0m\n",
      "\u001b[35mSM_HP_LR-DECAY-ITERS=150000\u001b[0m\n",
      "\u001b[35mSM_HP_LR-DECAY-STYLE=cosine\u001b[0m\n",
      "\u001b[35mSM_HP_LR-WARMUP-ITERS=2000\u001b[0m\n",
      "\u001b[35mSM_HP_MAX-POSITION-EMBEDDINGS=1024\u001b[0m\n",
      "\u001b[35mSM_HP_MERGE-FILE=/opt/ml/input/data/dataset/gpt2-merges.txt\u001b[0m\n",
      "\u001b[35mSM_HP_MICRO-BATCH-SIZE=12\u001b[0m\n",
      "\u001b[35mSM_HP_NUM-ATTENTION-HEADS=12\u001b[0m\n",
      "\u001b[35mSM_HP_NUM-LAYERS=12\u001b[0m\n",
      "\u001b[35mSM_HP_PIPELINE-MODEL-PARALLEL-SIZE=1\u001b[0m\n",
      "\u001b[35mSM_HP_SAVE=/opt/ml/model/\u001b[0m\n",
      "\u001b[35mSM_HP_SAVE-INTERVAL=2000\u001b[0m\n",
      "\u001b[35mSM_HP_SEQ-LENGTH=1024\u001b[0m\n",
      "\u001b[35mSM_HP_TENSOR-MODEL-PARALLEL-SIZE=4\u001b[0m\n",
      "\u001b[35mSM_HP_TRAIN-ITERS=4000\u001b[0m\n",
      "\u001b[35mSM_HP_VOCAB-FILE=/opt/ml/input/data/dataset/gpt2-vocab.json\u001b[0m\n",
      "\u001b[35mSM_HP_WEIGHT-DECAY=0.1\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mmpirun --host algo-2:8,algo-1:8 -np 16 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.10/site-packages/gethostname.cpython-310-x86_64-linux-gnu.so -x FI_PROVIDER=efa -x NCCL_PROTO=simple -x FI_EFA_USE_DEVICE_RDMA=1 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_NUM_NEURONS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_DATASET -x SM_HP_ADAM-BETA2 -x SM_HP_DATA-PATH -x SM_HP_EVAL-INTERVAL -x SM_HP_EVAL-ITERS -x SM_HP_FP16 -x SM_HP_GLOBAL-BATCH-SIZE -x SM_HP_HIDDEN-SIZE -x SM_HP_LOG-INTERVAL -x SM_HP_LR -x SM_HP_LR-DECAY-ITERS -x SM_HP_LR-DECAY-STYLE -x SM_HP_LR-WARMUP-ITERS -x SM_HP_MAX-POSITION-EMBEDDINGS -x SM_HP_MERGE-FILE -x SM_HP_MICRO-BATCH-SIZE -x SM_HP_NUM-ATTENTION-HEADS -x SM_HP_NUM-LAYERS -x SM_HP_PIPELINE-MODEL-PARALLEL-SIZE -x SM_HP_SAVE -x SM_HP_SAVE-INTERVAL -x SM_HP_SEQ-LENGTH -x SM_HP_TENSOR-MODEL-PARALLEL-SIZE -x SM_HP_TRAIN-ITERS -x SM_HP_VOCAB-FILE -x SM_HP_WEIGHT-DECAY -x PYTHONPATH /opt/conda/bin/python3.10 -m mpi4py -m pretrain_gpt --adam-beta2 0.999 --data-path /opt/ml/input/data/dataset/codeparrot_content_document --eval-interval 200 --eval-iters 10 --fp16 true --global-batch-size 192 --hidden-size 768 --log-interval 10 --lr 0.0005 --lr-decay-iters 150000 --lr-decay-style cosine --lr-warmup-iters 2000 --max-position-embeddings 1024 --merge-file /opt/ml/input/data/dataset/gpt2-merges.txt --micro-batch-size 12 --num-attention-heads 12 --num-layers 12 --pipeline-model-parallel-size 1 --save /opt/ml/model/ --save-interval 2000 --seq-length 1024 --tensor-model-parallel-size 4 --train-iters 4000 --vocab-file /opt/ml/input/data/dataset/gpt2-vocab.json --weight-decay 0.1\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:44,155 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-05 12:47:44,190 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_ADAM-BETA2\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_DATA-PATH\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_EVAL-INTERVAL\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_EVAL-ITERS\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_GLOBAL-BATCH-SIZE\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_HIDDEN-SIZE\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_LOG-INTERVAL\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_LR-DECAY-ITERS\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_LR-DECAY-STYLE\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_LR-WARMUP-ITERS\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_MAX-POSITION-EMBEDDINGS\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_MERGE-FILE\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_MICRO-BATCH-SIZE\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_NUM-ATTENTION-HEADS\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_NUM-LAYERS\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_PIPELINE-MODEL-PARALLEL-SIZE\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_SAVE-INTERVAL\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_SEQ-LENGTH\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_TENSOR-MODEL-PARALLEL-SIZE\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_TRAIN-ITERS\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_VOCAB-FILE\"\u001b[0m\n",
      "\u001b[35m[algo-2:00076] Warning: could not find environment variable \"SM_HP_WEIGHT-DECAY\"\u001b[0m\n",
      "\u001b[35mWarning: Permanently added 'algo-1,10.0.49.154' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[35mData for JOB [22888,1] offset 0 Total slots allocated 16\u001b[0m\n",
      "\u001b[35m========================   JOB MAP   ========================\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [22888,1] App: 0 Process rank: 15 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:45,182 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=77, name='orted', status='sleeping', started='12:47:43')]\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:45,182 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=77, name='orted', status='sleeping', started='12:47:43')]\u001b[0m\n",
      "\u001b[34m2023-08-05 12:47:45,182 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=77, name='orted', status='sleeping', started='12:47:43')]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:using world size: 16, data-parallel-size: 4, tensor-model-parallel size: 4, pipeline-model-parallel size: 1 \u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:using torch.float16 for parameters ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:------------------------ arguments ------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  accumulate_allreduce_grads_in_fp32 .............. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  adam_beta1 ...................................... 0.9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  adam_beta2 ...................................... 0.999\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  adam_eps ........................................ 1e-08\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  add_bias_linear ................................. True[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  add_position_embedding .......................... True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  adlr_autoresume ................................. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  adlr_autoresume_interval ........................ 1000[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  apply_layernorm_1p .............................. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  apply_query_key_layer_scaling ................... True[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  apply_residual_connection_post_layernorm ........ False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  async_tensor_model_parallel_allreduce ........... True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  attention_dropout ............................... 0.1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  attention_softmax_in_fp32 ....................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  barrier_with_L1_time ............................ True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  bert_binary_head ................................ True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  bert_embedder_type .............................. megatron\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  bert_load ....................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  bf16 ............................................ False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  bias_dropout_fusion ............................. True[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  bias_gelu_fusion ................................ True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  biencoder_projection_dim ........................ 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  biencoder_shared_query_context_model ............ False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  block_data_path ................................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  classes_fraction ................................ 1.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  clip_grad ....................................... 1.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  consumed_train_samples .......................... 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  consumed_valid_samples .......................... 0[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  data_cache_path ................................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  data_impl ....................................... infer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  data_parallel_random_init ....................... False[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  data_parallel_size .............................. 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  data_path ....................................... ['/opt/ml/input/data/dataset/codeparrot_content_document']\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  data_per_class_fraction ......................... 1.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  data_sharding ................................... True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  dataloader_type ................................. single\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  DDP_impl ........................................ local\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  decoder_num_layers .............................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  decoder_seq_length .............................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  dino_bottleneck_size ............................ 256\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  dino_freeze_last_layer .......................... 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  dino_head_hidden_size ........................... 2048\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  dino_local_crops_number ......................... 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  dino_local_img_size ............................. 96\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  dino_norm_last_layer ............................ False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  dino_teacher_temp ............................... 0.07\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  dino_warmup_teacher_temp ........................ 0.04[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  dino_warmup_teacher_temp_epochs ................. 30\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  distribute_saved_activations .................... False[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  distributed_backend ............................. nccl\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  distributed_timeout_minutes ..................... 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  embedding_path .................................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  embedding_weights_in_fp32 ....................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  empty_unused_memory_level ....................... 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  encoder_num_layers .............................. 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  encoder_seq_length .............................. 1024\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  end_weight_decay ................................ 0.1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eod_mask_loss ................................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_interval ................................... 200\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  eval_iters ...................................... 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  evidence_data_path .............................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  exit_duration_in_mins ........................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  exit_interval ................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  exit_on_missing_checkpoint ...................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  exit_signal_handler ............................. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  ffn_hidden_size ................................. 3072\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  finetune ........................................ False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  fp16 ............................................ True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  fp16_lm_cross_entropy ........................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  fp32_residual_connection ........................ False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  fp8_amax_compute_algo ........................... most_recent[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  fp8_amax_history_len ............................ 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  fp8_e4m3 ........................................ False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  fp8_hybrid ...................................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  fp8_interval .................................... 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  fp8_margin ...................................... 0[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  fp8_wgrad ....................................... True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  global_batch_size ............................... 192\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  gradient_accumulation_fusion .................... True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  group_query_attention ........................... False[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  head_lr_mult .................................... 1.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  hidden_dropout .................................. 0.1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  hidden_size ..................................... 768[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  hysteresis ...................................... 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  ict_head_size ................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  ict_load ........................................ None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  img_h ........................................... 224\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  img_w ........................................... 224[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  indexer_batch_size .............................. 128\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  indexer_log_interval ............................ 1000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  inference_batch_times_seqlen_threshold .......... 512[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  init_method_std ................................. 0.02[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  init_method_xavier_uniform ...................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  initial_loss_scale .............................. 4294967296\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  iter_per_epoch .................................. 1250[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  kv_channels ..................................... 64\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  layernorm_epsilon ............................... 1e-05[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  lazy_mpu_init ................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  load ............................................ None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  local_rank ...................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  log_batch_size_to_tensorboard ................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  log_interval .................................... 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  log_learning_rate_to_tensorboard ................ True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  log_loss_scale_to_tensorboard ................... True[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  log_memory_to_tensorboard ....................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  log_num_zeros_in_grad ........................... False[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  log_params_norm ................................. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  log_timers_to_tensorboard ....................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  log_validation_ppl_to_tensorboard ............... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  log_world_size_to_tensorboard ................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  loss_scale ...................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  loss_scale_window ............................... 1000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  lr .............................................. 0.0005\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  lr_decay_iters .................................. 150000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  lr_decay_samples ................................ None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  lr_decay_style .................................. cosine\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  lr_warmup_fraction .............................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  lr_warmup_iters ................................. 2000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  lr_warmup_samples ............................... 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  make_vocab_size_divisible_by .................... 128\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  mask_factor ..................................... 1.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  mask_prob ....................................... 0.15\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  mask_type ....................................... random\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  masked_softmax_fusion ........................... True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  max_position_embeddings ......................... 1024\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  max_tokens_to_oom ............................... 12000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  merge_file ...................................... /opt/ml/input/data/dataset/gpt2-merges.txt\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  micro_batch_size ................................ 12[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  min_loss_scale .................................. 1.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  min_lr .......................................... 0.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  mmap_warmup ..................................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  no_load_optim ................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  no_load_rng ..................................... None[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  no_persist_layer_norm ........................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  no_save_optim ................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  no_save_rng ..................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  num_attention_heads ............................. 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  num_channels .................................... 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  num_classes ..................................... 1000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  num_experts ..................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  num_layers ...................................... 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  num_layers_per_virtual_pipeline_stage ........... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  num_query_groups ................................ 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  num_workers ..................................... 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  onnx_safe ....................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  openai_gelu ..................................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  optimizer ....................................... adam\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  output_bert_embeddings .......................... False[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  overlap_p2p_comm ................................ False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  override_opt_param_scheduler .................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  params_dtype .................................... torch.float16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  patch_dim ....................................... 16[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  perform_initialization .......................... True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  pipeline_model_parallel_size .................... 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  pipeline_model_parallel_split_rank .............. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  position_embedding_type ......................... learned_absolute\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  profile ......................................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  profile_ranks ................................... [0]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  profile_step_end ................................ 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  profile_step_start .............................. 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  query_in_block_prob ............................. 0.1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  rampup_batch_size ............................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  rank ............................................ 0[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  recompute_granularity ........................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  recompute_method ................................ None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  recompute_num_layers ............................ 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  reset_attention_mask ............................ False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  reset_position_ids .............................. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retriever_report_topk_accuracies ................ []\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retriever_score_scaling ......................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retriever_seq_length ............................ 256\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retro_add_retriever ............................. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retro_cyclic_train_iters ........................ None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retro_encoder_attention_dropout ................. 0.1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retro_encoder_hidden_dropout .................... 0.1[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retro_encoder_layers ............................ 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retro_num_neighbors ............................. 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retro_num_retrieved_chunks ...................... 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retro_return_doc_ids ............................ False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  retro_workdir ................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  rotary_percent .................................. 1.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  sample_rate ..................................... 1.0[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  save ............................................ /opt/ml/model/\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  save_interval ................................... 2000[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  scatter_gather_tensors_in_pipeline .............. True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  seed ............................................ 1234\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  seq_length ...................................... 1024\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  sequence_parallel ............................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  sgd_momentum .................................... 0.9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  short_seq_prob .................................. 0.1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  skip_train ...................................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  split ........................................... 969, 30, 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  squared_relu .................................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  standalone_embedding_stage ...................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  start_weight_decay .............................. 0.1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  swiglu .......................................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  swin_backbone_type .............................. tiny[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  tensor_model_parallel_size ...................... 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  tensorboard_dir ................................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  tensorboard_log_interval ........................ 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  tensorboard_queue_size .......................... 1000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  test_data_path .................................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  timing_log_level ................................ 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  timing_log_option ............................... minmax\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  titles_data_path ................................ None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  tokenizer_model ................................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  tokenizer_type .................................. GPT2BPETokenizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  train_data_path ................................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  train_iters ..................................... 4000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  train_samples ................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  transformer_impl ................................ local\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  transformer_pipeline_model_parallel_size ........ 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  untie_embeddings_and_output_weights ............. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  use_checkpoint_args ............................. False[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  use_checkpoint_opt_param_scheduler .............. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  use_contiguous_buffers_in_local_ddp ............. True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  use_cpu_initialization .......................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  use_distributed_optimizer ....................... False[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  use_flash_attn .................................. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  use_one_sent_docs ............................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  use_ring_exchange_p2p ........................... False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  use_rotary_position_embeddings .................. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  valid_data_path ................................. None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  variable_seq_lengths ............................ False[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  virtual_pipeline_model_parallel_size ............ None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  vision_backbone_type ............................ vit[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  vision_pretraining .............................. False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  vision_pretraining_type ......................... classify\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  vocab_extra_ids ................................. 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  vocab_file ...................................... /opt/ml/input/data/dataset/gpt2-vocab.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  vocab_size ...................................... None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  weight_decay .................................... 0.1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  weight_decay_incr_style ......................... constant\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  world_size ...................................... 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-------------------- end of arguments ---------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:setting number of micro-batches to constant 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:> building GPT2BPETokenizer tokenizer ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:> initializing torch distributed ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:> initialized tensor model parallel with size 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:> initialized pipeline model parallel with size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:> setting random seeds to 1234 ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:> compiling dataset index builder ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:make: Entering directory '/opt/ml/code/megatron/data'\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:make: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:make: Leaving directory '/opt/ml/code/megatron/data'\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:>>> done with dataset index builder. Compilation time: 0.030 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:> compiling and loading fused kernels ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:81 [0] NCCL INFO Bootstrap : Using eth0:10.0.51.209<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:81 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:81 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:81 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:82 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:85 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:86 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:87 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:88 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:83 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:84 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:81 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:84 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:86 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:85 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:87 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:82 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:80 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:83 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:86 [5] NCCL INFO Bootstrap : Using eth0:10.0.51.209<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:86 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:86 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:83 [2] NCCL INFO Bootstrap : Using eth0:10.0.51.209<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:83 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:83 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:84 [3] NCCL INFO Bootstrap : Using eth0:10.0.51.209<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:82 [1] NCCL INFO Bootstrap : Using eth0:10.0.51.209<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:82 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:82 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:84 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:84 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:87 [6] NCCL INFO Bootstrap : Using eth0:10.0.51.209<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:88 [7] NCCL INFO Bootstrap : Using eth0:10.0.51.209<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:87 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:87 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:85 [4] NCCL INFO Bootstrap : Using eth0:10.0.51.209<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:88 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:88 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:85 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:85 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:80 [0] NCCL INFO Bootstrap : Using eth0:10.0.49.154<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:83 [3] NCCL INFO Bootstrap : Using eth0:10.0.49.154<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:87 [7] NCCL INFO Bootstrap : Using eth0:10.0.49.154<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:84 [4] NCCL INFO Bootstrap : Using eth0:10.0.49.154<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:83 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:83 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:80 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:80 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:87 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:87 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:84 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:84 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:82 [2] NCCL INFO Bootstrap : Using eth0:10.0.49.154<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:82 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:82 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:85 [5] NCCL INFO Bootstrap : Using eth0:10.0.49.154<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:85 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:85 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:81 [1] NCCL INFO Bootstrap : Using eth0:10.0.49.154<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:81 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:81 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:86 [6] NCCL INFO Bootstrap : Using eth0:10.0.49.154<0>\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:86 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:86 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 00/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 01/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 02/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 03/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 04/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/10/-1->2->-1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->10 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] 0/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/-1/-1->7->6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 05/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 06/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 07/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->7 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->8 [5] 1/-1/-1->0->7 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] 8/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/12/-1->4->-1 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->12 [7] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] -1/-1/-1->5->4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/14/-1->6->-1 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->4 [3] 13/-1/-1->12->11 [4] 13/-1/-1->12->11 [5] 13/-1/-1->12->11 [6] 13/4/-1->12->-1 [7] 13/-1/-1->12->11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->6 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->13 [7] 15/6/-1->14->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] 14/-1/-1->13->12 [7] -1/-1/-1->13->12\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->2 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/2/-1->10->-1 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] -1/-1/-1->11->10 [3] 12/-1/-1->11->10 [4] 12/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] -1/-1/-1->11->10 [7] 12/-1/-1->11->10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] -1/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->15 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/0/-1->8->-1 [5] 9/-1/-1->8->15 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 03/0 : 12[901c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 01/0 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 07/0 : 12[901c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 05/0 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 00/0 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 03/0 : 8[101c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 07/0 : 8[101c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 03/0 : 7[a01d0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 03/0 : 15[a01d0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 04/0 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 01/0 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 02/0 : 13[901d0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 03/0 : 15[a01d0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 07/0 : 15[a01d0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 02/0 : 13[901d0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 02/0 : 5[901d0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 00/0 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 07/0 : 7[a01d0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 00/0 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 01/0 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 01/0 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 06/0 : 13[901d0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 05/0 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 07/0 : 15[a01d0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 06/0 : 13[901d0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 06/0 : 5[901d0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 04/0 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 02/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 06/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 05/0 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 04/0 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 05/0 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 00/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 01/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 03/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 04/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 05/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 07/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 00/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 00/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 01/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 01/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 02/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 02/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 04/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 04/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 05/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 00/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 06/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 05/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 02/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 06/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 03/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 00/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 04/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 01/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 06/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 02/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 07/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 04/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 05/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 06/0 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 00/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 01/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 02/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 04/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 05/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 06/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 00/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 01/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 02/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 04/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 05/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 06/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 00/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 04/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 00/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 02/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 03/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 04/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 06/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 07/0 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 01/0 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 05/0 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 01/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 00/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 02/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 03/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 04/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 05/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 06/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 07/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 00/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 01/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 03/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 04/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 05/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 07/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 01/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 02/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 03/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 05/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 06/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 07/0 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 00/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 00/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 04/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 00/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 01/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 01/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 00/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 03/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 04/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 04/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 05/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 07/0 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 05/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 02/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 00/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 03/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 02/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 03/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 04/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 04/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 06/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 06/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 07/0 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 07/0 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 00/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 00/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 01/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 01/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 02/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 03/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 01/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 04/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 03/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 00/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 05/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 05/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Channel 04/0 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 07/0 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 07/0 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 04/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 05/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 06/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 00/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 01/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 01/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 02/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 02/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 04/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 03/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 05/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 05/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 06/0 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 06/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 01/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 07/0 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Channel 05/0 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 01/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 02/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 05/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 06/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 02/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Channel 06/0 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 01/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 03/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 05/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Channel 07/0 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 03/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Channel 07/0 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:318 [0] NCCL INFO comm 0x55d677611120 rank 0 nranks 16 cudaDev 0 busId 101c0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:319 [5] NCCL INFO comm 0x555d2911d950 rank 5 nranks 16 cudaDev 5 busId 901d0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:322 [3] NCCL INFO comm 0x55d609f38560 rank 3 nranks 16 cudaDev 3 busId 201d0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:321 [1] NCCL INFO comm 0x55d018d4dca0 rank 1 nranks 16 cudaDev 1 busId 101d0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:324 [7] NCCL INFO comm 0x557f5e773480 rank 7 nranks 16 cudaDev 7 busId a01d0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:325 [4] NCCL INFO comm 0x561aad5b6870 rank 4 nranks 16 cudaDev 4 busId 901c0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:323 [6] NCCL INFO comm 0x55d764728240 rank 6 nranks 16 cudaDev 6 busId a01c0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:320 [2] NCCL INFO comm 0x56040bf76330 rank 2 nranks 16 cudaDev 2 busId 201c0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:274 [0] NCCL INFO comm 0x560dad0babb0 rank 8 nranks 16 cudaDev 0 busId 101c0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:273 [4] NCCL INFO comm 0x55680b0e1650 rank 12 nranks 16 cudaDev 4 busId 901c0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:277 [2] NCCL INFO comm 0x55579a1bb610 rank 10 nranks 16 cudaDev 2 busId 201c0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:280 [6] NCCL INFO comm 0x557e82086ae0 rank 14 nranks 16 cudaDev 6 busId a01c0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:279 [1] NCCL INFO comm 0x560de69bb3d0 rank 9 nranks 16 cudaDev 1 busId 101d0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:278 [5] NCCL INFO comm 0x556764eea240 rank 13 nranks 16 cudaDev 5 busId 901d0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:275 [7] NCCL INFO comm 0x564eeea5f770 rank 15 nranks 16 cudaDev 7 busId a01d0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:276 [3] NCCL INFO comm 0x56395a199750 rank 11 nranks 16 cudaDev 3 busId 201d0 commId 0xa77ff7f077bef56d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:>>> done with compiling and loading fused kernels. Compilation time: 2.614 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:time to initialize megatron (seconds): 8.315\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[after megatron is initialized] datetime: 2023-08-05 12:48:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:building GPT model ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>: > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 31825152\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>: > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 31825152\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>: > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 31825152\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 31825152\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:> learning rate decay style: cosine\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[after model, optimizer, and learning rate scheduler are built] datetime: 2023-08-05 12:48:00\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:> building train, validation, and test datasets ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > datasets target sizes (minimum size):\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    train:      768000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    validation: 40320\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    test:       1920\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:> building train, validation, and test datasets for GPT ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:Single data path provided for train, valid & test\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:data_prefix : /opt/ml/input/data/dataset/codeparrot_content_document\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:data_impl : infer\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:skip_warmup : True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > building dataset index ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:data_prefix : /opt/ml/input/data/dataset/codeparrot_content_document\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:data_impl : infer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:skip_warmup : True\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:data_prefix : /opt/ml/input/data/dataset/codeparrot_content_document\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:data_impl : infer\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:skip_warmup : True\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:data_prefix : /opt/ml/input/data/dataset/codeparrot_content_document\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:data_impl : infer\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:skip_warmup : True\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:************* path : /opt/ml/input/data/dataset/codeparrot_content_document\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:************* path : /opt/ml/input/data/dataset/codeparrot_content_document\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:************* path : /opt/ml/input/data/dataset/codeparrot_content_document\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:************* path : /opt/ml/input/data/dataset/codeparrot_content_document\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    reading sizes...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    reading pointers...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    reading document index...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    creating numpy buffer of mmap...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    creating memory view of numpy buffer...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > finished creating indexed dataset in 0.028265 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    number of documents: 5300000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > dataset split:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    train:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     document indices in [0, 5135700) total of 5135700 documents\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    validation:[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     document indices in [5135700, 5294700) total of 159000 documents\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    test:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     document indices in [5294700, 5300000) total of 5300 documents\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > WARNING: could not find index map files, building the indices on rank 0 ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > only one epoch required, setting separate_last_epoch to False\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>: > only one epoch required, setting separate_last_epoch to False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > elasped time to build and save doc-idx mapping (seconds): 0.519262\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    using:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     number of documents:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:5135700\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     number of epochs:          1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     sequence length:           [1,mpirank:0,algo-1]<stdout>:1024\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     total number of samples:   23115941\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:    using:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     number of documents:       5135700\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     number of epochs:          1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     sequence length:           1024\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     total number of samples:   23115941\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > elasped time to build and save sample-idx mapping (seconds): 1.071756\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > building shuffle index with split [0, 23115941) and [23115941, 23115941) ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>: > building shuffle index with split [0, 23115941) and [23115941, 23115941) ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > elasped time to build and save shuffle-idx mapping (seconds): 1.440585\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/3/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 00/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 01/04 :    0   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 02/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 03/04 :    0   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->2 [3] -1/-1/-1->0->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] -1/-1/-1->2->3 [2] 3/0/-1->2->-1 [3] -1/-1/-1->2->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/-1/-1->3->1 [2] -1/-1/-1->3->2 [3] 2/1/-1->3->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 00/0 : 1[901c0] -> 2[101c0] [send] via NET/AWS Libfabric/0(0)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 00/0 : 3[901c0] -> 0[101c0] [send] via NET/AWS Libfabric/0(2)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 00/0 : 1[901c0] -> 2[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 00/0 : 3[901c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 02/0 : 3[901c0] -> 0[101c0] [send] via NET/AWS Libfabric/0(2)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 02/0 : 1[901c0] -> 2[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 00/0 : 2[101c0] -> 3[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 02/0 : 2[101c0] -> 3[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 02/0 : 1[901c0] -> 2[101c0] [send] via NET/AWS Libfabric/0(0)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 02/0 : 3[901c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 1[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 1[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 3[901c0] [send] via NET/AWS Libfabric/2(1)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 01/0 : 0[101c0] -> 3[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 01/0 : 2[101c0] -> 1[901c0] [send] via NET/AWS Libfabric/2(3)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 01/0 : 2[101c0] -> 1[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 03/0 : 2[101c0] -> 1[901c0] [send] via NET/AWS Libfabric/2(3)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 03/0 : 0[101c0] -> 3[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 01/0 : 3[901c0] -> 2[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 03/0 : 3[901c0] -> 2[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 3[901c0] [send] via NET/AWS Libfabric/2(1)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 03/0 : 2[101c0] -> 1[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 01/0 : 1[901c0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 03/0 : 1[901c0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 01/0 : 2[101c0] -> 3[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 03/0 : 2[101c0] -> 3[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 1[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 1[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 2[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 01/0 : 1[901c0] -> 3[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 01/0 : 3[901c0] -> 1[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 00/0 : 2[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 2[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 03/0 : 3[901c0] -> 1[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 02/0 : 2[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 03/0 : 1[901c0] -> 3[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 00/0 : 2[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 01/0 : 1[901c0] -> 3[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 2[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 01/0 : 3[901c0] -> 1[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 03/0 : 1[901c0] -> 3[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 2[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Channel 02/0 : 2[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 03/0 : 3[901c0] -> 1[901c0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 00/0 : 1[901c0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 00/0 : 3[901c0] -> 2[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Channel 02/0 : 1[901c0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Channel 02/0 : 3[901c0] -> 2[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:305 [4] NCCL INFO comm 0x55680e044fe0 rank 3 nranks 4 cudaDev 4 busId 901c0 commId 0x2d89a31447815fef - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:306 [0] NCCL INFO comm 0x560db012f880 rank 2 nranks 4 cudaDev 0 busId 101c0 commId 0x2d89a31447815fef - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:351 [4] NCCL INFO comm 0x561ab00d6450 rank 1 nranks 4 cudaDev 4 busId 901c0 commId 0x2d89a31447815fef - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:350 [0] NCCL INFO comm 0x55d67a5556f0 rank 0 nranks 4 cudaDev 0 busId 101c0 commId 0x2d89a31447815fef - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:359 [4] NCCL INFO comm 0x561ab05360f0 rank 0 nranks 1 cudaDev 4 busId 901c0 commId 0x5dd4d331f957b357 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:314 [4] NCCL INFO comm 0x55680e05a700 rank 0 nranks 1 cudaDev 4 busId 901c0 commId 0xfba8a8a915906fb6 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:313 [0] NCCL INFO comm 0x560daf72d720 rank 0 nranks 1 cudaDev 0 busId 101c0 commId 0xb82f1f29e0d924da - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:358 [0] NCCL INFO comm 0x55d6794e7360 rank 0 nranks 1 cudaDev 0 busId 101c0 commId 0x67bf3858550b4622 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > temp_data ['/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_shuffle_idx.npy', '/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8.dsc', '/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_doc_idx.npy', '/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_sample_idx.npy']\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > loading doc-idx mapping from /opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_doc_idx.npy\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > loading sample-idx mapping from /opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_sample_idx.npy\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > loading shuffle-idx mapping from /opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_shuffle_idx.npy\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    loaded indexed file in 0.079 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    total number of samples: 23115942\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    total number of epochs: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > WARNING: could not find index map files, building the indices on rank 0 ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > only one epoch required, setting separate_last_epoch to False\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>: > only one epoch required, setting separate_last_epoch to False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > elasped time to build and save doc-idx mapping (seconds): 0.090219\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    using:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     number of documents:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:159000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     number of epochs:          [1,mpirank:0,algo-1]<stdout>:1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     sequence length:           1024\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     total number of samples:   694873\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:    using:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     number of documents:       159000\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     number of epochs:          1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     sequence length:           1024\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     total number of samples:   694873\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > elasped time to build and save sample-idx mapping (seconds): 0.115039\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > building shuffle index with split [0, 694873) and [694873, 694873) ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>: > building shuffle index with split [0, 694873) and [694873, 694873) ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > elasped time to build and save shuffle-idx mapping (seconds): 0.137233\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > temp_data ['/opt/ml/input/data/dataset/index-cache/467b62776b75aa5304e1365ce73f9b2f_doc_idx.npy', '/opt/ml/input/data/dataset/index-cache/467b62776b75aa5304e1365ce73f9b2f_sample_idx.npy', '/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_shuffle_idx.npy', '/opt/ml/input/data/dataset/index-cache/467b62776b75aa5304e1365ce73f9b2f.dsc', '/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8.dsc', '/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_doc_idx.npy', '/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_sample_idx.npy', '/opt/ml/input/data/dataset/index-cache/467b62776b75aa5304e1365ce73f9b2f_shuffle_idx.npy']\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > loading doc-idx mapping from /opt/ml/input/data/dataset/index-cache/467b62776b75aa5304e1365ce73f9b2f_doc_idx.npy\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > loading sample-idx mapping from /opt/ml/input/data/dataset/index-cache/467b62776b75aa5304e1365ce73f9b2f_sample_idx.npy\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>: > only one epoch required, setting separate_last_epoch to False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > loading shuffle-idx mapping from /opt/ml/input/data/dataset/index-cache/467b62776b75aa5304e1365ce73f9b2f_shuffle_idx.npy\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    loaded indexed file in 0.041 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    total number of samples: 694874\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    total number of epochs: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > WARNING: could not find index map files, building the indices on rank 0 ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > only one epoch required, setting separate_last_epoch to False\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:    using:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     number of documents:       5300\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     number of epochs:          1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     sequence length:           1024\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:     total number of samples:   23378\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>: > building shuffle index with split [0, 23378) and [23378, 23378) ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > elasped time to build and save doc-idx mapping (seconds): 0.025320\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    using:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     number of documents:       5300\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     number of epochs:          1[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     sequence length:           [1,mpirank:0,algo-1]<stdout>:1024\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:     total number of samples:   23378\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > elasped time to build and save sample-idx mapping (seconds): 0.038450\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > building shuffle index with split [0, 23378) and [23378, 23378) ...[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > elasped time to build and save shuffle-idx mapping (seconds): 0.036080\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > temp_data ['/opt/ml/input/data/dataset/index-cache/7f1dc9c34ae533ef73bb7eeda56296ca.dsc', '/opt/ml/input/data/dataset/index-cache/467b62776b75aa5304e1365ce73f9b2f_doc_idx.npy', '/opt/ml/input/data/dataset/index-cache/467b62776b75aa5304e1365ce73f9b2f_sample_idx.npy', '/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_shuffle_idx.npy', '/opt/ml/input/data/dataset/index-cache/467b62776b75aa5304e1365ce73f9b2f.dsc', '/opt/ml/input/data/dataset/index-cache/7f1dc9c34ae533ef73bb7eeda56296ca_sample_idx.npy', '/opt/ml/input/data/dataset/index-cache/7f1dc9c34ae533ef73bb7eeda56296ca_doc_idx.npy', '/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8.dsc', '/opt/ml/input/data/dataset/index-cache/7f1dc9c34ae533ef73bb7eeda56296ca_shuffle_idx.npy', '/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_doc_idx.npy', '/opt/ml/input/data/dataset/index-cache/7cd70afc2b12d4665679e7cefe0372e8_sample_idx.npy', '/opt/ml/input/data/dataset/index-cache/467b62776b75aa5304e1365ce73f9b2f_shuffle_idx.npy']\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > loading doc-idx mapping from /opt/ml/input/data/dataset/index-cache/7f1dc9c34ae533ef73bb7eeda56296ca_doc_idx.npy\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > loading sample-idx mapping from /opt/ml/input/data/dataset/index-cache/7f1dc9c34ae533ef73bb7eeda56296ca_sample_idx.npy\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>: > loading shuffle-idx mapping from /opt/ml/input/data/dataset/index-cache/7f1dc9c34ae533ef73bb7eeda56296ca_shuffle_idx.npy\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    loaded indexed file in 0.010 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    total number of samples: 23379\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:    total number of epochs: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:> finished creating GPT datasets ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 00/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 01/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 02/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 03/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 04/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 05/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 06/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 07/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 08/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 09/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 10/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 11/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 12/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 13/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 14/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 15/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 16/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 17/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 18/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 19/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 20/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 21/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 22/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 23/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 00/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 01/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 02/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 03/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 04/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 05/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 06/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 07/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 08/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 09/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 10/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 11/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 12/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 13/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 14/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 15/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 16/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 17/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 18/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 19/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 20/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 21/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 22/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 23/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 08/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 09/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 10/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 11/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 12/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 13/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 14/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 15/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 16/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 17/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 18/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 19/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 00/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 01/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 02/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 03/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 04/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 05/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 06/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 07/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 08/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 09/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 10/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 11/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 12/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 13/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 14/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 15/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 16/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 17/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 18/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 19/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 20/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 21/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 22/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 23/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 20/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 08/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 09/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 21/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 10/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 11/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 22/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 12/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 13/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Channel 23/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 14/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 15/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 16/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 17/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 18/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 19/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 20/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 21/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 22/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Channel 23/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 00/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 01/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 02/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 03/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 04/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 05/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 06/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 07/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 08/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 09/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 10/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 11/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 12/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 13/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 14/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 15/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 16/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 17/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 18/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 19/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 20/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 21/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 22/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 23/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 00/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 00/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 01/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 00/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 01/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 00/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 02/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 01/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 02/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 01/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 03/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 02/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 03/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 03/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 02/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 04/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 04/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 04/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 03/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 05/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 05/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 05/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 04/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 06/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 06/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 06/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 05/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 07/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 07/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 07/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 08/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 06/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 08/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 08/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 09/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 07/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 09/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 10/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 09/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 08/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 10/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 11/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 10/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 09/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 11/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 12/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 11/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 10/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 12/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 13/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 12/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 11/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 13/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 14/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 13/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 12/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 14/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 00/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 00/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 15/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 15/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 14/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 13/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 01/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 00/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 01/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 16/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 16/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 15/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 14/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 02/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 02/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 17/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 03/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 17/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 16/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 03/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 15/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 04/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 18/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 04/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 18/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 05/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 17/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 16/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 05/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 19/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 06/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 19/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 06/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 18/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 17/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 00/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 07/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 20/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 07/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 20/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 01/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 08/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 19/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 18/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 21/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 08/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 21/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 02/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 09/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 09/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 20/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 19/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 22/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 10/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 03/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 22/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 10/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 21/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 11/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 04/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 20/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 23/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 11/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Channel 23/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 12/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 05/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 12/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 22/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 13/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 06/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 21/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 13/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 01/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 14/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 23/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 22/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 07/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 14/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 02/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 23/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 15/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 08/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 15/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 03/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 16/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 09/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 16/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 04/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 17/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 10/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 17/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 05/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 18/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 11/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 18/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 06/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 19/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 12/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 19/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 07/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 20/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 13/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 20/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 08/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 21/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 14/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 21/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 09/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 22/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 15/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 22/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 10/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Channel 23/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 16/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 23/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 11/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 17/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 12/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 18/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 13/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 19/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 14/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 20/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 15/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 21/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 16/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 22/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 17/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 23/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 18/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 19/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 20/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 21/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 22/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 23/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 00/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 01/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 02/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 03/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 04/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 05/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 06/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 07/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 00/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 08/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 01/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 09/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 02/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 10/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 03/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 11/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 04/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 12/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 05/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 13/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 00/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 06/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 14/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 00/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 01/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 07/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 15/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 01/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 08/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 02/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 16/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 03/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 02/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 09/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 04/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 17/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 05/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 03/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 10/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 06/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 18/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 04/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 11/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 07/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 19/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 05/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 08/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 12/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 09/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 10/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 20/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 06/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 13/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 11/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 21/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 12/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 14/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 07/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 13/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 22/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 08/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 15/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 14/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 00/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 15/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 16/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Channel 23/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 09/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 01/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 16/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 17/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 10/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 02/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 17/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 18/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 11/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 03/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 18/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 19/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 12/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 04/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 19/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 05/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 20/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 13/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 20/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 06/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 21/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 14/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 21/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 07/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 22/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 15/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 22/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 08/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Channel 23/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 16/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Channel 23/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 09/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 10/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 17/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 11/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 12/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 18/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 13/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 19/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 14/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 20/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 15/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 00/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 16/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 01/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 21/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 17/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 02/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 22/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Channel 23/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 18/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 03/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 19/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 04/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 20/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 05/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 21/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 06/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 22/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 07/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Channel 23/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 08/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 09/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 10/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 11/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 12/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 13/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 14/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 15/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 16/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 17/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 18/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 19/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 20/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 21/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 22/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Channel 23/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:325 [1] NCCL INFO comm 0x560de9963c20 rank 1 nranks 4 cudaDev 1 busId 101d0 commId 0x9123266afd03dda8 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:326 [2] NCCL INFO comm 0x55579d35f550 rank 2 nranks 4 cudaDev 2 busId 201c0 commId 0x9123266afd03dda8 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:321 [0] NCCL INFO comm 0x560daf905550 rank 0 nranks 4 cudaDev 0 busId 101c0 commId 0x9123266afd03dda8 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:327 [3] NCCL INFO comm 0x56395d157a10 rank 3 nranks 4 cudaDev 3 busId 201d0 commId 0x9123266afd03dda8 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:371 [2] NCCL INFO comm 0x56040eea3350 rank 2 nranks 4 cudaDev 2 busId 201c0 commId 0x8dcbea5e69d8f3ca - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:369 [3] NCCL INFO comm 0x55d60d0b2110 rank 3 nranks 4 cudaDev 3 busId 201d0 commId 0x8dcbea5e69d8f3ca - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:368 [1] NCCL INFO comm 0x55d01bcfeb70 rank 1 nranks 4 cudaDev 1 busId 101d0 commId 0x8dcbea5e69d8f3ca - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:366 [0] NCCL INFO comm 0x55d67a559e70 rank 0 nranks 4 cudaDev 0 busId 101c0 commId 0x8dcbea5e69d8f3ca - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:373 [6] NCCL INFO comm 0x55d766dde2d0 rank 2 nranks 4 cudaDev 6 busId a01c0 commId 0x239ea9818f8983ac - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:372 [5] NCCL INFO comm 0x555d2bc678e0 rank 1 nranks 4 cudaDev 5 busId 901d0 commId 0x239ea9818f8983ac - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:370 [7] NCCL INFO comm 0x557f618ba650 rank 3 nranks 4 cudaDev 7 busId a01d0 commId 0x239ea9818f8983ac - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:367 [4] NCCL INFO comm 0x561ab0621f40 rank 0 nranks 4 cudaDev 4 busId 901c0 commId 0x239ea9818f8983ac - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:328 [5] NCCL INFO comm 0x5567675bfd10 rank 1 nranks 4 cudaDev 5 busId 901d0 commId 0x3f3468e096a1cc5a - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:323 [7] NCCL INFO comm 0x564ef19b37f0 rank 3 nranks 4 cudaDev 7 busId a01d0 commId 0x3f3468e096a1cc5a - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:324 [6] NCCL INFO comm 0x557e84fcfb40 rank 2 nranks 4 cudaDev 6 busId a01c0 commId 0x3f3468e096a1cc5a - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:322 [4] NCCL INFO comm 0x55680e05d3a0 rank 0 nranks 4 cudaDev 4 busId 901c0 commId 0x3f3468e096a1cc5a - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[after dataloaders are built] datetime: 2023-08-05 12:48:07 \u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:done with setup ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:training ...\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:(min, max) time across ranks (ms):\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:    model-and-optimizer-setup ......................: (53.56, 76.42)\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:    train/valid/test-data-iterators-setup ..........: (6204.26, 6333.38)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[before the start of training step] datetime: 2023-08-05 12:48:07\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 00/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 01/04 :    0   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 02/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 03/04 :    0   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/3/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->2 [3] -1/-1/-1->0->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] -1/-1/-1->2->3 [2] 3/0/-1->2->-1 [3] -1/-1/-1->2->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/-1/-1->3->1 [2] -1/-1/-1->3->2 [3] 2/1/-1->3->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 00/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 01/04 :    0   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 02/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 03/04 :    0   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->2 [3] -1/-1/-1->0->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/3/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] -1/-1/-1->2->3 [2] 3/0/-1->2->-1 [3] -1/-1/-1->2->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/-1/-1->3->1 [2] -1/-1/-1->3->2 [3] 2/1/-1->3->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 00/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 01/04 :    0   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 02/04 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 03/04 :    0   3   2   1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->2 [3] -1/-1/-1->0->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/3/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] -1/-1/-1->2->3 [2] 3/0/-1->2->-1 [3] -1/-1/-1->2->3\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/-1/-1->3->1 [2] -1/-1/-1->3->2 [3] 2/1/-1->3->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO P2P Chunksize set to 131072\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 00/0 : 1[a01d0] -> 2[201d0] [send] via NET/AWS Libfabric/1(0)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 00/0 : 3[a01d0] -> 0[201d0] [send] via NET/AWS Libfabric/1(2)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 00/0 : 1[901d0] -> 2[101d0] [send] via NET/AWS Libfabric/0(0)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 00/0 : 3[901d0] -> 0[101d0] [send] via NET/AWS Libfabric/0(2)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 00/0 : 1[a01d0] -> 2[201d0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 00/0 : 3[a01d0] -> 0[201d0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 00/0 : 1[901d0] -> 2[101d0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 00/0 : 3[901d0] -> 0[101d0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 00/0 : 1[a01c0] -> 2[201c0] [send] via NET/AWS Libfabric/1(0)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 00/0 : 1[a01c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 02/0 : 1[a01d0] -> 2[201d0] [send] via NET/AWS Libfabric/1(0)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 02/0 : 3[a01d0] -> 0[201d0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 00/0 : 0[201d0] -> 1[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 02/0 : 0[201d0] -> 1[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 00/0 : 3[a01c0] -> 0[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 02/0 : 3[a01d0] -> 0[201d0] [send] via NET/AWS Libfabric/1(2)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 02/0 : 3[901d0] -> 0[101d0] [send] via NET/AWS Libfabric/0(2)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 02/0 : 1[a01d0] -> 2[201d0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 02/0 : 1[901d0] -> 2[101d0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 00/0 : 2[101d0] -> 3[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 00/0 : 2[201d0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 02/0 : 2[101d0] -> 3[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 02/0 : 2[201d0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 02/0 : 1[901d0] -> 2[101d0] [send] via NET/AWS Libfabric/0(0)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 02/0 : 3[901d0] -> 0[101d0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 00/0 : 0[101d0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 02/0 : 0[101d0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 00/0 : 3[a01c0] -> 0[201c0] [send] via NET/AWS Libfabric/1(2)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 02/0 : 1[a01c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 3[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 3[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 02/0 : 1[a01c0] -> 2[201c0] [send] via NET/AWS Libfabric/1(0)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 02/0 : 3[a01c0] -> 0[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 00/0 : 0[201c0] -> 1[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 02/0 : 0[201c0] -> 1[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 02/0 : 3[a01c0] -> 0[201c0] [send] via NET/AWS Libfabric/1(2)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 01/0 : 2[201d0] -> 1[a01d0] [send] via NET/AWS Libfabric/3(3)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 01/0 : 0[101d0] -> 3[901d0] [send] via NET/AWS Libfabric/2(1)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 01/0 : 0[201c0] -> 3[a01c0] [send] via NET/AWS Libfabric/3(1)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 1[a01c0] [send] via NET/AWS Libfabric/3(3)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 01/0 : 0[201d0] -> 3[a01d0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 01/0 : 2[101d0] -> 1[901d0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 01/0 : 0[201d0] -> 3[a01d0] [send] via NET/AWS Libfabric/3(1)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 01/0 : 2[201c0] -> 1[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 01/0 : 0[201c0] -> 3[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 01/0 : 2[101d0] -> 1[901d0] [send] via NET/AWS Libfabric/2(3)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 03/0 : 2[201d0] -> 1[a01d0] [send] via NET/AWS Libfabric/3(3)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 03/0 : 0[201d0] -> 3[a01d0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 01/0 : 3[a01d0] -> 2[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 03/0 : 3[a01d0] -> 2[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 01/0 : 2[201d0] -> 1[a01d0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 03/0 : 0[101d0] -> 3[901d0] [send] via NET/AWS Libfabric/2(1)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 03/0 : 2[101d0] -> 1[901d0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 01/0 : 1[901d0] -> 0[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 03/0 : 1[901d0] -> 0[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 1[a01c0] [send] via NET/AWS Libfabric/3(3)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 03/0 : 0[201c0] -> 3[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 01/0 : 3[a01c0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 03/0 : 3[a01c0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 03/0 : 0[201c0] -> 3[a01c0] [send] via NET/AWS Libfabric/3(1)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 03/0 : 2[201c0] -> 1[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 01/0 : 1[a01c0] -> 0[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 03/0 : 1[a01c0] -> 0[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 01/0 : 0[101d0] -> 3[901d0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 03/0 : 0[201d0] -> 3[a01d0] [send] via NET/AWS Libfabric/3(1)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 03/0 : 2[201d0] -> 1[a01d0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 01/0 : 1[a01d0] -> 0[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 03/0 : 1[a01d0] -> 0[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 03/0 : 2[101d0] -> 1[901d0] [send] via NET/AWS Libfabric/2(3)/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 03/0 : 0[101d0] -> 3[901d0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 01/0 : 3[901d0] -> 2[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 03/0 : 3[901d0] -> 2[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 3[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 3[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 01/0 : 0[201c0] -> 1[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 03/0 : 0[201c0] -> 1[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 01/0 : 2[201d0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 03/0 : 2[201d0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 01/0 : 0[201d0] -> 1[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 03/0 : 0[201d0] -> 1[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 01/0 : 1[a01c0] -> 3[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 0[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 01/0 : 3[a01c0] -> 1[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 00/0 : 0[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 01/0 : 0[101d0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 03/0 : 0[101d0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 01/0 : 2[101d0] -> 3[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 03/0 : 2[101d0] -> 3[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 00/0 : 0[201d0] -> 2[201d0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 01/0 : 1[a01d0] -> 3[a01d0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 01/0 : 3[a01d0] -> 1[a01d0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 00/0 : 2[201d0] -> 0[201d0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 0[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 03/0 : 3[a01c0] -> 1[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 03/0 : 1[a01c0] -> 3[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 02/0 : 0[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 00/0 : 2[101d0] -> 0[101d0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 01/0 : 3[901d0] -> 1[901d0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 01/0 : 1[901d0] -> 3[901d0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 00/0 : 0[101d0] -> 2[101d0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 02/0 : 0[201d0] -> 2[201d0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 03/0 : 1[a01d0] -> 3[a01d0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 02/0 : 2[201d0] -> 0[201d0] [receive] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 03/0 : 3[a01d0] -> 1[a01d0] [receive] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 01/0 : 1[a01c0] -> 3[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 00/0 : 0[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 0[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 01/0 : 3[a01c0] -> 1[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 03/0 : 3[901d0] -> 1[901d0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 02/0 : 2[101d0] -> 0[101d0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 00/0 : 2[201d0] -> 0[201d0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 01/0 : 3[a01d0] -> 1[a01d0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 03/0 : 1[901d0] -> 3[901d0] [receive] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 02/0 : 0[101d0] -> 2[101d0] [receive] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 01/0 : 1[a01d0] -> 3[a01d0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 00/0 : 0[201d0] -> 2[201d0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 03/0 : 1[a01c0] -> 3[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Channel 02/0 : 0[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 03/0 : 3[a01c0] -> 1[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 0[201c0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 00/0 : 0[101d0] -> 2[101d0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 00/0 : 1[a01c0] -> 0[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Channel 02/0 : 1[a01c0] -> 0[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 00/0 : 3[a01c0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Channel 02/0 : 3[a01c0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 01/0 : 1[901d0] -> 3[901d0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 03/0 : 1[a01d0] -> 3[a01d0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Channel 02/0 : 2[201d0] -> 0[201d0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 03/0 : 3[a01d0] -> 1[a01d0] [send] via NET/AWS Libfabric/3/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 01/0 : 3[901d0] -> 1[901d0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 00/0 : 2[101d0] -> 0[101d0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 00/0 : 1[a01d0] -> 0[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Channel 02/0 : 0[201d0] -> 2[201d0] [send] via NET/AWS Libfabric/1/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Channel 02/0 : 1[a01d0] -> 0[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 00/0 : 3[a01d0] -> 2[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Channel 02/0 : 3[a01d0] -> 2[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Channel 02/0 : 0[101d0] -> 2[101d0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Channel 02/0 : 2[101d0] -> 0[101d0] [send] via NET/AWS Libfabric/0/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 03/0 : 1[901d0] -> 3[901d0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 03/0 : 3[901d0] -> 1[901d0] [send] via NET/AWS Libfabric/2/GDRDMA\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 00/0 : 3[901d0] -> 2[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Channel 02/0 : 3[901d0] -> 2[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 00/0 : 1[901d0] -> 0[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Channel 02/0 : 1[901d0] -> 0[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:782 [6] NCCL INFO comm 0x557e94c39980 rank 3 nranks 4 cudaDev 6 busId a01c0 commId 0xf777028ae92d7c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:771 [2] NCCL INFO comm 0x5557ac78c7a0 rank 2 nranks 4 cudaDev 2 busId 201c0 commId 0xf777028ae92d7c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:832 [6] NCCL INFO comm 0x55d776119d10 rank 1 nranks 4 cudaDev 6 busId a01c0 commId 0xf777028ae92d7c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:829 [2] NCCL INFO comm 0x56041db4e4b0 rank 0 nranks 4 cudaDev 2 busId 201c0 commId 0xf777028ae92d7c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:831 [7] NCCL INFO comm 0x557f702daaa0 rank 1 nranks 4 cudaDev 7 busId a01d0 commId 0x7b0c1a2a4035b74b - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:830 [3] NCCL INFO comm 0x55d61c991d30 rank 0 nranks 4 cudaDev 3 busId 201d0 commId 0x7b0c1a2a4035b74b - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:783 [3] NCCL INFO comm 0x56396cc8c170 rank 2 nranks 4 cudaDev 3 busId 201d0 commId 0x7b0c1a2a4035b74b - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:784 [7] NCCL INFO comm 0x564f01034430 rank 3 nranks 4 cudaDev 7 busId a01d0 commId 0x7b0c1a2a4035b74b - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:781 [5] NCCL INFO comm 0x5567774bc6a0 rank 3 nranks 4 cudaDev 5 busId 901d0 commId 0xacad97d456d1f619 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:779 [1] NCCL INFO comm 0x560df91e3e20 rank 2 nranks 4 cudaDev 1 busId 101d0 commId 0xacad97d456d1f619 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:825 [1] NCCL INFO comm 0x55d02a6b5750 rank 0 nranks 4 cudaDev 1 busId 101d0 commId 0xacad97d456d1f619 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:826 [5] NCCL INFO comm 0x555d3af05940 rank 1 nranks 4 cudaDev 5 busId 901d0 commId 0xacad97d456d1f619 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 00/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 01/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 02/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 03/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 04/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 05/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 06/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 07/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 08/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 09/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 10/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 11/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 12/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 13/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 14/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 15/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 16/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 17/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 18/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 19/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 20/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 21/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 22/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 23/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 00/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 01/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 02/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 03/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 04/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 05/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 06/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 07/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 08/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 09/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 10/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 11/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 12/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 13/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 14/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 15/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 16/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 17/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 18/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 19/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 20/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 21/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 22/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 23/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 00/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 01/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 02/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 03/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 04/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 05/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 06/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 07/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 08/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 09/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 10/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 11/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 12/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 13/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 14/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 15/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 16/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 17/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 18/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 19/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 20/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 21/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 22/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 23/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 08/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 09/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 10/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 11/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 12/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 13/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 14/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 15/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 00/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 16/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 17/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 18/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 01/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 00/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 00/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 19/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 20/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 02/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 21/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 01/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 01/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 22/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 00/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 03/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Channel 23/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 02/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 02/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 01/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 04/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 03/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 03/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 05/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 02/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 04/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 04/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 06/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 03/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 05/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 05/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 07/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 04/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 06/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 06/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 08/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 05/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 07/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 07/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 09/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 06/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 08/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 08/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 08/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 10/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 09/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 07/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 09/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 09/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 11/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 00/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 01/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 02/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 03/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 04/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 05/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 06/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 07/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 08/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 09/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 10/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 11/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 12/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 13/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 14/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 15/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 16/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 17/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 18/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 19/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 20/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 21/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 22/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 23/24 :    0   1   2   3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 10/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 08/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 10/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 10/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 12/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 11/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 11/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 09/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 11/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 13/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 12/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 12/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 10/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 12/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 14/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 13/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 13/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 13/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 11/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 15/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 14/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 14/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 16/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 12/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 14/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 15/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 15/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 17/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 13/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 15/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 16/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 16/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 18/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 14/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 16/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 17/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 17/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 19/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 15/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 17/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 18/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 18/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 20/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 19/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 16/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 18/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 19/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 21/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 20/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 19/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 20/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 17/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 22/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 21/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 20/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 21/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 18/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 23/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 22/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 21/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 22/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 19/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 23/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 22/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Channel 23/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 20/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Channel 23/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 21/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 22/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 23/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 00/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 00/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 00/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 01/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 00/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 01/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 01/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 02/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 01/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 02/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 02/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 03/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 02/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 03/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 04/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 03/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 03/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 04/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 05/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 04/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 04/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 05/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 06/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 05/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 05/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 06/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 07/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 06/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 06/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 07/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 08/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 07/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 07/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 08/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 09/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 08/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 08/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 09/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 10/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 09/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 09/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 10/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 11/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 10/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 10/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 11/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 12/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 11/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 11/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 12/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 13/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 12/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 12/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 13/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 14/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 13/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 13/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 14/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 15/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 14/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 14/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 15/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 16/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 15/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 15/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 16/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 17/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 16/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 16/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 17/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 18/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 17/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 17/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 18/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 19/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 18/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 18/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 19/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 20/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 19/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 19/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 20/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 21/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 20/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 20/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 21/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 22/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 21/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 21/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 22/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 22/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 23/0 : 2[a01c0] -> 3[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 22/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Channel 23/0 : 0[901c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 23/0 : 1[901d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 23/0 : 3[a01d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 00/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 01/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 02/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 03/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 04/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 05/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 06/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 07/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 08/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 09/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 10/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 11/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 00/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 12/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 01/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 13/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 02/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 14/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 00/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 15/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 03/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 04/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 05/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 06/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 07/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 16/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 01/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 08/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 02/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 17/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 03/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 09/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 18/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 04/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 05/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 10/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 06/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 07/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 11/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 08/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 19/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 09/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 12/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 20/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 10/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 21/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 13/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 11/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 22/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 12/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Channel 23/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 14/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 13/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 00/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 14/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 01/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 15/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 15/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 02/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 16/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 03/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 16/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 17/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 04/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 18/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 05/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 17/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 19/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 06/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 20/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 07/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 18/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 21/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 08/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 22/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 19/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 09/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Channel 23/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 10/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 20/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 11/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 12/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 21/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 13/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 14/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 22/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 15/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Channel 23/0 : 3[a01d0] -> 2[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 00/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 16/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 01/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 02/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 03/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 17/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 04/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 18/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 05/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 06/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 07/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 00/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 08/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 01/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 09/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 02/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 10/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 11/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 03/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 12/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 04/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 13/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 05/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 14/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 06/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 15/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 16/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 07/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 17/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 08/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 18/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 09/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 19/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 10/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 20/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 21/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 11/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 19/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 22/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 20/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 12/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Channel 23/0 : 1[901d0] -> 0[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 13/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 14/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 15/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 16/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 17/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 18/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 19/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 21/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 20/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 22/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 21/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Channel 23/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 22/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Channel 23/0 : 2[a01c0] -> 1[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:algo-1:80:855 [0] NCCL INFO comm 0x560dbff54ca0 rank 0 nranks 4 cudaDev 0 busId 101c0 commId 0x4d4fe94176973e4d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:algo-1:82:862 [2] NCCL INFO comm 0x55579d5476d0 rank 2 nranks 4 cudaDev 2 busId 201c0 commId 0x4d4fe94176973e4d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:algo-1:81:857 [1] NCCL INFO comm 0x560df923b200 rank 1 nranks 4 cudaDev 1 busId 101d0 commId 0x4d4fe94176973e4d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:algo-1:83:858 [3] NCCL INFO comm 0x56396ccbbff0 rank 3 nranks 4 cudaDev 3 busId 201d0 commId 0x4d4fe94176973e4d - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:algo-1:87:860 [7] NCCL INFO comm 0x564f01086910 rank 3 nranks 4 cudaDev 7 busId a01d0 commId 0x2e28e921e6697c05 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:algo-1:84:856 [4] NCCL INFO comm 0x55681f3e9c30 rank 0 nranks 4 cudaDev 4 busId 901c0 commId 0x2e28e921e6697c05 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:algo-1:86:859 [6] NCCL INFO comm 0x557e853e79b0 rank 2 nranks 4 cudaDev 6 busId a01c0 commId 0x2e28e921e6697c05 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:algo-1:85:861 [5] NCCL INFO comm 0x5567774e5180 rank 1 nranks 4 cudaDev 5 busId 901d0 commId 0x2e28e921e6697c05 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:algo-2:82:906 [1] NCCL INFO comm 0x55d01c1dbfb0 rank 1 nranks 4 cudaDev 1 busId 101d0 commId 0x25547236f4f1187c - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:algo-2:83:908 [2] NCCL INFO comm 0x56041dba9020 rank 2 nranks 4 cudaDev 2 busId 201c0 commId 0x25547236f4f1187c - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:algo-2:84:905 [3] NCCL INFO comm 0x55d61a49d4e0 rank 3 nranks 4 cudaDev 3 busId 201d0 commId 0x25547236f4f1187c - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:algo-2:81:903 [0] NCCL INFO comm 0x55d68a6f8fc0 rank 0 nranks 4 cudaDev 0 busId 101c0 commId 0x25547236f4f1187c - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:algo-2:87:909 [6] NCCL INFO comm 0x55d776142740 rank 2 nranks 4 cudaDev 6 busId a01c0 commId 0x61c30a8a25a26aa8 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:algo-2:85:904 [4] NCCL INFO comm 0x561ac0672e40 rank 0 nranks 4 cudaDev 4 busId 901c0 commId 0x61c30a8a25a26aa8 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:algo-2:86:910 [5] NCCL INFO comm 0x555d3af524d0 rank 1 nranks 4 cudaDev 5 busId 901d0 commId 0x61c30a8a25a26aa8 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:algo-2:88:907 [7] NCCL INFO comm 0x557f70300e10 rank 3 nranks 4 cudaDev 7 busId a01d0 commId 0x61c30a8a25a26aa8 - Init COMPLETE\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration       10/    4000 | consumed samples:         1920 | elapsed time per iteration (ms): 636.4 | learning rate: 0.000E+00 | global batch size:   192 | loss scale: 8388608.0 | number of skipped iterations:  10 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration       20/    4000 | consumed samples:         3840 | elapsed time per iteration (ms): 259.8 | learning rate: 5.000E-07 | global batch size:   192 | lm loss: 1.052520E+01 | loss scale: 32768.0 | grad norm: 70.189 | number of skipped iterations:   8 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[Rank 3] (after 20 iterations) memory (MB) | allocated: 698.9130859375 | max allocated: 6107.9267578125 | reserved: 6598.0 | max reserved: 6598.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[Rank 0] (after 20 iterations) memory (MB) | allocated: 698.9130859375 | max allocated: 6107.9267578125 | reserved: 6598.0 | max reserved: 6598.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[Rank 1] (after 20 iterations) memory (MB) | allocated: 698.9130859375 | max allocated: 6107.9267578125 | reserved: 6598.0 | max reserved: 6598.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[Rank 2] (after 20 iterations) memory (MB) | allocated: 698.9130859375 | max allocated: 6107.9267578125 | reserved: 6598.0 | max reserved: 6598.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration       30/    4000 | consumed samples:         5760 | elapsed time per iteration (ms): 258.5 | learning rate: 3.000E-06 | global batch size:   192 | lm loss: 9.432549E+00 | loss scale: 32768.0 | grad norm: 31.154 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration       40/    4000 | consumed samples:         7680 | elapsed time per iteration (ms): 258.4 | learning rate: 5.500E-06 | global batch size:   192 | lm loss: 7.168327E+00 | loss scale: 32768.0 | grad norm: 15.022 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration       50/    4000 | consumed samples:         9600 | elapsed time per iteration (ms): 268.2 | learning rate: 8.000E-06 | global batch size:   192 | lm loss: 6.532948E+00 | loss scale: 32768.0 | grad norm: 2.974 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration       60/    4000 | consumed samples:        11520 | elapsed time per iteration (ms): 258.3 | learning rate: 1.050E-05 | global batch size:   192 | lm loss: 6.168018E+00 | loss scale: 32768.0 | grad norm: 1.952 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration       70/    4000 | consumed samples:        13440 | elapsed time per iteration (ms): 259.2 | learning rate: 1.300E-05 | global batch size:   192 | lm loss: 5.947585E+00 | loss scale: 32768.0 | grad norm: 1.578 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration       80/    4000 | consumed samples:        15360 | elapsed time per iteration (ms): 259.2 | learning rate: 1.550E-05 | global batch size:   192 | lm loss: 5.756058E+00 | loss scale: 32768.0 | grad norm: 1.610 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration       90/    4000 | consumed samples:        17280 | elapsed time per iteration (ms): 259.3 | learning rate: 1.800E-05 | global batch size:   192 | lm loss: 5.608166E+00 | loss scale: 32768.0 | grad norm: 1.504 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      100/    4000 | consumed samples:        19200 | elapsed time per iteration (ms): 257.0 | learning rate: 2.050E-05 | global batch size:   192 | lm loss: 5.392398E+00 | loss scale: 32768.0 | grad norm: 2.312 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      110/    4000 | consumed samples:        21120 | elapsed time per iteration (ms): 258.3 | learning rate: 2.300E-05 | global batch size:   192 | lm loss: 5.190789E+00 | loss scale: 32768.0 | grad norm: 1.744 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      120/    4000 | consumed samples:        23040 | elapsed time per iteration (ms): 255.8 | learning rate: 2.550E-05 | global batch size:   192 | lm loss: 5.034251E+00 | loss scale: 32768.0 | grad norm: 1.332 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      130/    4000 | consumed samples:        24960 | elapsed time per iteration (ms): 260.3 | learning rate: 2.800E-05 | global batch size:   192 | lm loss: 4.940026E+00 | loss scale: 32768.0 | grad norm: 1.508 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      140/    4000 | consumed samples:        26880 | elapsed time per iteration (ms): 256.8 | learning rate: 3.050E-05 | global batch size:   192 | lm loss: 4.735176E+00 | loss scale: 32768.0 | grad norm: 1.400 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      150/    4000 | consumed samples:        28800 | elapsed time per iteration (ms): 259.1 | learning rate: 3.300E-05 | global batch size:   192 | lm loss: 4.587415E+00 | loss scale: 32768.0 | grad norm: 2.084 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      160/    4000 | consumed samples:        30720 | elapsed time per iteration (ms): 259.1 | learning rate: 3.550E-05 | global batch size:   192 | lm loss: 4.446881E+00 | loss scale: 32768.0 | grad norm: 2.273 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      170/    4000 | consumed samples:        32640 | elapsed time per iteration (ms): 255.4 | learning rate: 3.800E-05 | global batch size:   192 | lm loss: 4.272594E+00 | loss scale: 32768.0 | grad norm: 1.493 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      180/    4000 | consumed samples:        34560 | elapsed time per iteration (ms): 255.7 | learning rate: 4.050E-05 | global batch size:   192 | lm loss: 4.212723E+00 | loss scale: 32768.0 | grad norm: 1.285 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      190/    4000 | consumed samples:        36480 | elapsed time per iteration (ms): 255.6 | learning rate: 4.300E-05 | global batch size:   192 | lm loss: 4.073534E+00 | loss scale: 32768.0 | grad norm: 1.145 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      200/    4000 | consumed samples:        38400 | elapsed time per iteration (ms): 255.3 | learning rate: 4.550E-05 | global batch size:   192 | lm loss: 4.007561E+00 | loss scale: 32768.0 | grad norm: 1.801 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-----------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 200 | lm loss value: 3.980814E+00 | lm loss PPL: 5.356064E+01 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-----------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      210/    4000 | consumed samples:        40320 | elapsed time per iteration (ms): 405.4 | learning rate: 4.800E-05 | global batch size:   192 | lm loss: 3.970069E+00 | loss scale: 32768.0 | grad norm: 1.272 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      220/    4000 | consumed samples:        42240 | elapsed time per iteration (ms): 255.8 | learning rate: 5.050E-05 | global batch size:   192 | lm loss: 3.918336E+00 | loss scale: 32768.0 | grad norm: 1.394 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      230/    4000 | consumed samples:        44160 | elapsed time per iteration (ms): 255.6 | learning rate: 5.300E-05 | global batch size:   192 | lm loss: 3.880859E+00 | loss scale: 32768.0 | grad norm: 1.190 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      240/    4000 | consumed samples:        46080 | elapsed time per iteration (ms): 257.5 | learning rate: 5.550E-05 | global batch size:   192 | lm loss: 3.789032E+00 | loss scale: 32768.0 | grad norm: 1.548 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      250/    4000 | consumed samples:        48000 | elapsed time per iteration (ms): 258.3 | learning rate: 5.800E-05 | global batch size:   192 | lm loss: 3.751496E+00 | loss scale: 32768.0 | grad norm: 2.048 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      260/    4000 | consumed samples:        49920 | elapsed time per iteration (ms): 256.3 | learning rate: 6.050E-05 | global batch size:   192 | lm loss: 3.701163E+00 | loss scale: 32768.0 | grad norm: 1.654 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      270/    4000 | consumed samples:        51840 | elapsed time per iteration (ms): 257.2 | learning rate: 6.300E-05 | global batch size:   192 | lm loss: 3.711072E+00 | loss scale: 32768.0 | grad norm: 1.416 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      280/    4000 | consumed samples:        53760 | elapsed time per iteration (ms): 257.0 | learning rate: 6.550E-05 | global batch size:   192 | lm loss: 3.631817E+00 | loss scale: 32768.0 | grad norm: 1.140 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      290/    4000 | consumed samples:        55680 | elapsed time per iteration (ms): 257.1 | learning rate: 6.800E-05 | global batch size:   192 | lm loss: 3.621877E+00 | loss scale: 32768.0 | grad norm: 1.045 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      300/    4000 | consumed samples:        57600 | elapsed time per iteration (ms): 259.1 | learning rate: 7.050E-05 | global batch size:   192 | lm loss: 3.595134E+00 | loss scale: 32768.0 | grad norm: 1.563 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      310/    4000 | consumed samples:        59520 | elapsed time per iteration (ms): 257.1 | learning rate: 7.300E-05 | global batch size:   192 | lm loss: 3.553894E+00 | loss scale: 32768.0 | grad norm: 2.123 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      320/    4000 | consumed samples:        61440 | elapsed time per iteration (ms): 257.3 | learning rate: 7.550E-05 | global batch size:   192 | lm loss: 3.505318E+00 | loss scale: 32768.0 | grad norm: 1.394 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      330/    4000 | consumed samples:        63360 | elapsed time per iteration (ms): 257.1 | learning rate: 7.800E-05 | global batch size:   192 | lm loss: 3.533701E+00 | loss scale: 32768.0 | grad norm: 1.068 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      340/    4000 | consumed samples:        65280 | elapsed time per iteration (ms): 257.4 | learning rate: 8.050E-05 | global batch size:   192 | lm loss: 3.477990E+00 | loss scale: 32768.0 | grad norm: 1.258 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      350/    4000 | consumed samples:        67200 | elapsed time per iteration (ms): 257.3 | learning rate: 8.300E-05 | global batch size:   192 | lm loss: 3.446833E+00 | loss scale: 32768.0 | grad norm: 1.516 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      360/    4000 | consumed samples:        69120 | elapsed time per iteration (ms): 257.0 | learning rate: 8.550E-05 | global batch size:   192 | lm loss: 3.407000E+00 | loss scale: 32768.0 | grad norm: 1.369 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      370/    4000 | consumed samples:        71040 | elapsed time per iteration (ms): 256.9 | learning rate: 8.800E-05 | global batch size:   192 | lm loss: 3.353896E+00 | loss scale: 32768.0 | grad norm: 1.220 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      380/    4000 | consumed samples:        72960 | elapsed time per iteration (ms): 256.9 | learning rate: 9.050E-05 | global batch size:   192 | lm loss: 3.325163E+00 | loss scale: 32768.0 | grad norm: 1.577 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      390/    4000 | consumed samples:        74880 | elapsed time per iteration (ms): 257.0 | learning rate: 9.300E-05 | global batch size:   192 | lm loss: 3.313964E+00 | loss scale: 32768.0 | grad norm: 1.298 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      400/    4000 | consumed samples:        76800 | elapsed time per iteration (ms): 256.6 | learning rate: 9.550E-05 | global batch size:   192 | lm loss: 3.234482E+00 | loss scale: 32768.0 | grad norm: 1.044 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-----------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 400 | lm loss value: 3.190526E+00 | lm loss PPL: 2.430120E+01 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-----------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      410/    4000 | consumed samples:        78720 | elapsed time per iteration (ms): 372.7 | learning rate: 9.800E-05 | global batch size:   192 | lm loss: 3.197859E+00 | loss scale: 32768.0 | grad norm: 1.520 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      420/    4000 | consumed samples:        80640 | elapsed time per iteration (ms): 257.6 | learning rate: 1.005E-04 | global batch size:   192 | lm loss: 3.181020E+00 | loss scale: 32768.0 | grad norm: 1.071 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      430/    4000 | consumed samples:        82560 | elapsed time per iteration (ms): 263.9 | learning rate: 1.030E-04 | global batch size:   192 | lm loss: 3.166438E+00 | loss scale: 32768.0 | grad norm: 1.110 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      440/    4000 | consumed samples:        84480 | elapsed time per iteration (ms): 255.1 | learning rate: 1.055E-04 | global batch size:   192 | lm loss: 3.151872E+00 | loss scale: 32768.0 | grad norm: 1.913 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      450/    4000 | consumed samples:        86400 | elapsed time per iteration (ms): 259.4 | learning rate: 1.080E-04 | global batch size:   192 | lm loss: 3.102295E+00 | loss scale: 32768.0 | grad norm: 2.185 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      460/    4000 | consumed samples:        88320 | elapsed time per iteration (ms): 257.0 | learning rate: 1.105E-04 | global batch size:   192 | lm loss: 3.067071E+00 | loss scale: 32768.0 | grad norm: 1.555 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      470/    4000 | consumed samples:        90240 | elapsed time per iteration (ms): 254.8 | learning rate: 1.130E-04 | global batch size:   192 | lm loss: 3.005741E+00 | loss scale: 32768.0 | grad norm: 1.689 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      480/    4000 | consumed samples:        92160 | elapsed time per iteration (ms): 254.6 | learning rate: 1.155E-04 | global batch size:   192 | lm loss: 2.998676E+00 | loss scale: 32768.0 | grad norm: 1.763 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      490/    4000 | consumed samples:        94080 | elapsed time per iteration (ms): 255.3 | learning rate: 1.180E-04 | global batch size:   192 | lm loss: 2.991992E+00 | loss scale: 32768.0 | grad norm: 1.371 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      500/    4000 | consumed samples:        96000 | elapsed time per iteration (ms): 257.3 | learning rate: 1.205E-04 | global batch size:   192 | lm loss: 2.947637E+00 | loss scale: 32768.0 | grad norm: 1.488 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      510/    4000 | consumed samples:        97920 | elapsed time per iteration (ms): 254.6 | learning rate: 1.230E-04 | global batch size:   192 | lm loss: 2.934837E+00 | loss scale: 32768.0 | grad norm: 1.743 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      520/    4000 | consumed samples:        99840 | elapsed time per iteration (ms): 256.2 | learning rate: 1.255E-04 | global batch size:   192 | lm loss: 2.901123E+00 | loss scale: 32768.0 | grad norm: 1.453 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      530/    4000 | consumed samples:       101760 | elapsed time per iteration (ms): 255.4 | learning rate: 1.280E-04 | global batch size:   192 | lm loss: 2.874830E+00 | loss scale: 32768.0 | grad norm: 1.359 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      540/    4000 | consumed samples:       103680 | elapsed time per iteration (ms): 254.8 | learning rate: 1.305E-04 | global batch size:   192 | lm loss: 2.882224E+00 | loss scale: 32768.0 | grad norm: 1.408 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      550/    4000 | consumed samples:       105600 | elapsed time per iteration (ms): 254.4 | learning rate: 1.330E-04 | global batch size:   192 | lm loss: 2.826185E+00 | loss scale: 32768.0 | grad norm: 1.087 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      560/    4000 | consumed samples:       107520 | elapsed time per iteration (ms): 254.7 | learning rate: 1.355E-04 | global batch size:   192 | lm loss: 2.823030E+00 | loss scale: 32768.0 | grad norm: 1.219 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      570/    4000 | consumed samples:       109440 | elapsed time per iteration (ms): 254.9 | learning rate: 1.380E-04 | global batch size:   192 | lm loss: 2.792744E+00 | loss scale: 32768.0 | grad norm: 1.298 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      580/    4000 | consumed samples:       111360 | elapsed time per iteration (ms): 257.4 | learning rate: 1.405E-04 | global batch size:   192 | lm loss: 2.745389E+00 | loss scale: 32768.0 | grad norm: 1.401 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      590/    4000 | consumed samples:       113280 | elapsed time per iteration (ms): 257.6 | learning rate: 1.430E-04 | global batch size:   192 | lm loss: 2.729728E+00 | loss scale: 32768.0 | grad norm: 0.967 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      600/    4000 | consumed samples:       115200 | elapsed time per iteration (ms): 254.5 | learning rate: 1.455E-04 | global batch size:   192 | lm loss: 2.706213E+00 | loss scale: 32768.0 | grad norm: 1.364 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-----------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 600 | lm loss value: 2.667739E+00 | lm loss PPL: 1.440736E+01 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-----------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      610/    4000 | consumed samples:       117120 | elapsed time per iteration (ms): 364.6 | learning rate: 1.480E-04 | global batch size:   192 | lm loss: 2.685122E+00 | loss scale: 32768.0 | grad norm: 1.067 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      620/    4000 | consumed samples:       119040 | elapsed time per iteration (ms): 254.7 | learning rate: 1.505E-04 | global batch size:   192 | lm loss: 2.630623E+00 | loss scale: 32768.0 | grad norm: 1.191 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      630/    4000 | consumed samples:       120960 | elapsed time per iteration (ms): 254.6 | learning rate: 1.530E-04 | global batch size:   192 | lm loss: 2.614223E+00 | loss scale: 32768.0 | grad norm: 1.222 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      640/    4000 | consumed samples:       122880 | elapsed time per iteration (ms): 254.4 | learning rate: 1.555E-04 | global batch size:   192 | lm loss: 2.603975E+00 | loss scale: 32768.0 | grad norm: 1.943 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      650/    4000 | consumed samples:       124800 | elapsed time per iteration (ms): 254.7 | learning rate: 1.580E-04 | global batch size:   192 | lm loss: 2.577216E+00 | loss scale: 32768.0 | grad norm: 0.944 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      660/    4000 | consumed samples:       126720 | elapsed time per iteration (ms): 256.4 | learning rate: 1.605E-04 | global batch size:   192 | lm loss: 2.542206E+00 | loss scale: 32768.0 | grad norm: 1.170 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      670/    4000 | consumed samples:       128640 | elapsed time per iteration (ms): 256.6 | learning rate: 1.630E-04 | global batch size:   192 | lm loss: 2.507201E+00 | loss scale: 32768.0 | grad norm: 1.016 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      680/    4000 | consumed samples:       130560 | elapsed time per iteration (ms): 256.6 | learning rate: 1.655E-04 | global batch size:   192 | lm loss: 2.464544E+00 | loss scale: 32768.0 | grad norm: 1.094 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      690/    4000 | consumed samples:       132480 | elapsed time per iteration (ms): 256.8 | learning rate: 1.680E-04 | global batch size:   192 | lm loss: 2.459617E+00 | loss scale: 32768.0 | grad norm: 1.241 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      700/    4000 | consumed samples:       134400 | elapsed time per iteration (ms): 256.5 | learning rate: 1.705E-04 | global batch size:   192 | lm loss: 2.439710E+00 | loss scale: 32768.0 | grad norm: 1.413 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      710/    4000 | consumed samples:       136320 | elapsed time per iteration (ms): 256.7 | learning rate: 1.730E-04 | global batch size:   192 | lm loss: 2.405395E+00 | loss scale: 32768.0 | grad norm: 1.324 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      720/    4000 | consumed samples:       138240 | elapsed time per iteration (ms): 256.2 | learning rate: 1.755E-04 | global batch size:   192 | lm loss: 2.383860E+00 | loss scale: 32768.0 | grad norm: 1.073 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      730/    4000 | consumed samples:       140160 | elapsed time per iteration (ms): 276.2 | learning rate: 1.780E-04 | global batch size:   192 | lm loss: 2.341612E+00 | loss scale: 32768.0 | grad norm: 1.070 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      740/    4000 | consumed samples:       142080 | elapsed time per iteration (ms): 255.4 | learning rate: 1.805E-04 | global batch size:   192 | lm loss: 2.313253E+00 | loss scale: 32768.0 | grad norm: 1.127 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      750/    4000 | consumed samples:       144000 | elapsed time per iteration (ms): 255.3 | learning rate: 1.830E-04 | global batch size:   192 | lm loss: 2.242005E+00 | loss scale: 32768.0 | grad norm: 1.418 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      760/    4000 | consumed samples:       145920 | elapsed time per iteration (ms): 255.4 | learning rate: 1.855E-04 | global batch size:   192 | lm loss: 2.276093E+00 | loss scale: 32768.0 | grad norm: 1.273 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      770/    4000 | consumed samples:       147840 | elapsed time per iteration (ms): 259.7 | learning rate: 1.880E-04 | global batch size:   192 | lm loss: 2.230406E+00 | loss scale: 32768.0 | grad norm: 1.372 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      780/    4000 | consumed samples:       149760 | elapsed time per iteration (ms): 255.1 | learning rate: 1.905E-04 | global batch size:   192 | lm loss: 2.182733E+00 | loss scale: 32768.0 | grad norm: 1.252 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      790/    4000 | consumed samples:       151680 | elapsed time per iteration (ms): 255.2 | learning rate: 1.930E-04 | global batch size:   192 | lm loss: 2.160387E+00 | loss scale: 32768.0 | grad norm: 1.242 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      800/    4000 | consumed samples:       153600 | elapsed time per iteration (ms): 255.3 | learning rate: 1.955E-04 | global batch size:   192 | lm loss: 2.133379E+00 | loss scale: 32768.0 | grad norm: 1.363 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-----------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 800 | lm loss value: 1.994556E+00 | lm loss PPL: 7.348938E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-----------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      810/    4000 | consumed samples:       155520 | elapsed time per iteration (ms): 365.4 | learning rate: 1.980E-04 | global batch size:   192 | lm loss: 2.090523E+00 | loss scale: 32768.0 | grad norm: 1.305 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      820/    4000 | consumed samples:       157440 | elapsed time per iteration (ms): 255.2 | learning rate: 2.005E-04 | global batch size:   192 | lm loss: 2.074520E+00 | loss scale: 32768.0 | grad norm: 1.087 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      830/    4000 | consumed samples:       159360 | elapsed time per iteration (ms): 255.2 | learning rate: 2.030E-04 | global batch size:   192 | lm loss: 2.061096E+00 | loss scale: 32768.0 | grad norm: 1.297 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      840/    4000 | consumed samples:       161280 | elapsed time per iteration (ms): 255.3 | learning rate: 2.055E-04 | global batch size:   192 | lm loss: 2.042439E+00 | loss scale: 32768.0 | grad norm: 1.281 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      850/    4000 | consumed samples:       163200 | elapsed time per iteration (ms): 255.1 | learning rate: 2.080E-04 | global batch size:   192 | lm loss: 1.984499E+00 | loss scale: 32768.0 | grad norm: 1.525 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      860/    4000 | consumed samples:       165120 | elapsed time per iteration (ms): 255.2 | learning rate: 2.105E-04 | global batch size:   192 | lm loss: 1.990867E+00 | loss scale: 32768.0 | grad norm: 1.051 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      870/    4000 | consumed samples:       167040 | elapsed time per iteration (ms): 256.7 | learning rate: 2.130E-04 | global batch size:   192 | lm loss: 1.975216E+00 | loss scale: 32768.0 | grad norm: 1.099 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      880/    4000 | consumed samples:       168960 | elapsed time per iteration (ms): 255.2 | learning rate: 2.155E-04 | global batch size:   192 | lm loss: 1.937540E+00 | loss scale: 32768.0 | grad norm: 1.105 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      890/    4000 | consumed samples:       170880 | elapsed time per iteration (ms): 257.1 | learning rate: 2.180E-04 | global batch size:   192 | lm loss: 1.933253E+00 | loss scale: 32768.0 | grad norm: 1.081 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      900/    4000 | consumed samples:       172800 | elapsed time per iteration (ms): 254.6 | learning rate: 2.205E-04 | global batch size:   192 | lm loss: 1.886605E+00 | loss scale: 32768.0 | grad norm: 1.008 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      910/    4000 | consumed samples:       174720 | elapsed time per iteration (ms): 254.9 | learning rate: 2.230E-04 | global batch size:   192 | lm loss: 1.900890E+00 | loss scale: 32768.0 | grad norm: 1.464 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      920/    4000 | consumed samples:       176640 | elapsed time per iteration (ms): 256.7 | learning rate: 2.255E-04 | global batch size:   192 | lm loss: 1.876444E+00 | loss scale: 32768.0 | grad norm: 0.838 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      930/    4000 | consumed samples:       178560 | elapsed time per iteration (ms): 254.5 | learning rate: 2.280E-04 | global batch size:   192 | lm loss: 1.876832E+00 | loss scale: 32768.0 | grad norm: 1.184 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      940/    4000 | consumed samples:       180480 | elapsed time per iteration (ms): 254.5 | learning rate: 2.305E-04 | global batch size:   192 | lm loss: 1.870895E+00 | loss scale: 32768.0 | grad norm: 1.180 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      950/    4000 | consumed samples:       182400 | elapsed time per iteration (ms): 254.7 | learning rate: 2.330E-04 | global batch size:   192 | lm loss: 1.882458E+00 | loss scale: 32768.0 | grad norm: 0.778 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      960/    4000 | consumed samples:       184320 | elapsed time per iteration (ms): 254.9 | learning rate: 2.355E-04 | global batch size:   192 | lm loss: 1.827941E+00 | loss scale: 32768.0 | grad norm: 0.960 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      970/    4000 | consumed samples:       186240 | elapsed time per iteration (ms): 254.7 | learning rate: 2.380E-04 | global batch size:   192 | lm loss: 1.835031E+00 | loss scale: 32768.0 | grad norm: 1.085 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      980/    4000 | consumed samples:       188160 | elapsed time per iteration (ms): 254.7 | learning rate: 2.405E-04 | global batch size:   192 | lm loss: 1.790308E+00 | loss scale: 32768.0 | grad norm: 0.995 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration      990/    4000 | consumed samples:       190080 | elapsed time per iteration (ms): 266.0 | learning rate: 2.430E-04 | global batch size:   192 | lm loss: 1.791494E+00 | loss scale: 32768.0 | grad norm: 0.758 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1000/    4000 | consumed samples:       192000 | elapsed time per iteration (ms): 254.7 | learning rate: 2.455E-04 | global batch size:   192 | lm loss: 1.801813E+00 | loss scale: 32768.0 | grad norm: 0.823 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 1000 | lm loss value: 1.706127E+00 | lm loss PPL: 5.507587E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1010/    4000 | consumed samples:       193920 | elapsed time per iteration (ms): 364.7 | learning rate: 2.480E-04 | global batch size:   192 | lm loss: 1.758526E+00 | loss scale: 32768.0 | grad norm: 0.816 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1020/    4000 | consumed samples:       195840 | elapsed time per iteration (ms): 254.9 | learning rate: 2.505E-04 | global batch size:   192 | lm loss: 1.762576E+00 | loss scale: 65536.0 | grad norm: 0.828 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1030/    4000 | consumed samples:       197760 | elapsed time per iteration (ms): 254.3 | learning rate: 2.530E-04 | global batch size:   192 | lm loss: 1.728717E+00 | loss scale: 65536.0 | grad norm: 0.973 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1040/    4000 | consumed samples:       199680 | elapsed time per iteration (ms): 254.0 | learning rate: 2.555E-04 | global batch size:   192 | lm loss: 1.750628E+00 | loss scale: 65536.0 | grad norm: 0.848 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1050/    4000 | consumed samples:       201600 | elapsed time per iteration (ms): 254.5 | learning rate: 2.580E-04 | global batch size:   192 | lm loss: 1.699815E+00 | loss scale: 65536.0 | grad norm: 0.833 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1060/    4000 | consumed samples:       203520 | elapsed time per iteration (ms): 257.4 | learning rate: 2.605E-04 | global batch size:   192 | lm loss: 1.703102E+00 | loss scale: 65536.0 | grad norm: 0.970 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1070/    4000 | consumed samples:       205440 | elapsed time per iteration (ms): 255.2 | learning rate: 2.630E-04 | global batch size:   192 | lm loss: 1.713602E+00 | loss scale: 65536.0 | grad norm: 0.842 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1080/    4000 | consumed samples:       207360 | elapsed time per iteration (ms): 254.2 | learning rate: 2.655E-04 | global batch size:   192 | lm loss: 1.707915E+00 | loss scale: 65536.0 | grad norm: 0.830 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1090/    4000 | consumed samples:       209280 | elapsed time per iteration (ms): 254.5 | learning rate: 2.680E-04 | global batch size:   192 | lm loss: 1.687879E+00 | loss scale: 65536.0 | grad norm: 1.133 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1100/    4000 | consumed samples:       211200 | elapsed time per iteration (ms): 254.5 | learning rate: 2.705E-04 | global batch size:   192 | lm loss: 1.676160E+00 | loss scale: 65536.0 | grad norm: 0.737 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1110/    4000 | consumed samples:       213120 | elapsed time per iteration (ms): 254.6 | learning rate: 2.730E-04 | global batch size:   192 | lm loss: 1.645486E+00 | loss scale: 65536.0 | grad norm: 0.660 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1120/    4000 | consumed samples:       215040 | elapsed time per iteration (ms): 257.1 | learning rate: 2.755E-04 | global batch size:   192 | lm loss: 1.642938E+00 | loss scale: 65536.0 | grad norm: 0.748 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1130/    4000 | consumed samples:       216960 | elapsed time per iteration (ms): 254.2 | learning rate: 2.780E-04 | global batch size:   192 | lm loss: 1.649087E+00 | loss scale: 65536.0 | grad norm: 0.579 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1140/    4000 | consumed samples:       218880 | elapsed time per iteration (ms): 254.0 | learning rate: 2.805E-04 | global batch size:   192 | lm loss: 1.638964E+00 | loss scale: 65536.0 | grad norm: 0.718 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1150/    4000 | consumed samples:       220800 | elapsed time per iteration (ms): 254.4 | learning rate: 2.830E-04 | global batch size:   192 | lm loss: 1.633664E+00 | loss scale: 65536.0 | grad norm: 0.584 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1160/    4000 | consumed samples:       222720 | elapsed time per iteration (ms): 256.4 | learning rate: 2.855E-04 | global batch size:   192 | lm loss: 1.620425E+00 | loss scale: 65536.0 | grad norm: 0.856 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1170/    4000 | consumed samples:       224640 | elapsed time per iteration (ms): 254.1 | learning rate: 2.880E-04 | global batch size:   192 | lm loss: 1.629179E+00 | loss scale: 65536.0 | grad norm: 0.621 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1180/    4000 | consumed samples:       226560 | elapsed time per iteration (ms): 254.6 | learning rate: 2.905E-04 | global batch size:   192 | lm loss: 1.621319E+00 | loss scale: 65536.0 | grad norm: 0.699 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1190/    4000 | consumed samples:       228480 | elapsed time per iteration (ms): 254.1 | learning rate: 2.930E-04 | global batch size:   192 | lm loss: 1.611358E+00 | loss scale: 65536.0 | grad norm: 0.568 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1200/    4000 | consumed samples:       230400 | elapsed time per iteration (ms): 254.3 | learning rate: 2.955E-04 | global batch size:   192 | lm loss: 1.597108E+00 | loss scale: 65536.0 | grad norm: 0.642 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 1200 | lm loss value: 1.542728E+00 | lm loss PPL: 4.677332E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1210/    4000 | consumed samples:       232320 | elapsed time per iteration (ms): 370.5 | learning rate: 2.980E-04 | global batch size:   192 | lm loss: 1.614558E+00 | loss scale: 65536.0 | grad norm: 0.632 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1220/    4000 | consumed samples:       234240 | elapsed time per iteration (ms): 254.3 | learning rate: 3.005E-04 | global batch size:   192 | lm loss: 1.579203E+00 | loss scale: 65536.0 | grad norm: 0.676 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1230/    4000 | consumed samples:       236160 | elapsed time per iteration (ms): 254.6 | learning rate: 3.030E-04 | global batch size:   192 | lm loss: 1.578979E+00 | loss scale: 65536.0 | grad norm: 0.615 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1240/    4000 | consumed samples:       238080 | elapsed time per iteration (ms): 254.3 | learning rate: 3.055E-04 | global batch size:   192 | lm loss: 1.586124E+00 | loss scale: 65536.0 | grad norm: 1.126 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1250/    4000 | consumed samples:       240000 | elapsed time per iteration (ms): 256.1 | learning rate: 3.080E-04 | global batch size:   192 | lm loss: 1.578640E+00 | loss scale: 65536.0 | grad norm: 0.539 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1260/    4000 | consumed samples:       241920 | elapsed time per iteration (ms): 256.0 | learning rate: 3.105E-04 | global batch size:   192 | lm loss: 1.577495E+00 | loss scale: 65536.0 | grad norm: 0.773 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1270/    4000 | consumed samples:       243840 | elapsed time per iteration (ms): 254.3 | learning rate: 3.130E-04 | global batch size:   192 | lm loss: 1.557472E+00 | loss scale: 65536.0 | grad norm: 0.563 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1280/    4000 | consumed samples:       245760 | elapsed time per iteration (ms): 254.3 | learning rate: 3.155E-04 | global batch size:   192 | lm loss: 1.576072E+00 | loss scale: 65536.0 | grad norm: 0.573 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1290/    4000 | consumed samples:       247680 | elapsed time per iteration (ms): 254.0 | learning rate: 3.180E-04 | global batch size:   192 | lm loss: 1.569861E+00 | loss scale: 65536.0 | grad norm: 0.905 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1300/    4000 | consumed samples:       249600 | elapsed time per iteration (ms): 256.2 | learning rate: 3.205E-04 | global batch size:   192 | lm loss: 1.548567E+00 | loss scale: 65536.0 | grad norm: 0.672 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1310/    4000 | consumed samples:       251520 | elapsed time per iteration (ms): 254.1 | learning rate: 3.230E-04 | global batch size:   192 | lm loss: 1.533778E+00 | loss scale: 65536.0 | grad norm: 0.627 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1320/    4000 | consumed samples:       253440 | elapsed time per iteration (ms): 254.3 | learning rate: 3.255E-04 | global batch size:   192 | lm loss: 1.539505E+00 | loss scale: 65536.0 | grad norm: 0.619 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1330/    4000 | consumed samples:       255360 | elapsed time per iteration (ms): 254.0 | learning rate: 3.280E-04 | global batch size:   192 | lm loss: 1.542275E+00 | loss scale: 65536.0 | grad norm: 0.543 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1340/    4000 | consumed samples:       257280 | elapsed time per iteration (ms): 254.4 | learning rate: 3.305E-04 | global batch size:   192 | lm loss: 1.531413E+00 | loss scale: 65536.0 | grad norm: 0.646 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1350/    4000 | consumed samples:       259200 | elapsed time per iteration (ms): 255.3 | learning rate: 3.330E-04 | global batch size:   192 | lm loss: 1.535675E+00 | loss scale: 65536.0 | grad norm: 0.672 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1360/    4000 | consumed samples:       261120 | elapsed time per iteration (ms): 253.9 | learning rate: 3.355E-04 | global batch size:   192 | lm loss: 1.527925E+00 | loss scale: 65536.0 | grad norm: 0.563 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1370/    4000 | consumed samples:       263040 | elapsed time per iteration (ms): 254.3 | learning rate: 3.380E-04 | global batch size:   192 | lm loss: 1.512319E+00 | loss scale: 65536.0 | grad norm: 0.512 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1380/    4000 | consumed samples:       264960 | elapsed time per iteration (ms): 254.2 | learning rate: 3.405E-04 | global batch size:   192 | lm loss: 1.511594E+00 | loss scale: 65536.0 | grad norm: 0.841 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1390/    4000 | consumed samples:       266880 | elapsed time per iteration (ms): 254.4 | learning rate: 3.430E-04 | global batch size:   192 | lm loss: 1.518640E+00 | loss scale: 65536.0 | grad norm: 0.552 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1400/    4000 | consumed samples:       268800 | elapsed time per iteration (ms): 253.9 | learning rate: 3.455E-04 | global batch size:   192 | lm loss: 1.505304E+00 | loss scale: 65536.0 | grad norm: 0.532 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 1400 | lm loss value: 1.455195E+00 | lm loss PPL: 4.285319E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1410/    4000 | consumed samples:       270720 | elapsed time per iteration (ms): 363.5 | learning rate: 3.480E-04 | global batch size:   192 | lm loss: 1.491453E+00 | loss scale: 65536.0 | grad norm: 0.773 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1420/    4000 | consumed samples:       272640 | elapsed time per iteration (ms): 253.5 | learning rate: 3.505E-04 | global batch size:   192 | lm loss: 1.486593E+00 | loss scale: 65536.0 | grad norm: 0.535 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1430/    4000 | consumed samples:       274560 | elapsed time per iteration (ms): 253.6 | learning rate: 3.530E-04 | global batch size:   192 | lm loss: 1.483493E+00 | loss scale: 65536.0 | grad norm: 0.561 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1440/    4000 | consumed samples:       276480 | elapsed time per iteration (ms): 253.9 | learning rate: 3.555E-04 | global batch size:   192 | lm loss: 1.486384E+00 | loss scale: 65536.0 | grad norm: 0.660 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1450/    4000 | consumed samples:       278400 | elapsed time per iteration (ms): 258.6 | learning rate: 3.580E-04 | global batch size:   192 | lm loss: 1.468014E+00 | loss scale: 65536.0 | grad norm: 0.572 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1460/    4000 | consumed samples:       280320 | elapsed time per iteration (ms): 253.8 | learning rate: 3.605E-04 | global batch size:   192 | lm loss: 1.496777E+00 | loss scale: 65536.0 | grad norm: 0.531 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1470/    4000 | consumed samples:       282240 | elapsed time per iteration (ms): 255.9 | learning rate: 3.630E-04 | global batch size:   192 | lm loss: 1.515334E+00 | loss scale: 65536.0 | grad norm: 0.477 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1480/    4000 | consumed samples:       284160 | elapsed time per iteration (ms): 253.5 | learning rate: 3.655E-04 | global batch size:   192 | lm loss: 1.474387E+00 | loss scale: 65536.0 | grad norm: 0.550 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1490/    4000 | consumed samples:       286080 | elapsed time per iteration (ms): 253.6 | learning rate: 3.680E-04 | global batch size:   192 | lm loss: 1.483768E+00 | loss scale: 65536.0 | grad norm: 0.517 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1500/    4000 | consumed samples:       288000 | elapsed time per iteration (ms): 253.6 | learning rate: 3.705E-04 | global batch size:   192 | lm loss: 1.475024E+00 | loss scale: 65536.0 | grad norm: 0.588 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1510/    4000 | consumed samples:       289920 | elapsed time per iteration (ms): 254.3 | learning rate: 3.730E-04 | global batch size:   192 | lm loss: 1.455305E+00 | loss scale: 65536.0 | grad norm: 0.693 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1520/    4000 | consumed samples:       291840 | elapsed time per iteration (ms): 254.8 | learning rate: 3.755E-04 | global batch size:   192 | lm loss: 1.478736E+00 | loss scale: 65536.0 | grad norm: 0.501 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1530/    4000 | consumed samples:       293760 | elapsed time per iteration (ms): 253.8 | learning rate: 3.780E-04 | global batch size:   192 | lm loss: 1.457935E+00 | loss scale: 65536.0 | grad norm: 0.564 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1540/    4000 | consumed samples:       295680 | elapsed time per iteration (ms): 253.5 | learning rate: 3.805E-04 | global batch size:   192 | lm loss: 1.461189E+00 | loss scale: 65536.0 | grad norm: 0.489 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1550/    4000 | consumed samples:       297600 | elapsed time per iteration (ms): 253.9 | learning rate: 3.830E-04 | global batch size:   192 | lm loss: 1.467586E+00 | loss scale: 65536.0 | grad norm: 0.461 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1560/    4000 | consumed samples:       299520 | elapsed time per iteration (ms): 253.7 | learning rate: 3.855E-04 | global batch size:   192 | lm loss: 1.450312E+00 | loss scale: 65536.0 | grad norm: 0.662 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1570/    4000 | consumed samples:       301440 | elapsed time per iteration (ms): 253.7 | learning rate: 3.880E-04 | global batch size:   192 | lm loss: 1.453346E+00 | loss scale: 65536.0 | grad norm: 0.601 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1580/    4000 | consumed samples:       303360 | elapsed time per iteration (ms): 258.1 | learning rate: 3.905E-04 | global batch size:   192 | lm loss: 1.465394E+00 | loss scale: 65536.0 | grad norm: 0.543 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1590/    4000 | consumed samples:       305280 | elapsed time per iteration (ms): 256.3 | learning rate: 3.930E-04 | global batch size:   192 | lm loss: 1.445357E+00 | loss scale: 65536.0 | grad norm: 0.444 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1600/    4000 | consumed samples:       307200 | elapsed time per iteration (ms): 256.1 | learning rate: 3.955E-04 | global batch size:   192 | lm loss: 1.448048E+00 | loss scale: 65536.0 | grad norm: 0.561 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 1600 | lm loss value: 1.361615E+00 | lm loss PPL: 3.902490E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1610/    4000 | consumed samples:       309120 | elapsed time per iteration (ms): 370.6 | learning rate: 3.980E-04 | global batch size:   192 | lm loss: 1.422066E+00 | loss scale: 65536.0 | grad norm: 0.526 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1620/    4000 | consumed samples:       311040 | elapsed time per iteration (ms): 256.3 | learning rate: 4.005E-04 | global batch size:   192 | lm loss: 1.420882E+00 | loss scale: 65536.0 | grad norm: 0.529 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1630/    4000 | consumed samples:       312960 | elapsed time per iteration (ms): 256.4 | learning rate: 4.030E-04 | global batch size:   192 | lm loss: 1.440075E+00 | loss scale: 65536.0 | grad norm: 0.505 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1640/    4000 | consumed samples:       314880 | elapsed time per iteration (ms): 257.5 | learning rate: 4.055E-04 | global batch size:   192 | lm loss: 1.414366E+00 | loss scale: 65536.0 | grad norm: 0.441 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1650/    4000 | consumed samples:       316800 | elapsed time per iteration (ms): 258.2 | learning rate: 4.080E-04 | global batch size:   192 | lm loss: 1.443004E+00 | loss scale: 65536.0 | grad norm: 0.658 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1660/    4000 | consumed samples:       318720 | elapsed time per iteration (ms): 256.3 | learning rate: 4.105E-04 | global batch size:   192 | lm loss: 1.460837E+00 | loss scale: 65536.0 | grad norm: 0.463 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1670/    4000 | consumed samples:       320640 | elapsed time per iteration (ms): 256.1 | learning rate: 4.130E-04 | global batch size:   192 | lm loss: 1.426591E+00 | loss scale: 65536.0 | grad norm: 0.614 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1680/    4000 | consumed samples:       322560 | elapsed time per iteration (ms): 256.3 | learning rate: 4.155E-04 | global batch size:   192 | lm loss: 1.437247E+00 | loss scale: 65536.0 | grad norm: 0.442 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1690/    4000 | consumed samples:       324480 | elapsed time per iteration (ms): 256.1 | learning rate: 4.180E-04 | global batch size:   192 | lm loss: 1.415952E+00 | loss scale: 65536.0 | grad norm: 0.440 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1700/    4000 | consumed samples:       326400 | elapsed time per iteration (ms): 256.0 | learning rate: 4.205E-04 | global batch size:   192 | lm loss: 1.410732E+00 | loss scale: 65536.0 | grad norm: 0.446 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1710/    4000 | consumed samples:       328320 | elapsed time per iteration (ms): 257.3 | learning rate: 4.230E-04 | global batch size:   192 | lm loss: 1.432195E+00 | loss scale: 65536.0 | grad norm: 0.459 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1720/    4000 | consumed samples:       330240 | elapsed time per iteration (ms): 256.1 | learning rate: 4.255E-04 | global batch size:   192 | lm loss: 1.411134E+00 | loss scale: 65536.0 | grad norm: 0.449 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1730/    4000 | consumed samples:       332160 | elapsed time per iteration (ms): 256.2 | learning rate: 4.280E-04 | global batch size:   192 | lm loss: 1.401495E+00 | loss scale: 65536.0 | grad norm: 0.436 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1740/    4000 | consumed samples:       334080 | elapsed time per iteration (ms): 256.0 | learning rate: 4.305E-04 | global batch size:   192 | lm loss: 1.404799E+00 | loss scale: 65536.0 | grad norm: 0.462 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1750/    4000 | consumed samples:       336000 | elapsed time per iteration (ms): 255.9 | learning rate: 4.330E-04 | global batch size:   192 | lm loss: 1.393026E+00 | loss scale: 65536.0 | grad norm: 0.557 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1760/    4000 | consumed samples:       337920 | elapsed time per iteration (ms): 256.0 | learning rate: 4.355E-04 | global batch size:   192 | lm loss: 1.399406E+00 | loss scale: 65536.0 | grad norm: 0.588 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1770/    4000 | consumed samples:       339840 | elapsed time per iteration (ms): 255.9 | learning rate: 4.380E-04 | global batch size:   192 | lm loss: 1.414777E+00 | loss scale: 65536.0 | grad norm: 0.384 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1780/    4000 | consumed samples:       341760 | elapsed time per iteration (ms): 257.2 | learning rate: 4.405E-04 | global batch size:   192 | lm loss: 1.406001E+00 | loss scale: 65536.0 | grad norm: 0.749 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1790/    4000 | consumed samples:       343680 | elapsed time per iteration (ms): 253.9 | learning rate: 4.430E-04 | global batch size:   192 | lm loss: 1.377774E+00 | loss scale: 65536.0 | grad norm: 0.472 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1800/    4000 | consumed samples:       345600 | elapsed time per iteration (ms): 253.6 | learning rate: 4.455E-04 | global batch size:   192 | lm loss: 1.383403E+00 | loss scale: 65536.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 1800 | lm loss value: 1.319043E+00 | lm loss PPL: 3.739841E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1810/    4000 | consumed samples:       347520 | elapsed time per iteration (ms): 366.7 | learning rate: 4.480E-04 | global batch size:   192 | lm loss: 1.370966E+00 | loss scale: 65536.0 | grad norm: 0.454 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1820/    4000 | consumed samples:       349440 | elapsed time per iteration (ms): 255.9 | learning rate: 4.505E-04 | global batch size:   192 | lm loss: 1.388928E+00 | loss scale: 65536.0 | grad norm: 0.478 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1830/    4000 | consumed samples:       351360 | elapsed time per iteration (ms): 258.6 | learning rate: 4.530E-04 | global batch size:   192 | lm loss: 1.363940E+00 | loss scale: 65536.0 | grad norm: 0.431 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1840/    4000 | consumed samples:       353280 | elapsed time per iteration (ms): 255.9 | learning rate: 4.555E-04 | global batch size:   192 | lm loss: 1.386969E+00 | loss scale: 65536.0 | grad norm: 0.447 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1850/    4000 | consumed samples:       355200 | elapsed time per iteration (ms): 256.0 | learning rate: 4.580E-04 | global batch size:   192 | lm loss: 1.391616E+00 | loss scale: 65536.0 | grad norm: 0.588 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1860/    4000 | consumed samples:       357120 | elapsed time per iteration (ms): 256.1 | learning rate: 4.605E-04 | global batch size:   192 | lm loss: 1.385145E+00 | loss scale: 65536.0 | grad norm: 0.400 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1870/    4000 | consumed samples:       359040 | elapsed time per iteration (ms): 256.1 | learning rate: 4.630E-04 | global batch size:   192 | lm loss: 1.383574E+00 | loss scale: 65536.0 | grad norm: 0.409 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1880/    4000 | consumed samples:       360960 | elapsed time per iteration (ms): 255.9 | learning rate: 4.655E-04 | global batch size:   192 | lm loss: 1.365375E+00 | loss scale: 65536.0 | grad norm: 0.365 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1890/    4000 | consumed samples:       362880 | elapsed time per iteration (ms): 263.3 | learning rate: 4.680E-04 | global batch size:   192 | lm loss: 1.381653E+00 | loss scale: 65536.0 | grad norm: 0.450 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1900/    4000 | consumed samples:       364800 | elapsed time per iteration (ms): 270.4 | learning rate: 4.705E-04 | global batch size:   192 | lm loss: 1.365489E+00 | loss scale: 65536.0 | grad norm: 0.446 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1910/    4000 | consumed samples:       366720 | elapsed time per iteration (ms): 255.9 | learning rate: 4.730E-04 | global batch size:   192 | lm loss: 1.377019E+00 | loss scale: 65536.0 | grad norm: 0.548 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1920/    4000 | consumed samples:       368640 | elapsed time per iteration (ms): 255.9 | learning rate: 4.755E-04 | global batch size:   192 | lm loss: 1.383149E+00 | loss scale: 65536.0 | grad norm: 0.400 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1930/    4000 | consumed samples:       370560 | elapsed time per iteration (ms): 255.4 | learning rate: 4.780E-04 | global batch size:   192 | lm loss: 1.365822E+00 | loss scale: 65536.0 | grad norm: 0.475 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1940/    4000 | consumed samples:       372480 | elapsed time per iteration (ms): 253.7 | learning rate: 4.805E-04 | global batch size:   192 | lm loss: 1.358489E+00 | loss scale: 65536.0 | grad norm: 0.461 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1950/    4000 | consumed samples:       374400 | elapsed time per iteration (ms): 256.1 | learning rate: 4.830E-04 | global batch size:   192 | lm loss: 1.356650E+00 | loss scale: 65536.0 | grad norm: 0.469 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1960/    4000 | consumed samples:       376320 | elapsed time per iteration (ms): 256.4 | learning rate: 4.855E-04 | global batch size:   192 | lm loss: 1.370228E+00 | loss scale: 65536.0 | grad norm: 0.714 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1970/    4000 | consumed samples:       378240 | elapsed time per iteration (ms): 256.2 | learning rate: 4.880E-04 | global batch size:   192 | lm loss: 1.372931E+00 | loss scale: 65536.0 | grad norm: 0.353 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1980/    4000 | consumed samples:       380160 | elapsed time per iteration (ms): 257.8 | learning rate: 4.905E-04 | global batch size:   192 | lm loss: 1.354842E+00 | loss scale: 65536.0 | grad norm: 0.424 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     1990/    4000 | consumed samples:       382080 | elapsed time per iteration (ms): 256.0 | learning rate: 4.930E-04 | global batch size:   192 | lm loss: 1.352620E+00 | loss scale: 65536.0 | grad norm: 0.396 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2000/    4000 | consumed samples:       384000 | elapsed time per iteration (ms): 256.1 | learning rate: 4.955E-04 | global batch size:   192 | lm loss: 1.358470E+00 | loss scale: 65536.0 | grad norm: 0.475 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 2000 | lm loss value: 1.317515E+00 | lm loss PPL: 3.734129E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:saving checkpoint at iteration    2000 to /opt/ml/model/\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:  successfully saved checkpoint at iteration    2000 to /opt/ml/model/\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:(min, max) time across ranks (ms):\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:    save-checkpoint ................................: (711.59, 711.65)\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2010/    4000 | consumed samples:       385920 | elapsed time per iteration (ms): 442.1 | learning rate: 4.980E-04 | global batch size:   192 | lm loss: 1.331705E+00 | loss scale: 65536.0 | grad norm: 0.410 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2020/    4000 | consumed samples:       387840 | elapsed time per iteration (ms): 256.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.357885E+00 | loss scale: 131072.0 | grad norm: 0.458 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2030/    4000 | consumed samples:       389760 | elapsed time per iteration (ms): 259.5 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.344868E+00 | loss scale: 131072.0 | grad norm: 0.446 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2040/    4000 | consumed samples:       391680 | elapsed time per iteration (ms): 256.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.349390E+00 | loss scale: 131072.0 | grad norm: 0.492 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2050/    4000 | consumed samples:       393600 | elapsed time per iteration (ms): 256.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.343982E+00 | loss scale: 131072.0 | grad norm: 0.667 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2060/    4000 | consumed samples:       395520 | elapsed time per iteration (ms): 255.5 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.344192E+00 | loss scale: 131072.0 | grad norm: 0.432 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2070/    4000 | consumed samples:       397440 | elapsed time per iteration (ms): 253.8 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.330108E+00 | loss scale: 131072.0 | grad norm: 0.427 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2080/    4000 | consumed samples:       399360 | elapsed time per iteration (ms): 253.8 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.355378E+00 | loss scale: 131072.0 | grad norm: 0.378 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2090/    4000 | consumed samples:       401280 | elapsed time per iteration (ms): 253.5 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.330354E+00 | loss scale: 131072.0 | grad norm: 0.397 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2100/    4000 | consumed samples:       403200 | elapsed time per iteration (ms): 253.4 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.323778E+00 | loss scale: 131072.0 | grad norm: 0.562 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2110/    4000 | consumed samples:       405120 | elapsed time per iteration (ms): 253.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.334911E+00 | loss scale: 131072.0 | grad norm: 0.513 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2120/    4000 | consumed samples:       407040 | elapsed time per iteration (ms): 253.4 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.325447E+00 | loss scale: 131072.0 | grad norm: 0.368 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2130/    4000 | consumed samples:       408960 | elapsed time per iteration (ms): 253.9 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.347028E+00 | loss scale: 131072.0 | grad norm: 0.411 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2140/    4000 | consumed samples:       410880 | elapsed time per iteration (ms): 254.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.321877E+00 | loss scale: 131072.0 | grad norm: 0.347 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2150/    4000 | consumed samples:       412800 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.307175E+00 | loss scale: 131072.0 | grad norm: 0.407 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2160/    4000 | consumed samples:       414720 | elapsed time per iteration (ms): 254.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.333541E+00 | loss scale: 131072.0 | grad norm: 0.526 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2170/    4000 | consumed samples:       416640 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.332315E+00 | loss scale: 131072.0 | grad norm: 0.390 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2180/    4000 | consumed samples:       418560 | elapsed time per iteration (ms): 258.9 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.311046E+00 | loss scale: 131072.0 | grad norm: 0.658 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2190/    4000 | consumed samples:       420480 | elapsed time per iteration (ms): 253.9 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.318760E+00 | loss scale: 131072.0 | grad norm: 0.331 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2200/    4000 | consumed samples:       422400 | elapsed time per iteration (ms): 257.8 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.293833E+00 | loss scale: 131072.0 | grad norm: 0.558 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 2200 | lm loss value: 1.275817E+00 | lm loss PPL: 3.581627E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2210/    4000 | consumed samples:       424320 | elapsed time per iteration (ms): 364.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.327599E+00 | loss scale: 131072.0 | grad norm: 0.398 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2220/    4000 | consumed samples:       426240 | elapsed time per iteration (ms): 254.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.314894E+00 | loss scale: 131072.0 | grad norm: 0.326 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2230/    4000 | consumed samples:       428160 | elapsed time per iteration (ms): 254.1 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.296763E+00 | loss scale: 131072.0 | grad norm: 0.373 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2240/    4000 | consumed samples:       430080 | elapsed time per iteration (ms): 254.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.307761E+00 | loss scale: 131072.0 | grad norm: 0.408 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2250/    4000 | consumed samples:       432000 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.312689E+00 | loss scale: 131072.0 | grad norm: 0.719 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2260/    4000 | consumed samples:       433920 | elapsed time per iteration (ms): 261.6 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.290885E+00 | loss scale: 131072.0 | grad norm: 0.379 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2270/    4000 | consumed samples:       435840 | elapsed time per iteration (ms): 257.7 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.286520E+00 | loss scale: 131072.0 | grad norm: 0.447 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2280/    4000 | consumed samples:       437760 | elapsed time per iteration (ms): 256.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.292704E+00 | loss scale: 131072.0 | grad norm: 0.347 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2290/    4000 | consumed samples:       439680 | elapsed time per iteration (ms): 256.4 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.311753E+00 | loss scale: 131072.0 | grad norm: 0.355 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2300/    4000 | consumed samples:       441600 | elapsed time per iteration (ms): 258.6 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.279504E+00 | loss scale: 131072.0 | grad norm: 0.454 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2310/    4000 | consumed samples:       443520 | elapsed time per iteration (ms): 256.1 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.314868E+00 | loss scale: 131072.0 | grad norm: 0.355 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2320/    4000 | consumed samples:       445440 | elapsed time per iteration (ms): 256.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.303954E+00 | loss scale: 131072.0 | grad norm: 0.412 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2330/    4000 | consumed samples:       447360 | elapsed time per iteration (ms): 256.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.295788E+00 | loss scale: 131072.0 | grad norm: 0.337 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2340/    4000 | consumed samples:       449280 | elapsed time per iteration (ms): 258.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.292911E+00 | loss scale: 131072.0 | grad norm: 0.318 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2350/    4000 | consumed samples:       451200 | elapsed time per iteration (ms): 261.4 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.285328E+00 | loss scale: 131072.0 | grad norm: 0.374 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2360/    4000 | consumed samples:       453120 | elapsed time per iteration (ms): 262.1 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.278894E+00 | loss scale: 131072.0 | grad norm: 0.442 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2370/    4000 | consumed samples:       455040 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.269270E+00 | loss scale: 131072.0 | grad norm: 0.407 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2380/    4000 | consumed samples:       456960 | elapsed time per iteration (ms): 254.1 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.281330E+00 | loss scale: 131072.0 | grad norm: 0.310 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2390/    4000 | consumed samples:       458880 | elapsed time per iteration (ms): 258.6 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.286137E+00 | loss scale: 131072.0 | grad norm: 0.721 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2400/    4000 | consumed samples:       460800 | elapsed time per iteration (ms): 254.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.294503E+00 | loss scale: 131072.0 | grad norm: 0.344 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 2400 | lm loss value: 1.237173E+00 | lm loss PPL: 3.445858E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2410/    4000 | consumed samples:       462720 | elapsed time per iteration (ms): 372.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.284117E+00 | loss scale: 131072.0 | grad norm: 0.421 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2420/    4000 | consumed samples:       464640 | elapsed time per iteration (ms): 253.9 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.292129E+00 | loss scale: 131072.0 | grad norm: 0.304 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2430/    4000 | consumed samples:       466560 | elapsed time per iteration (ms): 260.5 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.329819E+00 | loss scale: 131072.0 | grad norm: 0.638 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2440/    4000 | consumed samples:       468480 | elapsed time per iteration (ms): 254.7 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.330877E+00 | loss scale: 131072.0 | grad norm: 0.505 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2450/    4000 | consumed samples:       470400 | elapsed time per iteration (ms): 253.9 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.302325E+00 | loss scale: 131072.0 | grad norm: 0.372 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2460/    4000 | consumed samples:       472320 | elapsed time per iteration (ms): 254.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.275673E+00 | loss scale: 131072.0 | grad norm: 0.365 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2470/    4000 | consumed samples:       474240 | elapsed time per iteration (ms): 263.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.253299E+00 | loss scale: 131072.0 | grad norm: 0.467 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2480/    4000 | consumed samples:       476160 | elapsed time per iteration (ms): 253.9 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.275093E+00 | loss scale: 131072.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2490/    4000 | consumed samples:       478080 | elapsed time per iteration (ms): 256.1 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.256770E+00 | loss scale: 131072.0 | grad norm: 0.329 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2500/    4000 | consumed samples:       480000 | elapsed time per iteration (ms): 264.7 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.282557E+00 | loss scale: 131072.0 | grad norm: 0.409 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2510/    4000 | consumed samples:       481920 | elapsed time per iteration (ms): 257.7 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.256024E+00 | loss scale: 131072.0 | grad norm: 0.391 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2520/    4000 | consumed samples:       483840 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.258201E+00 | loss scale: 131072.0 | grad norm: 0.487 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2530/    4000 | consumed samples:       485760 | elapsed time per iteration (ms): 253.9 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.256383E+00 | loss scale: 131072.0 | grad norm: 0.307 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2540/    4000 | consumed samples:       487680 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.278559E+00 | loss scale: 131072.0 | grad norm: 0.271 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2550/    4000 | consumed samples:       489600 | elapsed time per iteration (ms): 253.9 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.250537E+00 | loss scale: 131072.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2560/    4000 | consumed samples:       491520 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.268514E+00 | loss scale: 131072.0 | grad norm: 0.422 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2570/    4000 | consumed samples:       493440 | elapsed time per iteration (ms): 254.9 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.251284E+00 | loss scale: 131072.0 | grad norm: 0.585 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2580/    4000 | consumed samples:       495360 | elapsed time per iteration (ms): 256.6 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.276842E+00 | loss scale: 131072.0 | grad norm: 0.360 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2590/    4000 | consumed samples:       497280 | elapsed time per iteration (ms): 256.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.274899E+00 | loss scale: 131072.0 | grad norm: 0.294 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2600/    4000 | consumed samples:       499200 | elapsed time per iteration (ms): 256.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.291514E+00 | loss scale: 131072.0 | grad norm: 0.793 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 2600 | lm loss value: 1.242129E+00 | lm loss PPL: 3.462979E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2610/    4000 | consumed samples:       501120 | elapsed time per iteration (ms): 370.9 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.278515E+00 | loss scale: 131072.0 | grad norm: 0.368 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2620/    4000 | consumed samples:       503040 | elapsed time per iteration (ms): 256.4 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.265678E+00 | loss scale: 131072.0 | grad norm: 0.419 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2630/    4000 | consumed samples:       504960 | elapsed time per iteration (ms): 256.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.265614E+00 | loss scale: 131072.0 | grad norm: 0.269 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2640/    4000 | consumed samples:       506880 | elapsed time per iteration (ms): 260.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.262069E+00 | loss scale: 131072.0 | grad norm: 0.289 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2650/    4000 | consumed samples:       508800 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.248608E+00 | loss scale: 131072.0 | grad norm: 0.318 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2660/    4000 | consumed samples:       510720 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.233965E+00 | loss scale: 131072.0 | grad norm: 0.366 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2670/    4000 | consumed samples:       512640 | elapsed time per iteration (ms): 254.1 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.249906E+00 | loss scale: 131072.0 | grad norm: 0.296 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2680/    4000 | consumed samples:       514560 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.266271E+00 | loss scale: 131072.0 | grad norm: 0.414 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2690/    4000 | consumed samples:       516480 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.246745E+00 | loss scale: 131072.0 | grad norm: 0.358 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2700/    4000 | consumed samples:       518400 | elapsed time per iteration (ms): 255.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.247721E+00 | loss scale: 131072.0 | grad norm: 0.266 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2710/    4000 | consumed samples:       520320 | elapsed time per iteration (ms): 254.5 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.248192E+00 | loss scale: 131072.0 | grad norm: 0.358 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2720/    4000 | consumed samples:       522240 | elapsed time per iteration (ms): 258.4 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.259841E+00 | loss scale: 131072.0 | grad norm: 0.568 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2730/    4000 | consumed samples:       524160 | elapsed time per iteration (ms): 257.1 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.244479E+00 | loss scale: 131072.0 | grad norm: 0.344 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2740/    4000 | consumed samples:       526080 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.247831E+00 | loss scale: 131072.0 | grad norm: 0.293 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2750/    4000 | consumed samples:       528000 | elapsed time per iteration (ms): 254.1 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.227591E+00 | loss scale: 131072.0 | grad norm: 0.489 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2760/    4000 | consumed samples:       529920 | elapsed time per iteration (ms): 253.9 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.238207E+00 | loss scale: 131072.0 | grad norm: 0.296 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2770/    4000 | consumed samples:       531840 | elapsed time per iteration (ms): 254.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.220473E+00 | loss scale: 131072.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2780/    4000 | consumed samples:       533760 | elapsed time per iteration (ms): 254.1 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.232559E+00 | loss scale: 131072.0 | grad norm: 0.359 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2790/    4000 | consumed samples:       535680 | elapsed time per iteration (ms): 254.7 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.242616E+00 | loss scale: 131072.0 | grad norm: 0.308 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2800/    4000 | consumed samples:       537600 | elapsed time per iteration (ms): 255.4 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.219155E+00 | loss scale: 131072.0 | grad norm: 0.265 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 2800 | lm loss value: 1.197719E+00 | lm loss PPL: 3.312551E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2810/    4000 | consumed samples:       539520 | elapsed time per iteration (ms): 363.8 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.227988E+00 | loss scale: 131072.0 | grad norm: 0.322 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2820/    4000 | consumed samples:       541440 | elapsed time per iteration (ms): 254.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.259420E+00 | loss scale: 131072.0 | grad norm: 0.273 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2830/    4000 | consumed samples:       543360 | elapsed time per iteration (ms): 254.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.215851E+00 | loss scale: 131072.0 | grad norm: 0.394 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2840/    4000 | consumed samples:       545280 | elapsed time per iteration (ms): 254.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.202148E+00 | loss scale: 131072.0 | grad norm: 0.285 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2850/    4000 | consumed samples:       547200 | elapsed time per iteration (ms): 254.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.230271E+00 | loss scale: 131072.0 | grad norm: 0.403 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2860/    4000 | consumed samples:       549120 | elapsed time per iteration (ms): 254.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.241366E+00 | loss scale: 131072.0 | grad norm: 0.311 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2870/    4000 | consumed samples:       551040 | elapsed time per iteration (ms): 256.6 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.230279E+00 | loss scale: 131072.0 | grad norm: 0.425 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2880/    4000 | consumed samples:       552960 | elapsed time per iteration (ms): 254.1 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.214466E+00 | loss scale: 131072.0 | grad norm: 0.302 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2890/    4000 | consumed samples:       554880 | elapsed time per iteration (ms): 256.1 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.226886E+00 | loss scale: 131072.0 | grad norm: 0.305 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2900/    4000 | consumed samples:       556800 | elapsed time per iteration (ms): 254.0 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.237546E+00 | loss scale: 131072.0 | grad norm: 0.527 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2910/    4000 | consumed samples:       558720 | elapsed time per iteration (ms): 254.3 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.209866E+00 | loss scale: 131072.0 | grad norm: 0.312 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2920/    4000 | consumed samples:       560640 | elapsed time per iteration (ms): 254.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.222778E+00 | loss scale: 131072.0 | grad norm: 0.254 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2930/    4000 | consumed samples:       562560 | elapsed time per iteration (ms): 254.5 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.218228E+00 | loss scale: 131072.0 | grad norm: 0.310 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2940/    4000 | consumed samples:       564480 | elapsed time per iteration (ms): 254.5 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.235946E+00 | loss scale: 131072.0 | grad norm: 0.300 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2950/    4000 | consumed samples:       566400 | elapsed time per iteration (ms): 259.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.206984E+00 | loss scale: 131072.0 | grad norm: 0.254 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2960/    4000 | consumed samples:       568320 | elapsed time per iteration (ms): 257.2 | learning rate: 5.000E-04 | global batch size:   192 | lm loss: 1.236090E+00 | loss scale: 131072.0 | grad norm: 0.287 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2970/    4000 | consumed samples:       570240 | elapsed time per iteration (ms): 256.5 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.225540E+00 | loss scale: 131072.0 | grad norm: 0.289 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2980/    4000 | consumed samples:       572160 | elapsed time per iteration (ms): 255.1 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.230258E+00 | loss scale: 131072.0 | grad norm: 0.359 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     2990/    4000 | consumed samples:       574080 | elapsed time per iteration (ms): 254.8 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.222297E+00 | loss scale: 131072.0 | grad norm: 0.316 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3000/    4000 | consumed samples:       576000 | elapsed time per iteration (ms): 255.1 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.211284E+00 | loss scale: 131072.0 | grad norm: 0.318 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: validation loss at iteration 3000 | lm loss value: 1.174030E+00 | lm loss PPL: 3.235003E+00 | \u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3010/    4000 | consumed samples:       577920 | elapsed time per iteration (ms): 366.4 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.213697E+00 | loss scale: 131072.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3020/    4000 | consumed samples:       579840 | elapsed time per iteration (ms): 256.7 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.214409E+00 | loss scale: 262144.0 | grad norm: 0.254 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3030/    4000 | consumed samples:       581760 | elapsed time per iteration (ms): 254.0 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.216662E+00 | loss scale: 262144.0 | grad norm: 0.338 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3040/    4000 | consumed samples:       583680 | elapsed time per iteration (ms): 253.8 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.192316E+00 | loss scale: 262144.0 | grad norm: 0.426 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3050/    4000 | consumed samples:       585600 | elapsed time per iteration (ms): 254.0 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.221668E+00 | loss scale: 262144.0 | grad norm: 0.262 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3060/    4000 | consumed samples:       587520 | elapsed time per iteration (ms): 253.6 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.208688E+00 | loss scale: 262144.0 | grad norm: 0.302 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3070/    4000 | consumed samples:       589440 | elapsed time per iteration (ms): 260.9 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.219617E+00 | loss scale: 262144.0 | grad norm: 0.269 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3080/    4000 | consumed samples:       591360 | elapsed time per iteration (ms): 276.1 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.214652E+00 | loss scale: 262144.0 | grad norm: 0.468 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3090/    4000 | consumed samples:       593280 | elapsed time per iteration (ms): 256.2 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.214288E+00 | loss scale: 262144.0 | grad norm: 0.289 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3100/    4000 | consumed samples:       595200 | elapsed time per iteration (ms): 255.9 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.194206E+00 | loss scale: 262144.0 | grad norm: 0.258 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3110/    4000 | consumed samples:       597120 | elapsed time per iteration (ms): 254.1 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.215005E+00 | loss scale: 262144.0 | grad norm: 0.291 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3120/    4000 | consumed samples:       599040 | elapsed time per iteration (ms): 254.1 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.192767E+00 | loss scale: 262144.0 | grad norm: 0.278 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3130/    4000 | consumed samples:       600960 | elapsed time per iteration (ms): 255.9 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.179164E+00 | loss scale: 262144.0 | grad norm: 0.254 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3140/    4000 | consumed samples:       602880 | elapsed time per iteration (ms): 253.9 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.225626E+00 | loss scale: 262144.0 | grad norm: 0.267 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3150/    4000 | consumed samples:       604800 | elapsed time per iteration (ms): 254.1 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.196779E+00 | loss scale: 262144.0 | grad norm: 0.349 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3160/    4000 | consumed samples:       606720 | elapsed time per iteration (ms): 253.7 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.209348E+00 | loss scale: 262144.0 | grad norm: 0.330 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3170/    4000 | consumed samples:       608640 | elapsed time per iteration (ms): 253.7 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.191299E+00 | loss scale: 262144.0 | grad norm: 0.266 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3180/    4000 | consumed samples:       610560 | elapsed time per iteration (ms): 254.5 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.199996E+00 | loss scale: 262144.0 | grad norm: 0.251 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>: iteration     3190/    4000 | consumed samples:       612480 | elapsed time per iteration (ms): 255.0 | learning rate: 4.999E-04 | global batch size:   192 | lm loss: 1.212945E+00 | loss scale: 262144.0 | grad norm: 0.287 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[190], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sagemaker_session \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mSession()\n\u001b[0;32m----> 2\u001b[0m \u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:4849\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4829\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4830\u001b[0m \n\u001b[1;32m   4831\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4847\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4848\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4849\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:6710\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE:\n\u001b[1;32m   6708\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 6710\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE:\n\u001b[1;32m   6713\u001b[0m     state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
